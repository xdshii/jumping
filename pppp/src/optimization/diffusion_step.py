"""
æ‰©æ•£æ­¥éª¤å‡½æ•°å®ç°

è¯¥æ¨¡å—æä¾›æ‰©æ•£è¿‡ç¨‹ä¸­å•æ­¥è®¡ç®—çš„å°è£…ï¼ŒåŒ…æ‹¬ï¼š
1. DDIMå‰å‘/åå‘æ­¥éª¤
2. å™ªå£°é¢„æµ‹ä¸å¼•å¯¼
3. æ½œç©ºé—´æ›´æ–°
4. è°ƒåº¦å™¨é›†æˆ

ä½œè€…: AI Privacy Protection System
æ—¥æœŸ: 2025-07-28
"""

import torch
import torch.nn.functional as F
import logging
from typing import Optional, Tuple, Union, Dict, Any, Callable
from tqdm import tqdm
import numpy as np

try:
    from ..models.sd_loader import StableDiffusionLoader, ModelComponents
    from .ddim_inversion import DDIMInverter
except ImportError:
    import sys
    from pathlib import Path
    sys.path.append(str(Path(__file__).parent.parent))
    from models.sd_loader import StableDiffusionLoader, ModelComponents
    from optimization.ddim_inversion import DDIMInverter

logger = logging.getLogger(__name__)

class DiffusionStepper:
    """
    æ‰©æ•£æ­¥éª¤å¤„ç†å™¨
    
    å°è£…æ‰©æ•£è¿‡ç¨‹ä¸­çš„å•æ­¥è®¡ç®—ï¼ŒåŒ…æ‹¬DDIMæ­¥éª¤ã€å™ªå£°é¢„æµ‹å’Œæ½œç©ºé—´æ›´æ–°
    """
    
    def __init__(
        self,
        sd_loader: StableDiffusionLoader,
        guidance_scale: float = 7.5,
        eta: float = 0.0
    ):
        """
        åˆå§‹åŒ–æ‰©æ•£æ­¥éª¤å¤„ç†å™¨
        
        Args:
            sd_loader: SDåŠ è½½å™¨
            guidance_scale: åˆ†ç±»å¼•å¯¼å¼ºåº¦
            eta: DDIMå‚æ•°ï¼Œ0ä¸ºç¡®å®šæ€§ï¼Œ1ä¸ºéšæœº
        """
        self.sd_loader = sd_loader
        self.guidance_scale = guidance_scale
        self.eta = eta
        
        # ç¡®ä¿ç»„ä»¶å·²åŠ è½½
        if not hasattr(sd_loader, 'components') or sd_loader.components is None:
            self.components = sd_loader.load_components()
        else:
            self.components = sd_loader.components
        
        # åˆ›å»ºDDIMåè½¬å™¨ï¼ˆç”¨äºæŸäº›è®¡ç®—ï¼‰
        self.ddim_inverter = DDIMInverter(sd_loader, guidance_scale=guidance_scale, eta=eta)
        
        logger.info(f"æ‰©æ•£æ­¥éª¤å¤„ç†å™¨åˆå§‹åŒ–: guidance_scale={guidance_scale}, eta={eta}")
    
    def predict_noise(
        self,
        latents: torch.Tensor,
        timestep: Union[int, torch.Tensor],
        text_embeddings: torch.Tensor,
        uncond_embeddings: Optional[torch.Tensor] = None,
        guidance_scale: Optional[float] = None
    ) -> torch.Tensor:
        """
        é¢„æµ‹å™ªå£°
        
        Args:
            latents: æ½œç©ºé—´è¡¨ç¤º [batch_size, 4, H, W]
            timestep: æ—¶é—´æ­¥ 
            text_embeddings: æ¡ä»¶æ–‡æœ¬åµŒå…¥
            uncond_embeddings: æ— æ¡ä»¶æ–‡æœ¬åµŒå…¥ï¼ˆå¯é€‰ï¼‰
            guidance_scale: å¼•å¯¼å¼ºåº¦ï¼ˆå¯é€‰ï¼Œè¦†ç›–é»˜è®¤å€¼ï¼‰
            
        Returns:
            é¢„æµ‹çš„å™ªå£° [batch_size, 4, H, W]
        """
        if guidance_scale is None:
            guidance_scale = self.guidance_scale
            
        # ç¡®ä¿timestepæ ¼å¼æ­£ç¡®
        if isinstance(timestep, int):
            timestep = torch.tensor([timestep], device=latents.device)
        elif timestep.dim() == 0:
            timestep = timestep.unsqueeze(0)
        
        # ç¡®ä¿timestepåœ¨æ­£ç¡®è®¾å¤‡ä¸Šä¸”å½¢çŠ¶åŒ¹é…
        timestep = timestep.to(latents.device)
        if timestep.shape[0] != latents.shape[0]:
            timestep = timestep.repeat(latents.shape[0])
        
        # æ¡ä»¶é¢„æµ‹
        noise_pred_cond = self.components.unet(
            latents,
            timestep,
            encoder_hidden_states=text_embeddings
        ).sample
        
        # å¦‚æœæœ‰æ— æ¡ä»¶åµŒå…¥ä¸”å¼•å¯¼å¼ºåº¦>1ï¼Œè¿›è¡Œå¼•å¯¼
        if uncond_embeddings is not None and guidance_scale > 1.0:
            # æ— æ¡ä»¶é¢„æµ‹
            noise_pred_uncond = self.components.unet(
                latents,
                timestep,
                encoder_hidden_states=uncond_embeddings
            ).sample
            
            # åˆ†ç±»å¼•å¯¼
            noise_pred = noise_pred_uncond + guidance_scale * (noise_pred_cond - noise_pred_uncond)
        else:
            noise_pred = noise_pred_cond
            
        return noise_pred
    
    def ddim_step_forward(
        self,
        latents: torch.Tensor,
        noise_pred: torch.Tensor,
        timestep: int,
        prev_timestep: int,
        eta: Optional[float] = None
    ) -> torch.Tensor:
        """
        DDIMå‰å‘æ­¥éª¤
        
        Args:
            latents: å½“å‰æ½œç©ºé—´
            noise_pred: é¢„æµ‹çš„å™ªå£°
            timestep: å½“å‰æ—¶é—´æ­¥
            prev_timestep: å‰ä¸€ä¸ªæ—¶é—´æ­¥
            eta: DDIMå‚æ•°
            
        Returns:
            æ›´æ–°åçš„æ½œç©ºé—´
        """
        if eta is None:
            eta = self.eta
            
        # è·å–è°ƒåº¦å™¨å‚æ•°
        alpha_prod_t = self.components.scheduler.alphas_cumprod[timestep]
        alpha_prod_t_prev = self.components.scheduler.alphas_cumprod[prev_timestep] if prev_timestep >= 0 else self.components.scheduler.final_alpha_cumprod
        
        beta_prod_t = 1 - alpha_prod_t
        beta_prod_t_prev = 1 - alpha_prod_t_prev
        
        # è®¡ç®—é¢„æµ‹çš„åŸå§‹æ ·æœ¬
        pred_original_sample = (latents - beta_prod_t ** 0.5 * noise_pred) / alpha_prod_t ** 0.5
        
        # è®¡ç®—æ–¹å‘å‘é‡
        pred_sample_direction = (beta_prod_t_prev ** 0.5) * noise_pred
        
        # è®¡ç®—å‰ä¸€æ­¥çš„æ ·æœ¬
        prev_sample = alpha_prod_t_prev ** 0.5 * pred_original_sample + pred_sample_direction
        
        # æ·»åŠ éšæœºæ€§ï¼ˆå¦‚æœeta > 0ï¼‰
        if eta > 0:
            variance = (beta_prod_t_prev / beta_prod_t) * (1 - alpha_prod_t / alpha_prod_t_prev)
            sigma = eta * variance ** 0.5
            
            # æ·»åŠ å™ªå£°
            noise = torch.randn_like(latents)
            prev_sample = prev_sample + sigma * noise
            
        return prev_sample
    
    def ddim_step_reverse(
        self,
        latents: torch.Tensor,
        noise_pred: torch.Tensor,
        timestep: int,
        next_timestep: int
    ) -> torch.Tensor:
        """
        DDIMåå‘æ­¥éª¤ï¼ˆç”¨äºåè½¬ï¼‰
        
        Args:
            latents: å½“å‰æ½œç©ºé—´
            noise_pred: é¢„æµ‹çš„å™ªå£°
            timestep: å½“å‰æ—¶é—´æ­¥
            next_timestep: ä¸‹ä¸€ä¸ªæ—¶é—´æ­¥
            
        Returns:
            åè½¬åçš„æ½œç©ºé—´
        """
        # ä½¿ç”¨DDIMInverterä¸­çš„å®ç°
        return self.ddim_inverter.ddim_step_reverse(latents, noise_pred, timestep, next_timestep)
    
    def scheduler_step(
        self,
        noise_pred: torch.Tensor,
        timestep: int,
        sample: torch.Tensor,
        eta: Optional[float] = None,
        use_clipped_model_output: bool = False,
        generator: Optional[torch.Generator] = None
    ) -> torch.Tensor:
        """
        ä½¿ç”¨è°ƒåº¦å™¨è¿›è¡Œå•æ­¥æ›´æ–°
        
        Args:
            noise_pred: é¢„æµ‹çš„å™ªå£°
            timestep: å½“å‰æ—¶é—´æ­¥
            sample: å½“å‰æ ·æœ¬
            eta: DDIMå‚æ•°
            use_clipped_model_output: æ˜¯å¦ä½¿ç”¨è£å‰ªçš„æ¨¡å‹è¾“å‡º
            generator: éšæœºæ•°ç”Ÿæˆå™¨
            
        Returns:
            æ›´æ–°åçš„æ ·æœ¬
        """
        if eta is None:
            eta = self.eta
            
        # è®¾ç½®è°ƒåº¦å™¨çš„etaå‚æ•°
        original_eta = getattr(self.components.scheduler, 'eta', 0.0)
        self.components.scheduler.eta = eta
        
        try:
            # ä½¿ç”¨è°ƒåº¦å™¨è¿›è¡Œæ­¥éª¤æ›´æ–°
            scheduler_output = self.components.scheduler.step(
                model_output=noise_pred,
                timestep=timestep,
                sample=sample,
                eta=eta,
                use_clipped_model_output=use_clipped_model_output,
                generator=generator,
                return_dict=True
            )
            
            return scheduler_output.prev_sample
            
        finally:
            # æ¢å¤åŸå§‹etaå€¼
            self.components.scheduler.eta = original_eta
    
    def compute_velocity(
        self,
        latents: torch.Tensor,
        noise: torch.Tensor,
        timestep: int
    ) -> torch.Tensor:
        """
        è®¡ç®—é€Ÿåº¦å‚æ•°åŒ–ï¼ˆç”¨äºæŸäº›æ‰©æ•£æ¨¡å‹å˜ä½“ï¼‰
        
        Args:
            latents: æ½œç©ºé—´è¡¨ç¤º
            noise: å™ªå£°
            timestep: æ—¶é—´æ­¥
            
        Returns:
            é€Ÿåº¦å‘é‡
        """
        alpha_prod_t = self.components.scheduler.alphas_cumprod[timestep]
        beta_prod_t = 1 - alpha_prod_t
        
        velocity = alpha_prod_t ** 0.5 * noise - beta_prod_t ** 0.5 * latents
        return velocity
    
    def apply_guidance(
        self,
        noise_pred_uncond: torch.Tensor,
        noise_pred_cond: torch.Tensor,
        guidance_scale: Optional[float] = None
    ) -> torch.Tensor:
        """
        åº”ç”¨åˆ†ç±»å¼•å¯¼
        
        Args:
            noise_pred_uncond: æ— æ¡ä»¶å™ªå£°é¢„æµ‹
            noise_pred_cond: æ¡ä»¶å™ªå£°é¢„æµ‹
            guidance_scale: å¼•å¯¼å¼ºåº¦
            
        Returns:
            å¼•å¯¼åçš„å™ªå£°é¢„æµ‹
        """
        if guidance_scale is None:
            guidance_scale = self.guidance_scale
            
        return noise_pred_uncond + guidance_scale * (noise_pred_cond - noise_pred_uncond)
    
    def get_timesteps(
        self,
        num_inference_steps: int,
        strength: float = 1.0
    ) -> torch.Tensor:
        """
        è·å–æ¨ç†æ—¶é—´æ­¥åºåˆ—
        
        Args:
            num_inference_steps: æ¨ç†æ­¥æ•°
            strength: å¼ºåº¦ï¼ˆç”¨äºå›¾åƒåˆ°å›¾åƒï¼‰
            
        Returns:
            æ—¶é—´æ­¥å¼ é‡
        """
        # è®¾ç½®è°ƒåº¦å™¨æ—¶é—´æ­¥
        self.components.scheduler.set_timesteps(num_inference_steps)
        timesteps = self.components.scheduler.timesteps
        
        # å¦‚æœstrength < 1.0ï¼Œè·³è¿‡ä¸€äº›åˆå§‹æ­¥éª¤
        if strength < 1.0:
            init_timestep = min(int(num_inference_steps * strength), num_inference_steps)
            t_start = max(num_inference_steps - init_timestep, 0)
            timesteps = timesteps[t_start:]
            
        return timesteps


def create_diffusion_stepper(
    sd_loader: StableDiffusionLoader,
    guidance_scale: float = 7.5,
    eta: float = 0.0
) -> DiffusionStepper:
    """
    åˆ›å»ºæ‰©æ•£æ­¥éª¤å¤„ç†å™¨çš„ä¾¿æ·å‡½æ•°
    
    Args:
        sd_loader: SDåŠ è½½å™¨
        guidance_scale: åˆ†ç±»å¼•å¯¼å¼ºåº¦
        eta: DDIMå‚æ•°
        
    Returns:
        æ‰©æ•£æ­¥éª¤å¤„ç†å™¨å®ä¾‹
    """
    return DiffusionStepper(sd_loader, guidance_scale, eta)


def test_diffusion_stepper():
    """æµ‹è¯•æ‰©æ•£æ­¥éª¤å¤„ç†å™¨"""
    print("ğŸ§ª æµ‹è¯•æ‰©æ•£æ­¥éª¤å¤„ç†å™¨...")
    
    try:
        # å¯¼å…¥SDåŠ è½½å™¨
        from models.sd_loader import create_sd_loader
        
        # åˆ›å»ºSDåŠ è½½å™¨å¹¶åŠ è½½ç»„ä»¶
        sd_loader = create_sd_loader()
        components = sd_loader.load_components()
        
        # åˆ›å»ºæ‰©æ•£æ­¥éª¤å¤„ç†å™¨
        stepper = create_diffusion_stepper(sd_loader)
        
        # åˆ›å»ºæµ‹è¯•æ•°æ®
        test_latents = torch.randn(1, 4, 64, 64, device=components.device, dtype=components.dtype)
        test_timestep = 100
        test_text_embeds = sd_loader.encode_text("a beautiful landscape")
        test_uncond_embeds = sd_loader.encode_text("")
        
        print(f"âœ… æ‰©æ•£æ­¥éª¤å¤„ç†å™¨åˆ›å»ºæˆåŠŸ")
        print(f"   è®¾å¤‡: {components.device}")
        print(f"   æ•°æ®ç±»å‹: {components.dtype}")
        print(f"   å¼•å¯¼å¼ºåº¦: {stepper.guidance_scale}")
        
        # æµ‹è¯•å™ªå£°é¢„æµ‹
        print("ğŸ”® æµ‹è¯•å™ªå£°é¢„æµ‹...")
        with torch.no_grad():
            noise_pred = stepper.predict_noise(
                test_latents, 
                test_timestep, 
                test_text_embeds,
                test_uncond_embeds
            )
        
        print(f"âœ… å™ªå£°é¢„æµ‹æˆåŠŸ: {noise_pred.shape}")
        
        # æµ‹è¯•DDIMå‰å‘æ­¥éª¤
        print("â¡ï¸ æµ‹è¯•DDIMå‰å‘æ­¥éª¤...")
        with torch.no_grad():
            next_latents = stepper.ddim_step_forward(
                test_latents,
                noise_pred,
                timestep=100,
                prev_timestep=90
            )
        
        print(f"âœ… DDIMå‰å‘æ­¥éª¤æˆåŠŸ: {next_latents.shape}")
        
        # æµ‹è¯•è°ƒåº¦å™¨æ­¥éª¤
        print("ğŸ“… æµ‹è¯•è°ƒåº¦å™¨æ­¥éª¤...")
        timesteps = stepper.get_timesteps(num_inference_steps=20)
        print(f"âœ… æ—¶é—´æ­¥è·å–æˆåŠŸ: {len(timesteps)} æ­¥")
        
        with torch.no_grad():
            scheduler_output = stepper.scheduler_step(
                noise_pred,
                timestep=timesteps[0].item(),
                sample=test_latents
            )
        
        print(f"âœ… è°ƒåº¦å™¨æ­¥éª¤æˆåŠŸ: {scheduler_output.shape}")
        
        # æµ‹è¯•å¼•å¯¼åº”ç”¨
        print("ğŸ¯ æµ‹è¯•å¼•å¯¼åº”ç”¨...")
        with torch.no_grad():
            uncond_noise = stepper.predict_noise(test_latents, test_timestep, test_uncond_embeds)
            cond_noise = stepper.predict_noise(test_latents, test_timestep, test_text_embeds)
            guided_noise = stepper.apply_guidance(uncond_noise, cond_noise, guidance_scale=7.5)
        
        print(f"âœ… å¼•å¯¼åº”ç”¨æˆåŠŸ: {guided_noise.shape}")
        
        print("ğŸ‰ æ‰©æ•£æ­¥éª¤å¤„ç†å™¨æµ‹è¯•å…¨éƒ¨é€šè¿‡ï¼")
        return True
        
    except Exception as e:
        print(f"âŒ æµ‹è¯•å¤±è´¥: {e}")
        import traceback
        traceback.print_exc()
        return False


if __name__ == "__main__":
    test_diffusion_stepper() 