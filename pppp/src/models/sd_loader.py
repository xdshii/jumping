"""
Stable Diffusionæ¨¡å‹åŠ è½½å™¨

è´Ÿè´£åŠ è½½å’Œç®¡ç†Stable Diffusion 2.0æ¨¡å‹çš„å„ä¸ªç»„ä»¶ï¼ŒåŒ…æ‹¬ï¼š
- UNetæ¨¡å‹ (å»å™ªç½‘ç»œ)
- VAEæ¨¡å‹ (å˜åˆ†è‡ªç¼–ç å™¨)
- Text Encoder (æ–‡æœ¬ç¼–ç å™¨)
- Tokenizer (åˆ†è¯å™¨)
- Scheduler (è°ƒåº¦å™¨)

ä½œè€…: AI Privacy Protection Team
åˆ›å»ºæ—¶é—´: 2025-01-28
ç‰ˆæœ¬: 1.0.0
"""

import os
import torch
import logging
from pathlib import Path
from typing import Optional, Dict, Any, Tuple
from dataclasses import dataclass

from diffusers import (
    StableDiffusionPipeline,
    UNet2DConditionModel,
    AutoencoderKL,
    DDIMScheduler,
    DPMSolverMultistepScheduler
)
from transformers import CLIPTextModel, CLIPTokenizer

logger = logging.getLogger(__name__)


@dataclass
class ModelComponents:
    """å­˜å‚¨Stable Diffusionæ¨¡å‹çš„å„ä¸ªç»„ä»¶"""
    unet: UNet2DConditionModel
    vae: AutoencoderKL
    text_encoder: CLIPTextModel
    tokenizer: CLIPTokenizer
    scheduler: DDIMScheduler
    device: torch.device
    dtype: torch.dtype


class StableDiffusionLoader:
    """Stable Diffusionæ¨¡å‹åŠ è½½å™¨"""
    
    def __init__(
        self,
        model_path: str = "checkpoints/sd2",
        device: Optional[str] = None,
        dtype: Optional[torch.dtype] = None,
        scheduler_type: str = "ddim"
    ):
        """
        åˆå§‹åŒ–æ¨¡å‹åŠ è½½å™¨
        
        Args:
            model_path: æ¨¡å‹è·¯å¾„
            device: è¿è¡Œè®¾å¤‡ ('cuda', 'cpu', 'auto')
            dtype: æ•°æ®ç±»å‹ (torch.float16, torch.float32)
            scheduler_type: è°ƒåº¦å™¨ç±»å‹ ('ddim', 'dpm')
        """
        self.model_path = Path(model_path)
        self.scheduler_type = scheduler_type
        
        # è‡ªåŠ¨æ£€æµ‹è®¾å¤‡å’Œæ•°æ®ç±»å‹
        if device is None or device == "auto":
            self.device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
        else:
            self.device = torch.device(device)
        
        if dtype is None:
            # CUDAä½¿ç”¨float16ä»¥èŠ‚çœæ˜¾å­˜ï¼ŒCPUä½¿ç”¨float32
            self.dtype = torch.float16 if self.device.type == "cuda" else torch.float32
        else:
            self.dtype = dtype
            
        self.components: Optional[ModelComponents] = None
        self.pipeline: Optional[StableDiffusionPipeline] = None
        
        logger.info(f"SDåŠ è½½å™¨åˆå§‹åŒ–: device={self.device}, dtype={self.dtype}")
    
    def load_components(self) -> ModelComponents:
        """
        åŠ è½½æ‰€æœ‰æ¨¡å‹ç»„ä»¶
        
        Returns:
            ModelComponents: åŒ…å«æ‰€æœ‰æ¨¡å‹ç»„ä»¶çš„æ•°æ®ç±»
        """
        if self.components is not None:
            logger.info("æ¨¡å‹ç»„ä»¶å·²åŠ è½½ï¼Œç›´æ¥è¿”å›")
            return self.components
        
        logger.info(f"å¼€å§‹åŠ è½½æ¨¡å‹ç»„ä»¶: {self.model_path}")
        
        try:
            # æ£€æŸ¥æ¨¡å‹è·¯å¾„
            if not self.model_path.exists():
                raise FileNotFoundError(f"æ¨¡å‹è·¯å¾„ä¸å­˜åœ¨: {self.model_path}")
            
            # åŠ è½½å„ä¸ªç»„ä»¶
            logger.info("åŠ è½½UNetæ¨¡å‹...")
            unet = UNet2DConditionModel.from_pretrained(
                self.model_path / "unet",
                torch_dtype=self.dtype
            )
            
            logger.info("åŠ è½½VAEæ¨¡å‹...")
            vae = AutoencoderKL.from_pretrained(
                self.model_path / "vae",
                torch_dtype=self.dtype
            )
            
            logger.info("åŠ è½½æ–‡æœ¬ç¼–ç å™¨...")
            text_encoder = CLIPTextModel.from_pretrained(
                self.model_path / "text_encoder",
                torch_dtype=self.dtype
            )
            
            logger.info("åŠ è½½åˆ†è¯å™¨...")
            tokenizer = CLIPTokenizer.from_pretrained(
                self.model_path / "tokenizer"
            )
            
            logger.info("åˆå§‹åŒ–è°ƒåº¦å™¨...")
            if self.scheduler_type == "ddim":
                scheduler = DDIMScheduler.from_pretrained(
                    self.model_path / "scheduler"
                )
            elif self.scheduler_type == "dpm":
                scheduler = DPMSolverMultistepScheduler.from_pretrained(
                    self.model_path / "scheduler"
                )
            else:
                raise ValueError(f"ä¸æ”¯æŒçš„è°ƒåº¦å™¨ç±»å‹: {self.scheduler_type}")
            
            # ç§»åŠ¨æ¨¡å‹åˆ°æŒ‡å®šè®¾å¤‡
            logger.info(f"ç§»åŠ¨æ¨¡å‹åˆ°è®¾å¤‡: {self.device}")
            unet = unet.to(self.device)
            vae = vae.to(self.device)
            text_encoder = text_encoder.to(self.device)
            
            # åˆ›å»ºç»„ä»¶å¯¹è±¡
            self.components = ModelComponents(
                unet=unet,
                vae=vae,
                text_encoder=text_encoder,
                tokenizer=tokenizer,
                scheduler=scheduler,
                device=self.device,
                dtype=self.dtype
            )
            
            logger.info("âœ… æ‰€æœ‰æ¨¡å‹ç»„ä»¶åŠ è½½å®Œæˆ")
            return self.components
            
        except Exception as e:
            logger.error(f"æ¨¡å‹ç»„ä»¶åŠ è½½å¤±è´¥: {e}")
            raise
    
    def load_pipeline(self) -> StableDiffusionPipeline:
        """
        åŠ è½½å®Œæ•´çš„Stable Diffusion Pipeline
        
        Returns:
            StableDiffusionPipeline: å®Œæ•´çš„SDæµæ°´çº¿
        """
        if self.pipeline is not None:
            logger.info("Pipelineå·²åŠ è½½ï¼Œç›´æ¥è¿”å›")
            return self.pipeline
        
        logger.info("åŠ è½½Stable Diffusion Pipeline...")
        
        try:
            self.pipeline = StableDiffusionPipeline.from_pretrained(
                str(self.model_path),
                torch_dtype=self.dtype,
                use_safetensors=True
            )
            self.pipeline = self.pipeline.to(self.device)
            
            # é…ç½®è°ƒåº¦å™¨
            if self.scheduler_type == "ddim":
                self.pipeline.scheduler = DDIMScheduler.from_config(
                    self.pipeline.scheduler.config
                )
            
            logger.info("âœ… Stable Diffusion PipelineåŠ è½½å®Œæˆ")
            return self.pipeline
            
        except Exception as e:
            logger.error(f"PipelineåŠ è½½å¤±è´¥: {e}")
            raise
    
    def get_vae_scale_factor(self) -> float:
        """è·å–VAEçš„ç¼©æ”¾å› å­"""
        if self.components is None:
            self.load_components()
        return 2 ** (len(self.components.vae.config.block_out_channels) - 1)
    
    def encode_text(self, prompt: str) -> torch.Tensor:
        """
        ç¼–ç æ–‡æœ¬æç¤º
        
        Args:
            prompt: æ–‡æœ¬æç¤º
            
        Returns:
            torch.Tensor: æ–‡æœ¬åµŒå…¥
        """
        if self.components is None:
            self.load_components()
        
        # åˆ†è¯
        text_inputs = self.components.tokenizer(
            prompt,
            padding="max_length",
            max_length=self.components.tokenizer.model_max_length,
            truncation=True,
            return_tensors="pt"
        )
        
        # ç¼–ç 
        with torch.no_grad():
            text_embeddings = self.components.text_encoder(
                text_inputs.input_ids.to(self.device)
            )[0]
        
        return text_embeddings
    
    def encode_images(self, images: torch.Tensor) -> torch.Tensor:
        """
        ä½¿ç”¨VAEç¼–ç å›¾åƒåˆ°æ½œç©ºé—´
        
        Args:
            images: å›¾åƒå¼ é‡ [B, C, H, W], å€¼åŸŸ[0, 1]
            
        Returns:
            torch.Tensor: æ½œåœ¨è¡¨ç¤º [B, C_latent, H//8, W//8]
        """
        if self.components is None:
            self.load_components()
        
        # è½¬æ¢åˆ°[-1, 1]
        images = 2.0 * images - 1.0
        
        with torch.no_grad():
            latents = self.components.vae.encode(images).latent_dist.sample()
            latents = latents * self.components.vae.config.scaling_factor
        
        return latents
    
    def decode_latents(self, latents: torch.Tensor) -> torch.Tensor:
        """
        ä½¿ç”¨VAEè§£ç æ½œåœ¨è¡¨ç¤ºåˆ°å›¾åƒ
        
        Args:
            latents: æ½œåœ¨è¡¨ç¤º [B, C_latent, H//8, W//8]
            
        Returns:
            torch.Tensor: å›¾åƒå¼ é‡ [B, C, H, W], å€¼åŸŸ[0, 1]
        """
        if self.components is None:
            self.load_components()
        
        # ç¼©æ”¾
        latents = latents / self.components.vae.config.scaling_factor
        
        with torch.no_grad():
            images = self.components.vae.decode(latents).sample
        
        # è½¬æ¢åˆ°[0, 1]
        images = (images + 1.0) / 2.0
        images = torch.clamp(images, 0.0, 1.0)
        
        return images
    
    def get_model_info(self) -> Dict[str, Any]:
        """è·å–æ¨¡å‹ä¿¡æ¯"""
        if self.components is None:
            self.load_components()
        
        return {
            "model_path": str(self.model_path),
            "device": str(self.device),
            "dtype": str(self.dtype),
            "scheduler_type": self.scheduler_type,
            "unet_config": self.components.unet.config,
            "vae_config": self.components.vae.config,
            "vae_scale_factor": self.get_vae_scale_factor(),
            "text_encoder_max_length": self.components.tokenizer.model_max_length
        }
    
    def free_memory(self):
        """é‡Šæ”¾GPUå†…å­˜"""
        if self.components is not None:
            del self.components
            self.components = None
        
        if self.pipeline is not None:
            del self.pipeline
            self.pipeline = None
        
        if torch.cuda.is_available():
            torch.cuda.empty_cache()
        
        logger.info("GPUå†…å­˜å·²é‡Šæ”¾")


def create_sd_loader(
    model_path: str = "checkpoints/sd2",
    device: str = "auto",
    dtype: Optional[torch.dtype] = None,
    scheduler_type: str = "ddim"
) -> StableDiffusionLoader:
    """
    åˆ›å»ºStable DiffusionåŠ è½½å™¨çš„ä¾¿æ·å‡½æ•°
    
    Args:
        model_path: æ¨¡å‹è·¯å¾„
        device: è¿è¡Œè®¾å¤‡
        dtype: æ•°æ®ç±»å‹
        scheduler_type: è°ƒåº¦å™¨ç±»å‹
        
    Returns:
        StableDiffusionLoader: é…ç½®å¥½çš„æ¨¡å‹åŠ è½½å™¨
    """
    return StableDiffusionLoader(
        model_path=model_path,
        device=device,
        dtype=dtype,
        scheduler_type=scheduler_type
    )


# æµ‹è¯•å‡½æ•°
def test_sd_loader():
    """æµ‹è¯•æ¨¡å‹åŠ è½½å™¨"""
    logger.info("å¼€å§‹æµ‹è¯•Stable DiffusionåŠ è½½å™¨...")
    
    try:
        # åˆ›å»ºåŠ è½½å™¨
        loader = create_sd_loader()
        
        # åŠ è½½ç»„ä»¶
        components = loader.load_components()
        logger.info("âœ… ç»„ä»¶åŠ è½½æµ‹è¯•é€šè¿‡")
        
        # æµ‹è¯•æ–‡æœ¬ç¼–ç 
        text_emb = loader.encode_text("a beautiful landscape")
        logger.info(f"âœ… æ–‡æœ¬ç¼–ç æµ‹è¯•é€šè¿‡: {text_emb.shape}")
        
        # æµ‹è¯•å›¾åƒç¼–ç è§£ç 
        dummy_image = torch.randn(1, 3, 512, 512, dtype=loader.dtype, device=loader.device)
        # ç¡®ä¿å›¾åƒåœ¨[0,1]èŒƒå›´å†…
        dummy_image = torch.clamp(dummy_image * 0.5 + 0.5, 0.0, 1.0)
        latents = loader.encode_images(dummy_image)
        decoded = loader.decode_latents(latents)
        logger.info(f"âœ… å›¾åƒç¼–ç è§£ç æµ‹è¯•é€šè¿‡: {dummy_image.shape} -> {latents.shape} -> {decoded.shape}")
        
        # æ˜¾ç¤ºæ¨¡å‹ä¿¡æ¯
        info = loader.get_model_info()
        logger.info(f"âœ… æ¨¡å‹ä¿¡æ¯: VAEç¼©æ”¾å› å­={info['vae_scale_factor']}")
        
        logger.info("ğŸ‰ æ‰€æœ‰æµ‹è¯•é€šè¿‡ï¼")
        return True
        
    except Exception as e:
        logger.error(f"âŒ æµ‹è¯•å¤±è´¥: {e}")
        return False


if __name__ == "__main__":
    # é…ç½®æ—¥å¿—
    logging.basicConfig(
        level=logging.INFO,
        format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
    )
    
    # è¿è¡Œæµ‹è¯•
    test_sd_loader() 