"""
æµ‹è¯•æ•°æ®é›†åˆ›å»ºè„šæœ¬

è¯¥è„šæœ¬ç”¨äºåˆ›å»ºæ ‡å‡†åŒ–çš„äººè„¸å›¾åƒæµ‹è¯•é›†ï¼Œæ”¯æŒä»å¤šç§æ•°æ®æºæ”¶é›†å›¾åƒï¼Œ
å¹¶è¿›è¡Œæ ‡å‡†åŒ–å¤„ç†ï¼Œä¸ºæ¨¡å‹è¯„ä¼°æä¾›ä¸€è‡´çš„æµ‹è¯•åŸºå‡†ã€‚

åŠŸèƒ½åŒ…æ‹¬ï¼š
1. ä»å…¬å¼€æ•°æ®é›†ä¸‹è½½äººè„¸å›¾åƒ
2. å›¾åƒè´¨é‡æ£€æµ‹å’Œç­›é€‰
3. äººè„¸æ£€æµ‹å’Œè£å‰ª
4. æ•°æ®é›†æ ‡å‡†åŒ–å¤„ç†
5. ç”Ÿæˆæ•°æ®é›†ç»Ÿè®¡æŠ¥å‘Š

ä½œè€…: AI Privacy Protection System
æ—¥æœŸ: 2025-07-28
"""

import os
import requests
import urllib.request
from pathlib import Path
import logging
from typing import List, Dict, Optional, Tuple
import numpy as np
import cv2
from PIL import Image
import json
import shutil
from tqdm import tqdm
import hashlib
from datetime import datetime

try:
    from ..src.utils.image_utils import ImageProcessor
except ImportError:
    import sys
    sys.path.append(str(Path(__file__).parent.parent))
    from src.utils.image_utils import ImageProcessor

logger = logging.getLogger(__name__)

class TestDatasetCreator:
    """æµ‹è¯•æ•°æ®é›†åˆ›å»ºå™¨"""
    
    def __init__(
        self,
        output_dir: str = "data/test_faces",
        target_count: int = 50,
        image_size: Tuple[int, int] = (512, 512),
        min_face_size: int = 128
    ):
        """
        åˆå§‹åŒ–æµ‹è¯•æ•°æ®é›†åˆ›å»ºå™¨
        
        Args:
            output_dir: è¾“å‡ºç›®å½•
            target_count: ç›®æ ‡å›¾åƒæ•°é‡
            image_size: ç›®æ ‡å›¾åƒå°ºå¯¸
            min_face_size: æœ€å°äººè„¸å°ºå¯¸
        """
        self.output_dir = Path(output_dir)
        self.target_count = target_count
        self.image_size = image_size
        self.min_face_size = min_face_size
        
        # åˆ›å»ºç›®å½•ç»“æ„
        self.output_dir.mkdir(parents=True, exist_ok=True)
        self.raw_dir = self.output_dir / "raw"
        self.processed_dir = self.output_dir / "processed"
        self.metadata_dir = self.output_dir / "metadata"
        
        for directory in [self.raw_dir, self.processed_dir, self.metadata_dir]:
            directory.mkdir(exist_ok=True)
        
        # åˆå§‹åŒ–å·¥å…·
        self.image_processor = ImageProcessor()
        
        # äººè„¸æ£€æµ‹å™¨ï¼ˆä½¿ç”¨OpenCVçš„Haarçº§è”ï¼‰
        self.face_cascade = None
        self._init_face_detector()
        
        # æ•°æ®æºé…ç½®
        self.data_sources = self._get_data_sources()
        
        logger.info(f"æµ‹è¯•æ•°æ®é›†åˆ›å»ºå™¨åˆå§‹åŒ–: ç›®æ ‡{target_count}å¼ å›¾åƒ, è¾“å‡ºåˆ° {output_dir}")
    
    def _init_face_detector(self):
        """åˆå§‹åŒ–äººè„¸æ£€æµ‹å™¨"""
        try:
            # å°è¯•åŠ è½½OpenCVçš„äººè„¸æ£€æµ‹å™¨
            cascade_path = cv2.data.haarcascades + 'haarcascade_frontalface_default.xml'
            if os.path.exists(cascade_path):
                self.face_cascade = cv2.CascadeClassifier(cascade_path)
                logger.info("OpenCVäººè„¸æ£€æµ‹å™¨åŠ è½½æˆåŠŸ")
            else:
                logger.warning("OpenCVäººè„¸æ£€æµ‹å™¨ä¸å¯ç”¨ï¼Œå°†ä½¿ç”¨å¤‡ç”¨æ–¹æ¡ˆ")
        except Exception as e:
            logger.warning(f"äººè„¸æ£€æµ‹å™¨åˆå§‹åŒ–å¤±è´¥: {e}")
    
    def _get_data_sources(self) -> Dict[str, Dict]:
        """è·å–æ•°æ®æºé…ç½®"""
        # æ³¨æ„ï¼šè¿™é‡Œæä¾›çš„æ˜¯ä¸€äº›ç¤ºä¾‹å’Œå…¬å¼€å¯ç”¨çš„æµ‹è¯•å›¾åƒ
        # åœ¨å®é™…ä½¿ç”¨ä¸­ï¼Œè¯·ç¡®ä¿éµå®ˆç›¸å…³çš„ç‰ˆæƒå’Œéšç§è§„å®š
        
        sources = {
            "sample_faces": {
                "description": "ç¤ºä¾‹äººè„¸å›¾åƒï¼ˆç”Ÿæˆæˆ–å…¬å¼€å¯ç”¨ï¼‰",
                "urls": [
                    # è¿™é‡Œå¯ä»¥æ·»åŠ ä¸€äº›å…¬å¼€å¯ç”¨çš„æµ‹è¯•å›¾åƒURL
                    # æ³¨æ„ï¼šå®é™…éƒ¨ç½²æ—¶éœ€è¦æ›¿æ¢ä¸ºåˆæ³•çš„æµ‹è¯•å›¾åƒæº
                ],
                "enabled": False  # é»˜è®¤å…³é—­ï¼Œéœ€è¦æ‰‹åŠ¨å¯ç”¨
            },
            "synthetic": {
                "description": "ç”Ÿæˆåˆæˆäººè„¸å›¾åƒ",
                "enabled": True
            }
        }
        
        return sources
    
    def generate_synthetic_faces(self, count: int) -> List[str]:
        """
        ç”Ÿæˆåˆæˆäººè„¸å›¾åƒï¼ˆç”¨äºæµ‹è¯•ï¼‰
        
        Args:
            count: ç”Ÿæˆæ•°é‡
            
        Returns:
            ç”Ÿæˆçš„å›¾åƒè·¯å¾„åˆ—è¡¨
        """
        generated_paths = []
        
        logger.info(f"ç”Ÿæˆ {count} å¼ åˆæˆæµ‹è¯•å›¾åƒ...")
        
        for i in tqdm(range(count), desc="ç”Ÿæˆåˆæˆå›¾åƒ"):
            # åˆ›å»ºä¸€ä¸ªå¸¦æœ‰ç®€å•æ¨¡å¼çš„æµ‹è¯•å›¾åƒ
            # è¿™æ˜¯ä¸€ä¸ªå ä½ç¬¦å®ç°ï¼Œå®é™…ä¸­å¯èƒ½éœ€è¦æ›´å¤æ‚çš„ç”Ÿæˆé€»è¾‘
            
            # ç”Ÿæˆéšæœºå›¾åƒä½œä¸ºåŸºç¡€
            img_array = np.random.randint(0, 256, (self.image_size[1], self.image_size[0], 3), dtype=np.uint8)
            
            # æ·»åŠ ä¸€äº›ç®€å•çš„å‡ ä½•å½¢çŠ¶æ¨¡æ‹Ÿäººè„¸ç‰¹å¾
            center_x, center_y = self.image_size[0] // 2, self.image_size[1] // 2
            
            # ç»˜åˆ¶æ¤­åœ†å½¢"è„¸éƒ¨"è½®å»“
            cv2.ellipse(img_array, (center_x, center_y), (80, 100), 0, 0, 360, (200, 180, 160), -1)
            
            # æ·»åŠ "çœ¼ç›"
            cv2.circle(img_array, (center_x - 25, center_y - 20), 8, (50, 50, 50), -1)
            cv2.circle(img_array, (center_x + 25, center_y - 20), 8, (50, 50, 50), -1)
            
            # æ·»åŠ "å˜´å·´"
            cv2.ellipse(img_array, (center_x, center_y + 30), (20, 10), 0, 0, 180, (100, 50, 50), -1)
            
            # æ·»åŠ ä¸€äº›å™ªå£°ä»¥å¢åŠ çœŸå®æ„Ÿ
            noise = np.random.normal(0, 25, img_array.shape).astype(np.uint8)
            img_array = cv2.add(img_array, noise)
            
            # ä¿å­˜å›¾åƒ
            filename = f"synthetic_face_{i:03d}.png"
            filepath = self.raw_dir / filename
            
            cv2.imwrite(str(filepath), img_array)
            generated_paths.append(str(filepath))
        
        logger.info(f"æˆåŠŸç”Ÿæˆ {len(generated_paths)} å¼ åˆæˆå›¾åƒ")
        return generated_paths
    
    def download_from_urls(self, urls: List[str]) -> List[str]:
        """
        ä»URLåˆ—è¡¨ä¸‹è½½å›¾åƒ
        
        Args:
            urls: URLåˆ—è¡¨
            
        Returns:
            ä¸‹è½½çš„å›¾åƒè·¯å¾„åˆ—è¡¨
        """
        downloaded_paths = []
        
        for i, url in enumerate(tqdm(urls, desc="ä¸‹è½½å›¾åƒ")):
            try:
                # ç”Ÿæˆæ–‡ä»¶å
                url_hash = hashlib.md5(url.encode()).hexdigest()[:8]
                filename = f"downloaded_{i:03d}_{url_hash}.jpg"
                filepath = self.raw_dir / filename
                
                # ä¸‹è½½å›¾åƒ
                response = requests.get(url, timeout=10, stream=True)
                response.raise_for_status()
                
                with open(filepath, 'wb') as f:
                    for chunk in response.iter_content(chunk_size=8192):
                        f.write(chunk)
                
                downloaded_paths.append(str(filepath))
                logger.info(f"ä¸‹è½½æˆåŠŸ: {filename}")
                
            except Exception as e:
                logger.warning(f"ä¸‹è½½å¤±è´¥ {url}: {e}")
                continue
        
        return downloaded_paths
    
    def detect_faces(self, image_path: str) -> List[Tuple[int, int, int, int]]:
        """
        æ£€æµ‹å›¾åƒä¸­çš„äººè„¸
        
        Args:
            image_path: å›¾åƒè·¯å¾„
            
        Returns:
            äººè„¸è¾¹ç•Œæ¡†åˆ—è¡¨ [(x, y, w, h), ...]
        """
        try:
            # è¯»å–å›¾åƒ
            img = cv2.imread(image_path)
            if img is None:
                return []
            
            # è½¬æ¢ä¸ºç°åº¦å›¾
            gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)
            
            # ä½¿ç”¨äººè„¸æ£€æµ‹å™¨
            if self.face_cascade is not None:
                faces = self.face_cascade.detectMultiScale(
                    gray,
                    scaleFactor=1.1,
                    minNeighbors=5,
                    minSize=(self.min_face_size, self.min_face_size)
                )
                # æ£€æŸ¥facesæ˜¯å¦ä¸ºç©ºæˆ–è€…ä¸æ˜¯numpyæ•°ç»„
                if len(faces) > 0 and hasattr(faces, 'tolist'):
                    return faces.tolist()
                else:
                    # å¤‡ç”¨æ–¹æ¡ˆï¼šå‡è®¾æ•´ä¸ªå›¾åƒæ˜¯äººè„¸
                    h, w = img.shape[:2]
                    return [(0, 0, w, h)]
            else:
                # å¤‡ç”¨æ–¹æ¡ˆï¼šå‡è®¾æ•´ä¸ªå›¾åƒæ˜¯äººè„¸
                h, w = img.shape[:2]
                return [(0, 0, w, h)]
                
        except Exception as e:
            logger.warning(f"äººè„¸æ£€æµ‹å¤±è´¥ {image_path}: {e}")
            return []
    
    def process_image(self, input_path: str, output_path: str) -> bool:
        """
        å¤„ç†å•å¼ å›¾åƒ
        
        Args:
            input_path: è¾“å…¥å›¾åƒè·¯å¾„
            output_path: è¾“å‡ºå›¾åƒè·¯å¾„
            
        Returns:
            å¤„ç†æ˜¯å¦æˆåŠŸ
        """
        try:
            # æ£€æµ‹äººè„¸
            faces = self.detect_faces(input_path)
            
            if not faces:
                logger.warning(f"æœªæ£€æµ‹åˆ°äººè„¸: {input_path}")
                return False
            
            # ä½¿ç”¨ç¬¬ä¸€ä¸ªæ£€æµ‹åˆ°çš„äººè„¸
            x, y, w, h = faces[0]
            
            # è¯»å–å›¾åƒ
            img = cv2.imread(input_path)
            if img is None:
                return False
            
            # è£å‰ªäººè„¸åŒºåŸŸï¼ˆæ·»åŠ ä¸€äº›è¾¹è·ï¼‰
            margin = max(w, h) // 4
            x1 = max(0, x - margin)
            y1 = max(0, y - margin)
            x2 = min(img.shape[1], x + w + margin)
            y2 = min(img.shape[0], y + h + margin)
            
            face_img = img[y1:y2, x1:x2]
            
            # è°ƒæ•´å°ºå¯¸
            resized_img = cv2.resize(face_img, self.image_size)
            
            # ä¿å­˜å¤„ç†åçš„å›¾åƒ
            cv2.imwrite(output_path, resized_img)
            
            return True
            
        except Exception as e:
            logger.error(f"å›¾åƒå¤„ç†å¤±è´¥ {input_path}: {e}")
            return False
    
    def create_dataset(self) -> Dict[str, any]:
        """
        åˆ›å»ºæµ‹è¯•æ•°æ®é›†
        
        Returns:
            æ•°æ®é›†ç»Ÿè®¡ä¿¡æ¯
        """
        logger.info("å¼€å§‹åˆ›å»ºæµ‹è¯•æ•°æ®é›†...")
        
        # æ”¶é›†åŸå§‹å›¾åƒ
        raw_images = []
        
        # 1. ç”Ÿæˆåˆæˆå›¾åƒ
        if self.data_sources["synthetic"]["enabled"]:
            synthetic_count = min(self.target_count, 30)  # é™åˆ¶åˆæˆå›¾åƒæ•°é‡
            synthetic_images = self.generate_synthetic_faces(synthetic_count)
            raw_images.extend(synthetic_images)
        
        # 2. ä»URLä¸‹è½½ï¼ˆå¦‚æœé…ç½®äº†ï¼‰
        for source_name, config in self.data_sources.items():
            if source_name != "synthetic" and config.get("enabled", False):
                urls = config.get("urls", [])
                if urls:
                    downloaded_images = self.download_from_urls(urls)
                    raw_images.extend(downloaded_images)
        
        logger.info(f"æ”¶é›†åˆ° {len(raw_images)} å¼ åŸå§‹å›¾åƒ")
        
        # å¤„ç†å›¾åƒ
        processed_count = 0
        failed_count = 0
        
        for i, raw_path in enumerate(tqdm(raw_images, desc="å¤„ç†å›¾åƒ")):
            if processed_count >= self.target_count:
                break
            
            output_filename = f"face_{processed_count:03d}.png"
            output_path = str(self.processed_dir / output_filename)
            
            if self.process_image(raw_path, output_path):
                processed_count += 1
            else:
                failed_count += 1
        
        # ç”Ÿæˆæ•°æ®é›†ç»Ÿè®¡ä¿¡æ¯
        stats = {
            "dataset_name": "privacy_protection_test_faces",
            "creation_date": datetime.now().isoformat(),
            "total_images": processed_count,
            "image_size": self.image_size,
            "raw_images_collected": len(raw_images),
            "processing_success_rate": processed_count / len(raw_images) if raw_images else 0,
            "failed_processing": failed_count,
            "data_sources": {k: v for k, v in self.data_sources.items() if v.get("enabled", False)}
        }
        
        # ä¿å­˜ç»Ÿè®¡ä¿¡æ¯
        stats_path = self.metadata_dir / "dataset_stats.json"
        with open(stats_path, 'w', encoding='utf-8') as f:
            json.dump(stats, f, indent=2, ensure_ascii=False)
        
        # åˆ›å»ºæ•°æ®é›†ç´¢å¼•
        self._create_dataset_index()
        
        logger.info(f"æ•°æ®é›†åˆ›å»ºå®Œæˆ: {processed_count} å¼ å›¾åƒ")
        logger.info(f"ç»Ÿè®¡ä¿¡æ¯å·²ä¿å­˜åˆ°: {stats_path}")
        
        return stats
    
    def _create_dataset_index(self):
        """åˆ›å»ºæ•°æ®é›†ç´¢å¼•æ–‡ä»¶"""
        index = {
            "version": "1.0",
            "description": "Privacy Protection Test Face Dataset",
            "images": []
        }
        
        # éå†å¤„ç†åçš„å›¾åƒ
        for img_path in sorted(self.processed_dir.glob("*.png")):
            try:
                # è·å–å›¾åƒä¿¡æ¯
                img = Image.open(img_path)
                width, height = img.size
                
                # è®¡ç®—æ–‡ä»¶å“ˆå¸Œ
                with open(img_path, 'rb') as f:
                    file_hash = hashlib.md5(f.read()).hexdigest()
                
                image_info = {
                    "filename": img_path.name,
                    "path": str(img_path.relative_to(self.output_dir)),
                    "size": [width, height],
                    "file_size": img_path.stat().st_size,
                    "md5_hash": file_hash
                }
                
                index["images"].append(image_info)
                
            except Exception as e:
                logger.warning(f"å¤„ç†ç´¢å¼•æ—¶å‡ºé”™ {img_path}: {e}")
        
        # ä¿å­˜ç´¢å¼•æ–‡ä»¶
        index_path = self.metadata_dir / "dataset_index.json"
        with open(index_path, 'w', encoding='utf-8') as f:
            json.dump(index, f, indent=2, ensure_ascii=False)
        
        logger.info(f"æ•°æ®é›†ç´¢å¼•å·²ä¿å­˜åˆ°: {index_path}")
    
    def validate_dataset(self) -> Dict[str, any]:
        """
        éªŒè¯æ•°æ®é›†è´¨é‡
        
        Returns:
            éªŒè¯ç»“æœ
        """
        logger.info("å¼€å§‹éªŒè¯æ•°æ®é›†...")
        
        validation_results = {
            "total_images": 0,
            "valid_images": 0,
            "corrupt_images": 0,
            "size_issues": 0,
            "face_detection_success": 0,
            "issues": []
        }
        
        for img_path in self.processed_dir.glob("*.png"):
            validation_results["total_images"] += 1
            
            try:
                # æ£€æŸ¥å›¾åƒæ˜¯å¦å¯ä»¥æ­£å¸¸è¯»å–
                img = Image.open(img_path)
                width, height = img.size
                
                # æ£€æŸ¥å°ºå¯¸
                if (width, height) != self.image_size:
                    validation_results["size_issues"] += 1
                    validation_results["issues"].append(f"å°ºå¯¸é”™è¯¯: {img_path.name} ({width}x{height})")
                
                # æ£€æŸ¥äººè„¸æ£€æµ‹
                faces = self.detect_faces(str(img_path))
                if faces:
                    validation_results["face_detection_success"] += 1
                else:
                    validation_results["issues"].append(f"äººè„¸æ£€æµ‹å¤±è´¥: {img_path.name}")
                
                validation_results["valid_images"] += 1
                
            except Exception as e:
                validation_results["corrupt_images"] += 1
                validation_results["issues"].append(f"å›¾åƒæŸå: {img_path.name} - {e}")
        
        # ä¿å­˜éªŒè¯ç»“æœ
        validation_path = self.metadata_dir / "validation_results.json"
        with open(validation_path, 'w', encoding='utf-8') as f:
            json.dump(validation_results, f, indent=2, ensure_ascii=False)
        
        logger.info(f"æ•°æ®é›†éªŒè¯å®Œæˆ: {validation_results['valid_images']}/{validation_results['total_images']} æœ‰æ•ˆ")
        
        return validation_results

def create_test_dataset(
    target_count: int = 50,
    output_dir: str = "data/test_faces",
    **kwargs
) -> Dict[str, any]:
    """
    åˆ›å»ºæµ‹è¯•æ•°æ®é›†çš„ä¾¿æ·å‡½æ•°
    
    Args:
        target_count: ç›®æ ‡å›¾åƒæ•°é‡
        output_dir: è¾“å‡ºç›®å½•
        **kwargs: å…¶ä»–å‚æ•°
        
    Returns:
        æ•°æ®é›†ç»Ÿè®¡ä¿¡æ¯
    """
    creator = TestDatasetCreator(
        output_dir=output_dir,
        target_count=target_count,
        **kwargs
    )
    
    stats = creator.create_dataset()
    validation_results = creator.validate_dataset()
    
    # åˆå¹¶ç»“æœ
    stats["validation"] = validation_results
    
    return stats

def main():
    """ä¸»å‡½æ•°"""
    print("ğŸ—‚ï¸ åˆ›å»ºæµ‹è¯•äººè„¸æ•°æ®é›†...")
    
    try:
        # åˆ›å»ºæ•°æ®é›†
        stats = create_test_dataset(
            target_count=50,
            output_dir="data/test_faces"
        )
        
        print("âœ… æµ‹è¯•æ•°æ®é›†åˆ›å»ºå®Œæˆï¼")
        print(f"ğŸ“Š ç»Ÿè®¡ä¿¡æ¯:")
        print(f"   æ€»å›¾åƒæ•°: {stats['total_images']}")
        print(f"   å›¾åƒå°ºå¯¸: {stats['image_size']}")
        print(f"   å¤„ç†æˆåŠŸç‡: {stats['processing_success_rate']:.2%}")
        print(f"   éªŒè¯ç»“æœ: {stats['validation']['valid_images']}/{stats['validation']['total_images']} æœ‰æ•ˆ")
        
        if stats['validation']['issues']:
            print(f"âš ï¸ å‘ç° {len(stats['validation']['issues'])} ä¸ªé—®é¢˜ï¼Œè¯¦è§éªŒè¯æŠ¥å‘Š")
        
        print(f"ğŸ“ æ•°æ®é›†ä½ç½®: {Path('data/test_faces').absolute()}")
        
        return True
        
    except Exception as e:
        print(f"âŒ åˆ›å»ºå¤±è´¥: {e}")
        import traceback
        traceback.print_exc()
        return False

if __name__ == "__main__":
    main() 