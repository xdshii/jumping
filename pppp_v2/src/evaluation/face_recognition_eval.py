"""
å¤šæ¨¡å‹é¢éƒ¨è¯†åˆ«è¯„ä¼°ç³»ç»Ÿ

è¯¥æ¨¡å—æä¾›å¯¹å¤šä¸ªé¢éƒ¨è¯†åˆ«æ¨¡å‹çš„ç»Ÿä¸€è¯„ä¼°æ¥å£ï¼Œç”¨äºæµ‹è¯•
éšç§ä¿æŠ¤ç®—æ³•å¯¹ä¸åŒè¯†åˆ«æ¨¡å‹çš„æœ‰æ•ˆæ€§å’Œå¯è½¬ç§»æ€§ã€‚

åŠŸèƒ½åŒ…æ‹¬ï¼š
1. æ”¯æŒå¤šç§é¢éƒ¨è¯†åˆ«æ¨¡å‹ (ArcFace, FaceNet, etc.)
2. æ‰¹é‡ç‰¹å¾æå–å’Œç›¸ä¼¼åº¦è®¡ç®—
3. å¯è½¬ç§»æ€§è¯„ä¼°å’Œå¯¹æ¯”åˆ†æ
4. è¯¦ç»†çš„è¯„ä¼°æŠ¥å‘Šç”Ÿæˆ
5. æ¨¡å‹æ€§èƒ½åŸºå‡†æµ‹è¯•

ä½œè€…: AI Privacy Protection System
æ—¥æœŸ: 2025-07-28
"""

import logging
from typing import List, Dict, Optional, Tuple, Any, Union
from pathlib import Path
import numpy as np
import torch
import torch.nn.functional as F
from PIL import Image
import cv2
import json
from datetime import datetime
from dataclasses import dataclass
from collections import defaultdict
import matplotlib.pyplot as plt
import seaborn as sns
from tqdm import tqdm

try:
    from ..losses.id_loss import IdentityLoss, create_identity_loss
    from ..utils.image_utils import ImageProcessor
except ImportError:
    import sys
    from pathlib import Path
    sys.path.append(str(Path(__file__).parent.parent))
    from losses.id_loss import IdentityLoss, create_identity_loss
    from utils.image_utils import ImageProcessor

logger = logging.getLogger(__name__)

@dataclass
class EvaluationResult:
    """è¯„ä¼°ç»“æœæ•°æ®ç±»"""
    model_name: str
    original_features: np.ndarray
    protected_features: np.ndarray
    similarities: np.ndarray
    distances: np.ndarray
    mean_similarity: float
    mean_distance: float
    std_similarity: float
    std_distance: float
    protection_rate: float  # ä½äºé˜ˆå€¼çš„æ¯”ä¾‹
    metadata: Optional[Dict[str, Any]] = None

class FaceRecognitionEvaluator:
    """é¢éƒ¨è¯†åˆ«è¯„ä¼°å™¨"""
    
    def __init__(
        self,
        model_types: List[str] = None,
        device: str = None,
        similarity_threshold: float = 0.6,
        distance_metric: str = "cosine"
    ):
        """
        åˆå§‹åŒ–é¢éƒ¨è¯†åˆ«è¯„ä¼°å™¨
        
        Args:
            model_types: æ¨¡å‹ç±»å‹åˆ—è¡¨
            device: è®¾å¤‡
            similarity_threshold: ç›¸ä¼¼åº¦é˜ˆå€¼
            distance_metric: è·ç¦»åº¦é‡æ–¹å¼
        """
        self.model_types = model_types or ["arcface", "facenet"]
        self.device = device or ("cuda" if torch.cuda.is_available() else "cpu")
        self.similarity_threshold = similarity_threshold
        self.distance_metric = distance_metric
        
        # åˆå§‹åŒ–å·¥å…·
        self.image_processor = ImageProcessor()
        self.models = {}
        
        # åŠ è½½æ¨¡å‹
        self._load_models()
        
        logger.info(f"é¢éƒ¨è¯†åˆ«è¯„ä¼°å™¨åˆå§‹åŒ–: {len(self.models)}ä¸ªæ¨¡å‹, è®¾å¤‡={self.device}")
    
    def _load_models(self):
        """åŠ è½½é¢éƒ¨è¯†åˆ«æ¨¡å‹"""
        for model_type in self.model_types:
            try:
                if model_type.lower() in ["arcface", "facenet"]:
                    # ä½¿ç”¨ç°æœ‰çš„IdentityLossæ¨¡å‹
                    model = create_identity_loss(
                        model_types=[model_type.lower()],
                        device=self.device,
                        fallback_to_l2=False
                    )
                    self.models[model_type] = model
                    logger.info(f"åŠ è½½æ¨¡å‹æˆåŠŸ: {model_type}")
                else:
                    logger.warning(f"ä¸æ”¯æŒçš„æ¨¡å‹ç±»å‹: {model_type}")
            except Exception as e:
                logger.error(f"åŠ è½½æ¨¡å‹å¤±è´¥ {model_type}: {e}")
    
    def extract_features(
        self,
        images: Union[torch.Tensor, List[str], str],
        model_name: str
    ) -> np.ndarray:
        """
        æå–å›¾åƒç‰¹å¾
        
        Args:
            images: å›¾åƒå¼ é‡ã€è·¯å¾„åˆ—è¡¨æˆ–å•ä¸ªè·¯å¾„
            model_name: æ¨¡å‹åç§°
            
        Returns:
            ç‰¹å¾å‘é‡æ•°ç»„
        """
        if model_name not in self.models:
            raise ValueError(f"æ¨¡å‹ {model_name} æœªåŠ è½½")
        
        model = self.models[model_name]
        
        # å¤„ç†è¾“å…¥
        if isinstance(images, str):
            # å•ä¸ªå›¾åƒè·¯å¾„
            img_tensor = self.image_processor.load_image(images)
            img_tensor = img_tensor.unsqueeze(0).to(self.device)
        elif isinstance(images, list):
            # å›¾åƒè·¯å¾„åˆ—è¡¨
            img_tensors = []
            for img_path in images:
                img_tensor = self.image_processor.load_image(img_path)
                img_tensors.append(img_tensor)
            img_tensor = torch.stack(img_tensors).to(self.device)
        else:
            # å·²ç»æ˜¯å¼ é‡
            img_tensor = images
            if img_tensor.device != self.device:
                img_tensor = img_tensor.to(self.device)
        
        # æå–ç‰¹å¾
        with torch.no_grad():
            if model_name.lower() == "arcface" and "arcface" in model.extractors:
                features = model.extractors["arcface"].extract_features(img_tensor)
            elif model_name.lower() == "facenet" and "facenet" in model.extractors:
                features = model.extractors["facenet"].extract_features(img_tensor)
            else:
                raise ValueError(f"æ¨¡å‹ {model_name} ä¸å¯ç”¨æˆ–ä¸æ”¯æŒ")
        
        return features.cpu().numpy()
    
    def compute_similarities(
        self,
        features1: np.ndarray,
        features2: np.ndarray,
        metric: str = None
    ) -> np.ndarray:
        """
        è®¡ç®—ç‰¹å¾ç›¸ä¼¼åº¦
        
        Args:
            features1: ç¬¬ä¸€ç»„ç‰¹å¾
            features2: ç¬¬äºŒç»„ç‰¹å¾
            metric: è·ç¦»åº¦é‡æ–¹å¼
            
        Returns:
            ç›¸ä¼¼åº¦æ•°ç»„
        """
        if metric is None:
            metric = self.distance_metric
        
        if metric == "cosine":
            # ä½™å¼¦ç›¸ä¼¼åº¦
            features1_norm = features1 / np.linalg.norm(features1, axis=1, keepdims=True)
            features2_norm = features2 / np.linalg.norm(features2, axis=1, keepdims=True)
            similarities = np.sum(features1_norm * features2_norm, axis=1)
        elif metric == "euclidean":
            # æ¬§æ°è·ç¦»è½¬ç›¸ä¼¼åº¦
            distances = np.linalg.norm(features1 - features2, axis=1)
            similarities = 1.0 / (1.0 + distances)
        elif metric == "manhattan":
            # æ›¼å“ˆé¡¿è·ç¦»è½¬ç›¸ä¼¼åº¦
            distances = np.sum(np.abs(features1 - features2), axis=1)
            similarities = 1.0 / (1.0 + distances)
        else:
            raise ValueError(f"ä¸æ”¯æŒçš„è·ç¦»åº¦é‡: {metric}")
        
        return similarities
    
    def compute_distances(
        self,
        features1: np.ndarray,
        features2: np.ndarray,
        metric: str = None
    ) -> np.ndarray:
        """
        è®¡ç®—ç‰¹å¾è·ç¦»
        
        Args:
            features1: ç¬¬ä¸€ç»„ç‰¹å¾
            features2: ç¬¬äºŒç»„ç‰¹å¾
            metric: è·ç¦»åº¦é‡æ–¹å¼
            
        Returns:
            è·ç¦»æ•°ç»„
        """
        if metric is None:
            metric = self.distance_metric
        
        if metric == "cosine":
            # ä½™å¼¦è·ç¦»
            similarities = self.compute_similarities(features1, features2, "cosine")
            distances = 1.0 - similarities
        elif metric == "euclidean":
            # æ¬§æ°è·ç¦»
            distances = np.linalg.norm(features1 - features2, axis=1)
        elif metric == "manhattan":
            # æ›¼å“ˆé¡¿è·ç¦»
            distances = np.sum(np.abs(features1 - features2), axis=1)
        else:
            raise ValueError(f"ä¸æ”¯æŒçš„è·ç¦»åº¦é‡: {metric}")
        
        return distances
    
    def evaluate_single_model(
        self,
        original_images: Union[torch.Tensor, List[str]],
        protected_images: Union[torch.Tensor, List[str]],
        model_name: str
    ) -> EvaluationResult:
        """
        è¯„ä¼°å•ä¸ªæ¨¡å‹
        
        Args:
            original_images: åŸå§‹å›¾åƒ
            protected_images: ä¿æŠ¤å›¾åƒ
            model_name: æ¨¡å‹åç§°
            
        Returns:
            è¯„ä¼°ç»“æœ
        """
        logger.info(f"å¼€å§‹è¯„ä¼°æ¨¡å‹: {model_name}")
        
        # æå–ç‰¹å¾
        original_features = self.extract_features(original_images, model_name)
        protected_features = self.extract_features(protected_images, model_name)
        
        # è®¡ç®—ç›¸ä¼¼åº¦å’Œè·ç¦»
        similarities = self.compute_similarities(original_features, protected_features)
        distances = self.compute_distances(original_features, protected_features)
        
        # è®¡ç®—ç»Ÿè®¡æŒ‡æ ‡
        mean_similarity = np.mean(similarities)
        mean_distance = np.mean(distances)
        std_similarity = np.std(similarities)
        std_distance = np.std(distances)
        
        # è®¡ç®—ä¿æŠ¤ç‡ï¼ˆä½äºé˜ˆå€¼çš„æ¯”ä¾‹ï¼‰
        protection_rate = np.mean(similarities < self.similarity_threshold)
        
        result = EvaluationResult(
            model_name=model_name,
            original_features=original_features,
            protected_features=protected_features,
            similarities=similarities,
            distances=distances,
            mean_similarity=mean_similarity,
            mean_distance=mean_distance,
            std_similarity=std_similarity,
            std_distance=std_distance,
            protection_rate=protection_rate,
            metadata={
                "threshold": self.similarity_threshold,
                "metric": self.distance_metric,
                "num_samples": len(similarities)
            }
        )
        
        logger.info(f"æ¨¡å‹ {model_name} è¯„ä¼°å®Œæˆ: å¹³å‡ç›¸ä¼¼åº¦={mean_similarity:.4f}, ä¿æŠ¤ç‡={protection_rate:.2%}")
        
        return result
    
    def evaluate_all_models(
        self,
        original_images: Union[torch.Tensor, List[str]],
        protected_images: Union[torch.Tensor, List[str]]
    ) -> Dict[str, EvaluationResult]:
        """
        è¯„ä¼°æ‰€æœ‰æ¨¡å‹
        
        Args:
            original_images: åŸå§‹å›¾åƒ
            protected_images: ä¿æŠ¤å›¾åƒ
            
        Returns:
            æ‰€æœ‰æ¨¡å‹çš„è¯„ä¼°ç»“æœ
        """
        results = {}
        
        for model_name in self.models.keys():
            try:
                result = self.evaluate_single_model(
                    original_images, protected_images, model_name
                )
                results[model_name] = result
            except Exception as e:
                logger.error(f"è¯„ä¼°æ¨¡å‹å¤±è´¥ {model_name}: {e}")
                continue
        
        return results
    
    def analyze_transferability(
        self,
        results: Dict[str, EvaluationResult]
    ) -> Dict[str, Any]:
        """
        åˆ†æå¯è½¬ç§»æ€§
        
        Args:
            results: è¯„ä¼°ç»“æœå­—å…¸
            
        Returns:
            å¯è½¬ç§»æ€§åˆ†æç»“æœ
        """
        if len(results) < 2:
            logger.warning("è‡³å°‘éœ€è¦2ä¸ªæ¨¡å‹æ¥åˆ†æå¯è½¬ç§»æ€§")
            return {}
        
        transferability_analysis = {
            "model_comparison": {},
            "cross_model_correlation": {},
            "overall_transferability": 0.0,
            "best_model": "",
            "worst_model": "",
            "consistency_score": 0.0
        }
        
        # æå–å„æ¨¡å‹çš„ä¿æŠ¤ç‡
        protection_rates = {name: result.protection_rate for name, result in results.items()}
        similarities = {name: result.similarities for name, result in results.items()}
        
        # æ‰¾å‡ºæœ€å¥½å’Œæœ€å·®çš„æ¨¡å‹
        best_model = max(protection_rates.keys(), key=lambda k: protection_rates[k])
        worst_model = min(protection_rates.keys(), key=lambda k: protection_rates[k])
        
        transferability_analysis["best_model"] = best_model
        transferability_analysis["worst_model"] = worst_model
        transferability_analysis["overall_transferability"] = np.mean(list(protection_rates.values()))
        
        # è®¡ç®—æ¨¡å‹é—´ç›¸ä¼¼åº¦ç›¸å…³æ€§
        model_names = list(similarities.keys())
        correlation_matrix = np.zeros((len(model_names), len(model_names)))
        
        for i, model1 in enumerate(model_names):
            for j, model2 in enumerate(model_names):
                if i == j:
                    correlation_matrix[i, j] = 1.0
                else:
                    corr = np.corrcoef(similarities[model1], similarities[model2])[0, 1]
                    correlation_matrix[i, j] = corr
        
        transferability_analysis["cross_model_correlation"] = {
            "matrix": correlation_matrix.tolist(),
            "model_names": model_names,
            "mean_correlation": np.mean(correlation_matrix[np.triu_indices_from(correlation_matrix, k=1)])
        }
        
        # è®¡ç®—ä¸€è‡´æ€§åˆ†æ•°ï¼ˆæ‰€æœ‰æ¨¡å‹ä¿æŠ¤ç‡çš„æ ‡å‡†å·®ï¼Œè¶Šå°è¶Šä¸€è‡´ï¼‰
        consistency_score = 1.0 / (1.0 + np.std(list(protection_rates.values())))
        transferability_analysis["consistency_score"] = consistency_score
        
        # æ¨¡å‹å¯¹æ¯”
        for name, result in results.items():
            transferability_analysis["model_comparison"][name] = {
                "protection_rate": result.protection_rate,
                "mean_similarity": result.mean_similarity,
                "mean_distance": result.mean_distance,
                "std_similarity": result.std_similarity,
                "rank": sorted(protection_rates.keys(), key=lambda k: protection_rates[k], reverse=True).index(name) + 1
            }
        
        return transferability_analysis
    
    def generate_report(
        self,
        results: Dict[str, EvaluationResult],
        transferability_analysis: Dict[str, Any],
        save_path: Optional[str] = None
    ) -> Dict[str, Any]:
        """
        ç”Ÿæˆè¯„ä¼°æŠ¥å‘Š
        
        Args:
            results: è¯„ä¼°ç»“æœ
            transferability_analysis: å¯è½¬ç§»æ€§åˆ†æ
            save_path: ä¿å­˜è·¯å¾„
            
        Returns:
            å®Œæ•´æŠ¥å‘Š
        """
        report = {
            "metadata": {
                "timestamp": datetime.now().isoformat(),
                "evaluator_config": {
                    "models": list(self.models.keys()),
                    "device": self.device,
                    "similarity_threshold": self.similarity_threshold,
                    "distance_metric": self.distance_metric
                },
                "num_samples": len(next(iter(results.values())).similarities) if results else 0
            },
            "individual_results": {},
            "transferability_analysis": transferability_analysis,
            "summary": {}
        }
        
        # ä¸ªåˆ«æ¨¡å‹ç»“æœ
        for name, result in results.items():
            report["individual_results"][name] = {
                "protection_rate": result.protection_rate,
                "mean_similarity": result.mean_similarity,
                "mean_distance": result.mean_distance,
                "std_similarity": result.std_similarity,
                "std_distance": result.std_distance,
                "metadata": result.metadata
            }
        
        # æ‘˜è¦ç»Ÿè®¡
        if results:
            protection_rates = [r.protection_rate for r in results.values()]
            similarities = [r.mean_similarity for r in results.values()]
            
            report["summary"] = {
                "total_models_evaluated": len(results),
                "average_protection_rate": np.mean(protection_rates),
                "min_protection_rate": np.min(protection_rates),
                "max_protection_rate": np.max(protection_rates),
                "std_protection_rate": np.std(protection_rates),
                "average_similarity": np.mean(similarities),
                "overall_effectiveness": "High" if np.mean(protection_rates) > 0.7 else "Medium" if np.mean(protection_rates) > 0.4 else "Low"
            }
        
        # ä¿å­˜æŠ¥å‘Š
        if save_path:
            with open(save_path, 'w', encoding='utf-8') as f:
                json.dump(report, f, indent=2, ensure_ascii=False)
            logger.info(f"è¯„ä¼°æŠ¥å‘Šå·²ä¿å­˜åˆ°: {save_path}")
        
        return report
    
    def plot_results(
        self,
        results: Dict[str, EvaluationResult],
        save_dir: Optional[str] = None
    ):
        """
        ç»˜åˆ¶è¯„ä¼°ç»“æœ
        
        Args:
            results: è¯„ä¼°ç»“æœ
            save_dir: ä¿å­˜ç›®å½•
        """
        if not results:
            logger.warning("æ²¡æœ‰ç»“æœå¯ç»˜åˆ¶")
            return
        
        fig, axes = plt.subplots(2, 2, figsize=(15, 12))
        
        # 1. ä¿æŠ¤ç‡å¯¹æ¯”
        model_names = list(results.keys())
        protection_rates = [results[name].protection_rate for name in model_names]
        
        axes[0, 0].bar(model_names, protection_rates)
        axes[0, 0].set_title('Protection Rate by Model')
        axes[0, 0].set_ylabel('Protection Rate')
        axes[0, 0].set_ylim(0, 1)
        for i, rate in enumerate(protection_rates):
            axes[0, 0].text(i, rate + 0.01, f'{rate:.2%}', ha='center')
        
        # 2. ç›¸ä¼¼åº¦åˆ†å¸ƒ
        for name, result in results.items():
            axes[0, 1].hist(result.similarities, alpha=0.7, label=name, bins=20)
        axes[0, 1].axvline(self.similarity_threshold, color='red', linestyle='--', label='Threshold')
        axes[0, 1].set_title('Similarity Distribution')
        axes[0, 1].set_xlabel('Similarity')
        axes[0, 1].set_ylabel('Frequency')
        axes[0, 1].legend()
        
        # 3. è·ç¦»åˆ†å¸ƒ
        for name, result in results.items():
            axes[1, 0].hist(result.distances, alpha=0.7, label=name, bins=20)
        axes[1, 0].set_title('Distance Distribution')
        axes[1, 0].set_xlabel('Distance')
        axes[1, 0].set_ylabel('Frequency')
        axes[1, 0].legend()
        
        # 4. æ¨¡å‹å¯¹æ¯”é›·è¾¾å›¾
        if len(results) >= 2:
            # é€‰æ‹©å‰ä¸¤ä¸ªæ¨¡å‹è¿›è¡Œå¯¹æ¯”
            model1, model2 = list(results.keys())[:2]
            result1, result2 = results[model1], results[model2]
            
            categories = ['Protection Rate', 'Mean Distance', '1-Mean Similarity', 'Consistency']
            values1 = [
                result1.protection_rate,
                min(result1.mean_distance, 2.0) / 2.0,  # å½’ä¸€åŒ–åˆ°[0,1]
                1 - result1.mean_similarity,
                1 - result1.std_similarity
            ]
            values2 = [
                result2.protection_rate,
                min(result2.mean_distance, 2.0) / 2.0,
                1 - result2.mean_similarity,
                1 - result2.std_similarity
            ]
            
            x = np.arange(len(categories))
            width = 0.35
            
            axes[1, 1].bar(x - width/2, values1, width, label=model1)
            axes[1, 1].bar(x + width/2, values2, width, label=model2)
            axes[1, 1].set_title('Model Comparison')
            axes[1, 1].set_xticks(x)
            axes[1, 1].set_xticklabels(categories, rotation=45)
            axes[1, 1].legend()
        
        plt.tight_layout()
        
        if save_dir:
            save_path = Path(save_dir) / "evaluation_results.png"
            plt.savefig(save_path, dpi=300, bbox_inches='tight')
            logger.info(f"ç»“æœå›¾è¡¨å·²ä¿å­˜åˆ°: {save_path}")
        
        plt.close()

def create_face_recognition_evaluator(
    model_types: List[str] = None,
    device: str = None,
    **kwargs
) -> FaceRecognitionEvaluator:
    """
    åˆ›å»ºé¢éƒ¨è¯†åˆ«è¯„ä¼°å™¨çš„ä¾¿æ·å‡½æ•°
    
    Args:
        model_types: æ¨¡å‹ç±»å‹åˆ—è¡¨
        device: è®¾å¤‡
        **kwargs: å…¶ä»–å‚æ•°
        
    Returns:
        è¯„ä¼°å™¨å®ä¾‹
    """
    return FaceRecognitionEvaluator(
        model_types=model_types,
        device=device,
        **kwargs
    )

def test_face_recognition_evaluator():
    """æµ‹è¯•é¢éƒ¨è¯†åˆ«è¯„ä¼°å™¨"""
    print("ğŸ§ª æµ‹è¯•é¢éƒ¨è¯†åˆ«è¯„ä¼°å™¨...")
    
    try:
        # åˆ›å»ºè¯„ä¼°å™¨
        evaluator = create_face_recognition_evaluator(
            model_types=["arcface"],  # å…ˆæµ‹è¯•ä¸€ä¸ªæ¨¡å‹
            similarity_threshold=0.6
        )
        
        print(f"âœ… è¯„ä¼°å™¨åˆ›å»ºæˆåŠŸ")
        print(f"   åŠ è½½æ¨¡å‹: {list(evaluator.models.keys())}")
        print(f"   è®¾å¤‡: {evaluator.device}")
        
        # åˆ›å»ºæµ‹è¯•æ•°æ®
        batch_size = 3
        test_images = torch.rand(batch_size, 3, 224, 224, device=evaluator.device)
        protected_images = torch.rand(batch_size, 3, 224, 224, device=evaluator.device)
        
        print("ğŸ”® æµ‹è¯•ç‰¹å¾æå–...")
        
        # æµ‹è¯•ç‰¹å¾æå–
        for model_name in evaluator.models.keys():
            try:
                original_features = evaluator.extract_features(test_images, model_name)
                protected_features = evaluator.extract_features(protected_images, model_name)
                
                print(f"âœ… {model_name} ç‰¹å¾æå–æˆåŠŸ: {original_features.shape}")
                
                # æµ‹è¯•ç›¸ä¼¼åº¦è®¡ç®—
                similarities = evaluator.compute_similarities(original_features, protected_features)
                distances = evaluator.compute_distances(original_features, protected_features)
                
                print(f"âœ… {model_name} ç›¸ä¼¼åº¦è®¡ç®—æˆåŠŸ: å¹³å‡ç›¸ä¼¼åº¦={np.mean(similarities):.4f}")
                
            except Exception as e:
                print(f"âš ï¸ {model_name} æµ‹è¯•å¤±è´¥: {e}")
                continue
        
        # æµ‹è¯•è¯„ä¼°
        print("ğŸ“Š æµ‹è¯•å®Œæ•´è¯„ä¼°...")
        results = evaluator.evaluate_all_models(test_images, protected_images)
        
        if results:
            print(f"âœ… å®Œæ•´è¯„ä¼°æˆåŠŸ: {len(results)}ä¸ªæ¨¡å‹")
            
            for name, result in results.items():
                print(f"   {name}: ä¿æŠ¤ç‡={result.protection_rate:.2%}, å¹³å‡ç›¸ä¼¼åº¦={result.mean_similarity:.4f}")
            
            # æµ‹è¯•å¯è½¬ç§»æ€§åˆ†æï¼ˆå¦‚æœæœ‰å¤šä¸ªæ¨¡å‹ï¼‰
            if len(results) >= 2:
                transferability = evaluator.analyze_transferability(results)
                print(f"âœ… å¯è½¬ç§»æ€§åˆ†æå®Œæˆ: æ•´ä½“å¯è½¬ç§»æ€§={transferability.get('overall_transferability', 0):.2%}")
            
            # æµ‹è¯•æŠ¥å‘Šç”Ÿæˆ
            report = evaluator.generate_report(results, {})
            print(f"âœ… æŠ¥å‘Šç”ŸæˆæˆåŠŸ: {report['summary'].get('total_models_evaluated', 0)}ä¸ªæ¨¡å‹")
        
        print("ğŸ‰ é¢éƒ¨è¯†åˆ«è¯„ä¼°å™¨æµ‹è¯•å®Œå…¨é€šè¿‡ï¼")
        return True
        
    except Exception as e:
        print(f"âŒ æµ‹è¯•å¤±è´¥: {e}")
        import traceback
        traceback.print_exc()
        return False

if __name__ == "__main__":
    test_face_recognition_evaluator() 