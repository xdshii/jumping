"""
èº«ä»½ä¿æŠ¤ç‡(PPR)è®¡ç®—æ¨¡å—

è¯¥æ¨¡å—å®ç°éšç§ä¿æŠ¤ç‡(Privacy Protection Rate)çš„è®¡ç®—ï¼Œ
è¿™æ˜¯è¯„ä¼°é¢éƒ¨éšç§ä¿æŠ¤ç®—æ³•æ•ˆæœçš„æ ¸å¿ƒæŒ‡æ ‡ã€‚

åŠŸèƒ½åŒ…æ‹¬ï¼š
1. æ ‡å‡†PPRè®¡ç®— (åŸºäºç›¸ä¼¼åº¦é˜ˆå€¼)
2. å¤šé˜ˆå€¼PPRåˆ†æ
3. æ¨¡å‹é—´PPRå¯¹æ¯”
4. æ—¶é—´åºåˆ—PPRè¿½è¸ª
5. PPRå¯è§†åŒ–å’ŒæŠ¥å‘Šç”Ÿæˆ

åŸºäºDiffPrivateè®ºæ–‡ä¸­çš„è¯„ä¼°æŒ‡æ ‡è®¾è®¡ã€‚

ä½œè€…: AI Privacy Protection System
æ—¥æœŸ: 2025-07-28
"""

import logging
from typing import List, Dict, Optional, Tuple, Any, Union
from pathlib import Path
import numpy as np
import torch
import json
from datetime import datetime
from dataclasses import dataclass
import matplotlib.pyplot as plt
import seaborn as sns
from collections import defaultdict

try:
    from ..evaluation.face_recognition_eval import FaceRecognitionEvaluator, EvaluationResult
    from ..utils.image_utils import ImageProcessor
except ImportError:
    import sys
    from pathlib import Path
    sys.path.append(str(Path(__file__).parent.parent))
    from evaluation.face_recognition_eval import FaceRecognitionEvaluator, EvaluationResult
    from utils.image_utils import ImageProcessor

logger = logging.getLogger(__name__)

@dataclass
class PPRResult:
    """PPRè®¡ç®—ç»“æœæ•°æ®ç±»"""
    model_name: str
    threshold: float
    ppr_value: float
    num_protected: int
    total_samples: int
    confidence_interval: Optional[Tuple[float, float]] = None
    additional_metrics: Optional[Dict[str, Any]] = None

class ProtectionRateCalculator:
    """èº«ä»½ä¿æŠ¤ç‡è®¡ç®—å™¨"""
    
    def __init__(
        self,
        default_threshold: float = 0.6,
        confidence_level: float = 0.95
    ):
        """
        åˆå§‹åŒ–ä¿æŠ¤ç‡è®¡ç®—å™¨
        
        Args:
            default_threshold: é»˜è®¤ç›¸ä¼¼åº¦é˜ˆå€¼
            confidence_level: ç½®ä¿¡åŒºé—´æ°´å¹³
        """
        self.default_threshold = default_threshold
        self.confidence_level = confidence_level
        
        # åˆå§‹åŒ–å·¥å…·
        self.image_processor = ImageProcessor()
        
        logger.info(f"ä¿æŠ¤ç‡è®¡ç®—å™¨åˆå§‹åŒ–: é˜ˆå€¼={default_threshold}, ç½®ä¿¡åº¦={confidence_level}")
    
    def calculate_ppr(
        self,
        similarities: np.ndarray,
        threshold: float = None,
        compute_confidence: bool = True
    ) -> PPRResult:
        """
        è®¡ç®—å•ä¸ªæ¨¡å‹çš„PPR
        
        Args:
            similarities: ç›¸ä¼¼åº¦æ•°ç»„
            threshold: ç›¸ä¼¼åº¦é˜ˆå€¼
            compute_confidence: æ˜¯å¦è®¡ç®—ç½®ä¿¡åŒºé—´
            
        Returns:
            PPRç»“æœ
        """
        if threshold is None:
            threshold = self.default_threshold
        
        # è¿‡æ»¤NaNå€¼
        valid_similarities = similarities[~np.isnan(similarities)]
        if len(valid_similarities) == 0:
            logger.warning("æ‰€æœ‰ç›¸ä¼¼åº¦å€¼éƒ½æ˜¯NaN")
            return PPRResult(
                model_name="unknown",
                threshold=threshold,
                ppr_value=0.0,
                num_protected=0,
                total_samples=len(similarities)
            )
        
        # è®¡ç®—ä¿æŠ¤çš„æ ·æœ¬æ•°ï¼ˆç›¸ä¼¼åº¦ä½äºé˜ˆå€¼ï¼‰
        protected_mask = valid_similarities < threshold
        num_protected = np.sum(protected_mask)
        total_samples = len(valid_similarities)
        
        # è®¡ç®—PPR
        ppr_value = num_protected / total_samples if total_samples > 0 else 0.0
        
        # è®¡ç®—ç½®ä¿¡åŒºé—´
        confidence_interval = None
        if compute_confidence and total_samples > 0:
            confidence_interval = self._compute_confidence_interval(
                ppr_value, total_samples, self.confidence_level
            )
        
        # è®¡ç®—é™„åŠ æŒ‡æ ‡
        additional_metrics = {
            "mean_similarity": np.mean(valid_similarities),
            "std_similarity": np.std(valid_similarities),
            "median_similarity": np.median(valid_similarities),
            "min_similarity": np.min(valid_similarities),
            "max_similarity": np.max(valid_similarities),
            "num_nan": len(similarities) - len(valid_similarities)
        }
        
        return PPRResult(
            model_name="unknown",
            threshold=threshold,
            ppr_value=ppr_value,
            num_protected=num_protected,
            total_samples=total_samples,
            confidence_interval=confidence_interval,
            additional_metrics=additional_metrics
        )
    
    def _compute_confidence_interval(
        self,
        ppr: float,
        n: int,
        confidence_level: float
    ) -> Tuple[float, float]:
        """
        è®¡ç®—PPRçš„ç½®ä¿¡åŒºé—´ï¼ˆä½¿ç”¨äºŒé¡¹åˆ†å¸ƒçš„æ­£æ€è¿‘ä¼¼ï¼‰
        
        Args:
            ppr: PPRå€¼
            n: æ ·æœ¬æ•°é‡
            confidence_level: ç½®ä¿¡æ°´å¹³
            
        Returns:
            (ä¸‹ç•Œ, ä¸Šç•Œ)
        """
        if n <= 0:
            return (0.0, 0.0)
        
        # è®¡ç®—æ ‡å‡†è¯¯å·®
        se = np.sqrt(ppr * (1 - ppr) / n)
        
        # Zåˆ†æ•°ï¼ˆå¯¹äº95%ç½®ä¿¡åŒºé—´ï¼Œz=1.96ï¼‰
        if confidence_level == 0.95:
            z = 1.96
        elif confidence_level == 0.99:
            z = 2.576
        elif confidence_level == 0.90:
            z = 1.645
        else:
            # è¿‘ä¼¼è®¡ç®—
            from scipy.stats import norm
            z = norm.ppf((1 + confidence_level) / 2)
        
        # è®¡ç®—ç½®ä¿¡åŒºé—´
        margin = z * se
        lower = max(0.0, ppr - margin)
        upper = min(1.0, ppr + margin)
        
        return (lower, upper)
    
    def calculate_multi_threshold_ppr(
        self,
        similarities: np.ndarray,
        thresholds: List[float] = None,
        model_name: str = "unknown"
    ) -> List[PPRResult]:
        """
        è®¡ç®—å¤šä¸ªé˜ˆå€¼ä¸‹çš„PPR
        
        Args:
            similarities: ç›¸ä¼¼åº¦æ•°ç»„
            thresholds: é˜ˆå€¼åˆ—è¡¨
            model_name: æ¨¡å‹åç§°
            
        Returns:
            PPRç»“æœåˆ—è¡¨
        """
        if thresholds is None:
            thresholds = [0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9]
        
        results = []
        for threshold in thresholds:
            result = self.calculate_ppr(similarities, threshold)
            result.model_name = model_name
            results.append(result)
        
        return results
    
    def calculate_multi_model_ppr(
        self,
        evaluation_results: Dict[str, EvaluationResult],
        threshold: float = None
    ) -> Dict[str, PPRResult]:
        """
        è®¡ç®—å¤šä¸ªæ¨¡å‹çš„PPR
        
        Args:
            evaluation_results: è¯„ä¼°ç»“æœå­—å…¸
            threshold: ç›¸ä¼¼åº¦é˜ˆå€¼
            
        Returns:
            å„æ¨¡å‹çš„PPRç»“æœ
        """
        if threshold is None:
            threshold = self.default_threshold
        
        ppr_results = {}
        
        for model_name, eval_result in evaluation_results.items():
            ppr_result = self.calculate_ppr(eval_result.similarities, threshold)
            ppr_result.model_name = model_name
            ppr_results[model_name] = ppr_result
        
        return ppr_results
    
    def analyze_ppr_trends(
        self,
        ppr_history: List[Dict[str, PPRResult]],
        time_points: Optional[List[str]] = None
    ) -> Dict[str, Any]:
        """
        åˆ†æPPRè¶‹åŠ¿
        
        Args:
            ppr_history: PPRå†å²è®°å½•åˆ—è¡¨
            time_points: æ—¶é—´ç‚¹åˆ—è¡¨
            
        Returns:
            è¶‹åŠ¿åˆ†æç»“æœ
        """
        if not ppr_history:
            return {}
        
        # æå–æ‰€æœ‰æ¨¡å‹åç§°
        all_models = set()
        for ppr_dict in ppr_history:
            all_models.update(ppr_dict.keys())
        
        trends = {}
        
        for model_name in all_models:
            model_pprs = []
            model_times = []
            
            for i, ppr_dict in enumerate(ppr_history):
                if model_name in ppr_dict:
                    model_pprs.append(ppr_dict[model_name].ppr_value)
                    model_times.append(time_points[i] if time_points else i)
            
            if len(model_pprs) >= 2:
                # è®¡ç®—è¶‹åŠ¿
                ppr_array = np.array(model_pprs)
                
                # çº¿æ€§å›å½’æ–œç‡ï¼ˆç®€å•è¶‹åŠ¿ï¼‰
                x = np.arange(len(ppr_array))
                slope = np.polyfit(x, ppr_array, 1)[0] if len(ppr_array) > 1 else 0
                
                trends[model_name] = {
                    "values": model_pprs,
                    "times": model_times,
                    "initial_ppr": model_pprs[0],
                    "final_ppr": model_pprs[-1],
                    "change": model_pprs[-1] - model_pprs[0],
                    "slope": slope,
                    "trend": "improving" if slope > 0.01 else "declining" if slope < -0.01 else "stable",
                    "max_ppr": np.max(ppr_array),
                    "min_ppr": np.min(ppr_array),
                    "mean_ppr": np.mean(ppr_array),
                    "std_ppr": np.std(ppr_array)
                }
        
        return trends
    
    def compare_models(
        self,
        ppr_results: Dict[str, PPRResult]
    ) -> Dict[str, Any]:
        """
        æ¯”è¾ƒä¸åŒæ¨¡å‹çš„PPR
        
        Args:
            ppr_results: PPRç»“æœå­—å…¸
            
        Returns:
            æ¨¡å‹æ¯”è¾ƒç»“æœ
        """
        if not ppr_results:
            return {}
        
        # æå–PPRå€¼
        model_names = list(ppr_results.keys())
        ppr_values = [ppr_results[name].ppr_value for name in model_names]
        
        # æ’åº
        sorted_indices = np.argsort(ppr_values)[::-1]  # é™åº
        
        comparison = {
            "best_model": model_names[sorted_indices[0]] if model_names else "",
            "worst_model": model_names[sorted_indices[-1]] if model_names else "",
            "rankings": [model_names[i] for i in sorted_indices],
            "ppr_values": [ppr_values[i] for i in sorted_indices],
            "mean_ppr": np.mean(ppr_values),
            "std_ppr": np.std(ppr_values),
            "range_ppr": np.max(ppr_values) - np.min(ppr_values) if ppr_values else 0,
            "consistency": 1.0 / (1.0 + np.std(ppr_values)) if ppr_values else 0  # ä¸€è‡´æ€§åˆ†æ•°
        }
        
        # ç»Ÿè®¡åˆ†æ
        if len(ppr_values) >= 2:
            comparison["statistical_significance"] = self._test_significance(ppr_results)
        
        return comparison
    
    def _test_significance(
        self,
        ppr_results: Dict[str, PPRResult]
    ) -> Dict[str, Any]:
        """
        æµ‹è¯•PPRå·®å¼‚çš„ç»Ÿè®¡æ˜¾è‘—æ€§
        
        Args:
            ppr_results: PPRç»“æœå­—å…¸
            
        Returns:
            æ˜¾è‘—æ€§æµ‹è¯•ç»“æœ
        """
        # ç®€åŒ–çš„å¡æ–¹æ£€éªŒ
        models = list(ppr_results.keys())
        significance_tests = {}
        
        for i, model1 in enumerate(models):
            for j, model2 in enumerate(models[i+1:], i+1):
                result1 = ppr_results[model1]
                result2 = ppr_results[model2]
                
                # æ„å»º2x2åˆ—è”è¡¨
                protected1 = result1.num_protected
                unprotected1 = result1.total_samples - result1.num_protected
                protected2 = result2.num_protected
                unprotected2 = result2.total_samples - result2.num_protected
                
                # ç®€å•çš„å·®å¼‚æ£€éªŒ
                if result1.total_samples > 0 and result2.total_samples > 0:
                    diff = abs(result1.ppr_value - result2.ppr_value)
                    significance_tests[f"{model1}_vs_{model2}"] = {
                        "difference": result1.ppr_value - result2.ppr_value,
                        "abs_difference": diff,
                        "is_significant": diff > 0.05,  # ç®€åŒ–çš„æ˜¾è‘—æ€§åˆ¤æ–­
                        "better_model": model1 if result1.ppr_value > result2.ppr_value else model2
                    }
        
        return significance_tests
    
    def generate_ppr_report(
        self,
        ppr_results: Dict[str, PPRResult],
        comparison: Dict[str, Any] = None,
        save_path: Optional[str] = None
    ) -> Dict[str, Any]:
        """
        ç”ŸæˆPPRè¯„ä¼°æŠ¥å‘Š
        
        Args:
            ppr_results: PPRç»“æœå­—å…¸
            comparison: æ¨¡å‹æ¯”è¾ƒç»“æœ
            save_path: ä¿å­˜è·¯å¾„
            
        Returns:
            å®Œæ•´æŠ¥å‘Š
        """
        if comparison is None:
            comparison = self.compare_models(ppr_results)
        
        report = {
            "metadata": {
                "timestamp": datetime.now().isoformat(),
                "calculator_config": {
                    "default_threshold": self.default_threshold,
                    "confidence_level": self.confidence_level
                },
                "total_models": len(ppr_results)
            },
            "individual_results": {},
            "comparison": comparison,
            "summary": {}
        }
        
        # ä¸ªåˆ«æ¨¡å‹ç»“æœ
        for model_name, ppr_result in ppr_results.items():
            report["individual_results"][model_name] = {
                "ppr_value": ppr_result.ppr_value,
                "threshold": ppr_result.threshold,
                "num_protected": ppr_result.num_protected,
                "total_samples": ppr_result.total_samples,
                "confidence_interval": ppr_result.confidence_interval,
                "additional_metrics": ppr_result.additional_metrics
            }
        
        # æ‘˜è¦ç»Ÿè®¡
        if ppr_results:
            ppr_values = [r.ppr_value for r in ppr_results.values()]
            report["summary"] = {
                "overall_ppr": np.mean(ppr_values),
                "best_ppr": np.max(ppr_values),
                "worst_ppr": np.min(ppr_values),
                "ppr_std": np.std(ppr_values),
                "effectiveness_rating": self._rate_effectiveness(np.mean(ppr_values)),
                "recommendation": self._generate_recommendation(comparison)
            }
        
        # ä¿å­˜æŠ¥å‘Š
        if save_path:
            with open(save_path, 'w', encoding='utf-8') as f:
                json.dump(report, f, indent=2, ensure_ascii=False)
            logger.info(f"PPRæŠ¥å‘Šå·²ä¿å­˜åˆ°: {save_path}")
        
        return report
    
    def _rate_effectiveness(self, ppr: float) -> str:
        """è¯„ä¼°ä¿æŠ¤æ•ˆæœç­‰çº§"""
        if ppr >= 0.85:
            return "Excellent"
        elif ppr >= 0.70:
            return "Good"
        elif ppr >= 0.50:
            return "Fair"
        elif ppr >= 0.30:
            return "Poor"
        else:
            return "Very Poor"
    
    def _generate_recommendation(self, comparison: Dict[str, Any]) -> str:
        """ç”Ÿæˆæ”¹è¿›å»ºè®®"""
        if not comparison:
            return "æ— è¶³å¤Ÿæ•°æ®ç”Ÿæˆå»ºè®®"
        
        mean_ppr = comparison.get("mean_ppr", 0)
        consistency = comparison.get("consistency", 0)
        
        if mean_ppr >= 0.8 and consistency >= 0.8:
            return "ä¿æŠ¤æ•ˆæœä¼˜ç§€ä¸”ä¸€è‡´ï¼Œå»ºè®®ä¿æŒå½“å‰é…ç½®"
        elif mean_ppr >= 0.7:
            return "ä¿æŠ¤æ•ˆæœè‰¯å¥½ï¼Œå¯è€ƒè™‘ä¼˜åŒ–ä¸€è‡´æ€§"
        elif mean_ppr >= 0.5:
            return "ä¿æŠ¤æ•ˆæœä¸­ç­‰ï¼Œå»ºè®®å¢å¼ºä¿æŠ¤å¼ºåº¦æˆ–ä¼˜åŒ–ç®—æ³•å‚æ•°"
        else:
            return "ä¿æŠ¤æ•ˆæœè¾ƒå·®ï¼Œå»ºè®®é‡æ–°è¯„ä¼°ç®—æ³•è®¾è®¡æˆ–å‚æ•°é…ç½®"
    
    def plot_ppr_comparison(
        self,
        ppr_results: Dict[str, PPRResult],
        save_path: Optional[str] = None,
        show_confidence: bool = True
    ):
        """
        ç»˜åˆ¶PPRå¯¹æ¯”å›¾
        
        Args:
            ppr_results: PPRç»“æœå­—å…¸
            save_path: ä¿å­˜è·¯å¾„
            show_confidence: æ˜¯å¦æ˜¾ç¤ºç½®ä¿¡åŒºé—´
        """
        if not ppr_results:
            logger.warning("æ²¡æœ‰PPRç»“æœå¯ç»˜åˆ¶")
            return
        
        fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 6))
        
        model_names = list(ppr_results.keys())
        ppr_values = [ppr_results[name].ppr_value for name in model_names]
        
        # 1. PPRæŸ±çŠ¶å›¾
        bars = ax1.bar(model_names, ppr_values)
        ax1.set_title('Privacy Protection Rate by Model')
        ax1.set_ylabel('PPR')
        ax1.set_ylim(0, 1)
        
        # æ·»åŠ æ•°å€¼æ ‡ç­¾
        for i, (bar, ppr) in enumerate(zip(bars, ppr_values)):
            ax1.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.01,
                    f'{ppr:.2%}', ha='center', va='bottom')
            
            # æ·»åŠ ç½®ä¿¡åŒºé—´
            if show_confidence:
                ppr_result = ppr_results[model_names[i]]
                if ppr_result.confidence_interval:
                    lower, upper = ppr_result.confidence_interval
                    ax1.errorbar(bar.get_x() + bar.get_width()/2, ppr,
                               yerr=[[ppr - lower], [upper - ppr]],
                               fmt='none', capsize=3, color='black')
        
        # 2. é˜ˆå€¼æ•æ„Ÿæ€§åˆ†æï¼ˆå¦‚æœæœ‰å¤šé˜ˆå€¼æ•°æ®ï¼‰
        # è¿™é‡Œæ˜¾ç¤ºæ ·æœ¬æ•°é‡åˆ†å¸ƒ
        sample_counts = [ppr_results[name].total_samples for name in model_names]
        
        ax2.bar(model_names, sample_counts, alpha=0.7, color='lightblue')
        ax2.set_title('Sample Count by Model')
        ax2.set_ylabel('Number of Samples')
        
        for i, count in enumerate(sample_counts):
            ax2.text(i, count + max(sample_counts) * 0.01, str(count),
                    ha='center', va='bottom')
        
        plt.tight_layout()
        
        if save_path:
            plt.savefig(save_path, dpi=300, bbox_inches='tight')
            logger.info(f"PPRå¯¹æ¯”å›¾å·²ä¿å­˜åˆ°: {save_path}")
        
        plt.close()

def create_protection_rate_calculator(
    default_threshold: float = 0.6,
    **kwargs
) -> ProtectionRateCalculator:
    """
    åˆ›å»ºä¿æŠ¤ç‡è®¡ç®—å™¨çš„ä¾¿æ·å‡½æ•°
    
    Args:
        default_threshold: é»˜è®¤é˜ˆå€¼
        **kwargs: å…¶ä»–å‚æ•°
        
    Returns:
        ä¿æŠ¤ç‡è®¡ç®—å™¨å®ä¾‹
    """
    return ProtectionRateCalculator(
        default_threshold=default_threshold,
        **kwargs
    )

def test_protection_rate_calculator():
    """æµ‹è¯•ä¿æŠ¤ç‡è®¡ç®—å™¨"""
    print("ğŸ§ª æµ‹è¯•èº«ä»½ä¿æŠ¤ç‡è®¡ç®—å™¨...")
    
    try:
        # åˆ›å»ºè®¡ç®—å™¨
        calculator = create_protection_rate_calculator(default_threshold=0.6)
        
        print("âœ… ä¿æŠ¤ç‡è®¡ç®—å™¨åˆ›å»ºæˆåŠŸ")
        print(f"   é»˜è®¤é˜ˆå€¼: {calculator.default_threshold}")
        print(f"   ç½®ä¿¡æ°´å¹³: {calculator.confidence_level}")
        
        # åˆ›å»ºæµ‹è¯•æ•°æ®
        np.random.seed(42)  # å›ºå®šéšæœºç§å­ä»¥è·å¾—ä¸€è‡´ç»“æœ
        
        # æ¨¡æ‹Ÿç›¸ä¼¼åº¦æ•°æ®ï¼ˆä¿æŠ¤ååº”è¯¥æ›´ä½ï¼‰
        original_similarities = np.random.uniform(0.7, 0.95, 100)  # é«˜ç›¸ä¼¼åº¦
        protected_similarities = np.random.uniform(0.2, 0.6, 100)   # ä½ç›¸ä¼¼åº¦
        
        print("ğŸ”® æµ‹è¯•å•ä¸ªPPRè®¡ç®—...")
        
        # æµ‹è¯•åŸå§‹å›¾åƒPPRï¼ˆåº”è¯¥å¾ˆä½ï¼‰
        original_ppr = calculator.calculate_ppr(original_similarities)
        print(f"âœ… åŸå§‹å›¾åƒPPR: {original_ppr.ppr_value:.2%} ({original_ppr.num_protected}/{original_ppr.total_samples})")
        
        # æµ‹è¯•ä¿æŠ¤å›¾åƒPPRï¼ˆåº”è¯¥è¾ƒé«˜ï¼‰
        protected_ppr = calculator.calculate_ppr(protected_similarities)
        print(f"âœ… ä¿æŠ¤å›¾åƒPPR: {protected_ppr.ppr_value:.2%} ({protected_ppr.num_protected}/{protected_ppr.total_samples})")
        
        # æµ‹è¯•å¤šé˜ˆå€¼PPR
        print("ğŸ“Š æµ‹è¯•å¤šé˜ˆå€¼PPR...")
        multi_threshold_results = calculator.calculate_multi_threshold_ppr(
            protected_similarities, 
            thresholds=[0.3, 0.5, 0.7, 0.9],
            model_name="test_model"
        )
        
        for result in multi_threshold_results:
            print(f"   é˜ˆå€¼{result.threshold}: PPR={result.ppr_value:.2%}")
        
        # æµ‹è¯•æ¨¡å‹æ¯”è¾ƒ
        print("ğŸ† æµ‹è¯•æ¨¡å‹æ¯”è¾ƒ...")
        
        # æ¨¡æ‹Ÿå¤šä¸ªæ¨¡å‹çš„PPRç»“æœ
        model_pprs = {
            "arcface": PPRResult("arcface", 0.6, 0.85, 85, 100),
            "facenet": PPRResult("facenet", 0.6, 0.78, 78, 100),
            "dummy": PPRResult("dummy", 0.6, 0.65, 65, 100)
        }
        
        comparison = calculator.compare_models(model_pprs)
        print(f"âœ… æœ€ä½³æ¨¡å‹: {comparison['best_model']} (PPR: {max(comparison['ppr_values']):.2%})")
        print(f"âœ… æœ€å·®æ¨¡å‹: {comparison['worst_model']} (PPR: {min(comparison['ppr_values']):.2%})")
        print(f"âœ… å¹³å‡PPR: {comparison['mean_ppr']:.2%}")
        print(f"âœ… ä¸€è‡´æ€§åˆ†æ•°: {comparison['consistency']:.3f}")
        
        # æµ‹è¯•æŠ¥å‘Šç”Ÿæˆ
        print("ğŸ“‹ æµ‹è¯•æŠ¥å‘Šç”Ÿæˆ...")
        report = calculator.generate_ppr_report(model_pprs, comparison)
        
        print(f"âœ… æŠ¥å‘Šç”ŸæˆæˆåŠŸ:")
        print(f"   è¯„ä¼°æ¨¡å‹æ•°: {report['metadata']['total_models']}")
        print(f"   æ•´ä½“PPR: {report['summary']['overall_ppr']:.2%}")
        print(f"   æ•ˆæœè¯„çº§: {report['summary']['effectiveness_rating']}")
        print(f"   å»ºè®®: {report['summary']['recommendation']}")
        
        print("ğŸ‰ èº«ä»½ä¿æŠ¤ç‡è®¡ç®—å™¨æµ‹è¯•å…¨éƒ¨é€šè¿‡ï¼")
        return True
        
    except Exception as e:
        print(f"âŒ æµ‹è¯•å¤±è´¥: {e}")
        import traceback
        traceback.print_exc()
        return False

if __name__ == "__main__":
    test_protection_rate_calculator() 