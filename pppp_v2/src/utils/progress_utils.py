"""
è¿›åº¦æ¡æ˜¾ç¤ºå’Œä¸­é—´ç»“æœä¿å­˜å·¥å…·

è¯¥æ¨¡å—æä¾›ä¼˜åŒ–è¿‡ç¨‹ä¸­çš„è¿›åº¦å¯è§†åŒ–å’Œä¸­é—´ç»“æœä¿å­˜åŠŸèƒ½ï¼Œ
å¸®åŠ©ç”¨æˆ·è·Ÿè¸ªä¼˜åŒ–è¿›åº¦å¹¶ä¿å­˜å…³é”®çš„ä¸­é—´çŠ¶æ€ã€‚

åŠŸèƒ½åŒ…æ‹¬ï¼š
1. è‡ªå®šä¹‰è¿›åº¦æ¡æ˜¾ç¤ºï¼ˆæ”¯æŒå¤šç§ä¿¡æ¯æ˜¾ç¤ºï¼‰
2. ä¸­é—´ç»“æœä¿å­˜ï¼ˆå›¾åƒã€æŸå¤±å†å²ã€æ¨¡å‹çŠ¶æ€ï¼‰
3. ä¼˜åŒ–è¿‡ç¨‹å¯è§†åŒ–ï¼ˆå®æ—¶æŸå¤±æ›²çº¿ï¼‰
4. ç»“æœå¯¹æ¯”å’Œåˆ†æå·¥å…·
5. å¼‚å¸¸æƒ…å†µçš„æ¢å¤æœºåˆ¶

ä½œè€…: AI Privacy Protection System
æ—¥æœŸ: 2025-07-28
"""

import os
import time
import logging
from typing import Dict, List, Optional, Any, Callable, Union
from pathlib import Path
import pickle
import json
from datetime import datetime
import numpy as np
import torch
from tqdm import tqdm
import matplotlib.pyplot as plt
from collections import defaultdict, deque
from dataclasses import dataclass, asdict
import threading

try:
    from ..utils.image_utils import ImageProcessor
except ImportError:
    import sys
    from pathlib import Path
    sys.path.append(str(Path(__file__).parent.parent))
    from utils.image_utils import ImageProcessor

logger = logging.getLogger(__name__)

@dataclass
class ProgressSnapshot:
    """è¿›åº¦å¿«ç…§æ•°æ®ç±»"""
    iteration: int
    timestamp: float
    losses: Dict[str, float]
    learning_rate: float
    protected_image: Optional[torch.Tensor] = None
    protected_latents: Optional[torch.Tensor] = None
    additional_data: Optional[Dict[str, Any]] = None

class AdvancedProgressBar:
    """é«˜çº§è¿›åº¦æ¡ç®¡ç†å™¨"""
    
    def __init__(
        self,
        total_iterations: int,
        description: str = "ä¼˜åŒ–è¿›åº¦",
        save_dir: Optional[str] = None,
        save_frequency: int = 10,
        display_metrics: List[str] = None,
        smoothing_window: int = 5
    ):
        """
        åˆå§‹åŒ–è¿›åº¦æ¡ç®¡ç†å™¨
        
        Args:
            total_iterations: æ€»è¿­ä»£æ¬¡æ•°
            description: è¿›åº¦æè¿°
            save_dir: ä¿å­˜ç›®å½•
            save_frequency: ä¿å­˜é¢‘ç‡ï¼ˆæ¯Næ¬¡è¿­ä»£ï¼‰
            display_metrics: æ˜¾ç¤ºçš„æŒ‡æ ‡åˆ—è¡¨
            smoothing_window: å¹³æ»‘çª—å£å¤§å°
        """
        self.total_iterations = total_iterations
        self.description = description
        self.save_dir = Path(save_dir) if save_dir else None
        self.save_frequency = save_frequency
        self.display_metrics = display_metrics or ["total_loss", "id_loss", "lpips_loss"]
        self.smoothing_window = smoothing_window
        
        # åˆå§‹åŒ–ä¿å­˜ç›®å½•
        if self.save_dir:
            self.save_dir.mkdir(parents=True, exist_ok=True)
            self.images_dir = self.save_dir / "images"
            self.data_dir = self.save_dir / "data"
            self.plots_dir = self.save_dir / "plots"
            
            for directory in [self.images_dir, self.data_dir, self.plots_dir]:
                directory.mkdir(exist_ok=True)
        
        # è¿›åº¦è·Ÿè¸ª
        self.pbar = None
        self.start_time = None
        self.loss_history = defaultdict(list)
        self.snapshots = []
        self.smoothed_losses = defaultdict(lambda: deque(maxlen=smoothing_window))
        self.image_processor = ImageProcessor()
        
        # å®æ—¶ç»˜å›¾ï¼ˆå¯é€‰ï¼‰
        self.live_plotting = False
        self.plot_thread = None
        self.plot_lock = threading.Lock()
        
        logger.info(f"è¿›åº¦æ¡ç®¡ç†å™¨åˆå§‹åŒ–: {total_iterations}æ¬¡è¿­ä»£, ä¿å­˜åˆ° {save_dir}")
    
    def start(self):
        """å¼€å§‹è¿›åº¦è·Ÿè¸ª"""
        self.start_time = time.time()
        self.pbar = tqdm(
            total=self.total_iterations,
            desc=self.description,
            ncols=120,
            bar_format='{l_bar}{bar}| {n_fmt}/{total_fmt} [{elapsed}<{remaining}, {rate_fmt}] {postfix}'
        )
        
        logger.info("å¼€å§‹è¿›åº¦è·Ÿè¸ª")
    
    def update(
        self,
        iteration: int,
        losses: Dict[str, float],
        learning_rate: float,
        protected_image: Optional[torch.Tensor] = None,
        protected_latents: Optional[torch.Tensor] = None,
        additional_data: Optional[Dict[str, Any]] = None
    ):
        """
        æ›´æ–°è¿›åº¦
        
        Args:
            iteration: å½“å‰è¿­ä»£æ¬¡æ•°
            losses: æŸå¤±å­—å…¸
            learning_rate: å­¦ä¹ ç‡
            protected_image: ä¿æŠ¤åçš„å›¾åƒ
            protected_latents: ä¿æŠ¤åçš„æ½œç©ºé—´
            additional_data: é™„åŠ æ•°æ®
        """
        if self.pbar is None:
            raise RuntimeError("è¯·å…ˆè°ƒç”¨start()æ–¹æ³•")
        
        # æ›´æ–°æŸå¤±å†å²
        for key, value in losses.items():
            if isinstance(value, torch.Tensor):
                value = value.item()
            self.loss_history[key].append(value)
            self.smoothed_losses[key].append(value)
        
        # è®¡ç®—å¹³æ»‘æŸå¤±
        smoothed_metrics = {}
        for key in self.display_metrics:
            if key in self.smoothed_losses and len(self.smoothed_losses[key]) > 0:
                smoothed_metrics[key] = np.mean(list(self.smoothed_losses[key]))
        
        # æ›´æ–°è¿›åº¦æ¡æ˜¾ç¤º
        postfix_dict = {}
        for key, value in smoothed_metrics.items():
            if 'loss' in key.lower():
                postfix_dict[key] = f"{value:.6f}"
            else:
                postfix_dict[key] = f"{value:.4f}"
        
        postfix_dict['lr'] = f"{learning_rate:.2e}"
        
        # è®¡ç®—ETA
        if iteration > 0:
            elapsed_time = time.time() - self.start_time
            time_per_iter = elapsed_time / iteration
            remaining_iters = self.total_iterations - iteration
            eta_seconds = time_per_iter * remaining_iters
            eta_formatted = self._format_time(eta_seconds)
            postfix_dict['ETA'] = eta_formatted
        
        self.pbar.set_postfix(**postfix_dict)
        self.pbar.update(1)
        
        # ä¿å­˜ä¸­é—´ç»“æœ
        if self.save_dir and iteration % self.save_frequency == 0:
            self._save_intermediate_results(
                iteration, losses, learning_rate, 
                protected_image, protected_latents, additional_data
            )
        
        # åˆ›å»ºå¿«ç…§
        snapshot = ProgressSnapshot(
            iteration=iteration,
            timestamp=time.time(),
            losses=losses.copy(),
            learning_rate=learning_rate,
            protected_image=protected_image.clone().detach() if protected_image is not None else None,
            protected_latents=protected_latents.clone().detach() if protected_latents is not None else None,
            additional_data=additional_data
        )
        self.snapshots.append(snapshot)
    
    def _save_intermediate_results(
        self,
        iteration: int,
        losses: Dict[str, float],
        learning_rate: float,
        protected_image: Optional[torch.Tensor],
        protected_latents: Optional[torch.Tensor],
        additional_data: Optional[Dict[str, Any]]
    ):
        """ä¿å­˜ä¸­é—´ç»“æœ"""
        try:
            timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
            
            # ä¿å­˜å›¾åƒï¼ˆå¦‚æœæä¾›ï¼‰
            if protected_image is not None:
                image_path = self.images_dir / f"iter_{iteration:04d}_{timestamp}.png"
                self.image_processor.save_image(protected_image, str(image_path))
            
            # ä¿å­˜æ½œç©ºé—´ï¼ˆå¦‚æœæä¾›ï¼‰
            if protected_latents is not None:
                latents_path = self.data_dir / f"latents_{iteration:04d}_{timestamp}.pt"
                torch.save(protected_latents, latents_path)
            
            # ä¿å­˜æŸå¤±å’Œå…ƒæ•°æ®
            metadata = {
                'iteration': iteration,
                'timestamp': timestamp,
                'losses': {k: float(v) if isinstance(v, torch.Tensor) else v for k, v in losses.items()},
                'learning_rate': learning_rate,
                'additional_data': additional_data
            }
            
            metadata_path = self.data_dir / f"metadata_{iteration:04d}_{timestamp}.json"
            with open(metadata_path, 'w', encoding='utf-8') as f:
                json.dump(metadata, f, indent=2, ensure_ascii=False)
            
        except Exception as e:
            logger.warning(f"ä¿å­˜ä¸­é—´ç»“æœå¤±è´¥ (è¿­ä»£ {iteration}): {e}")
    
    def _format_time(self, seconds: float) -> str:
        """æ ¼å¼åŒ–æ—¶é—´æ˜¾ç¤º"""
        if seconds < 60:
            return f"{seconds:.0f}s"
        elif seconds < 3600:
            return f"{seconds/60:.0f}m{seconds%60:.0f}s"
        else:
            return f"{seconds/3600:.0f}h{(seconds%3600)/60:.0f}m"
    
    def finish(self, final_losses: Optional[Dict[str, float]] = None):
        """å®Œæˆè¿›åº¦è·Ÿè¸ª"""
        if self.pbar:
            self.pbar.close()
        
        total_time = time.time() - self.start_time if self.start_time else 0
        
        logger.info(f"ä¼˜åŒ–å®Œæˆ: æ€»æ—¶é—´ {self._format_time(total_time)}")
        
        # ä¿å­˜æœ€ç»ˆç»“æœæ‘˜è¦
        if self.save_dir:
            self._save_final_summary(final_losses, total_time)
    
    def _save_final_summary(self, final_losses: Optional[Dict[str, float]], total_time: float):
        """ä¿å­˜æœ€ç»ˆæ‘˜è¦"""
        try:
            summary = {
                'total_iterations': len(self.snapshots),
                'total_time_seconds': total_time,
                'total_time_formatted': self._format_time(total_time),
                'final_losses': final_losses or {},
                'loss_history': {k: list(v) for k, v in self.loss_history.items()},
                'timestamp': datetime.now().isoformat(),
                'snapshots_count': len(self.snapshots)
            }
            
            summary_path = self.save_dir / "optimization_summary.json"
            with open(summary_path, 'w', encoding='utf-8') as f:
                json.dump(summary, f, indent=2, ensure_ascii=False)
            
            # ä¿å­˜å¿«ç…§æ•°æ®
            snapshots_path = self.save_dir / "snapshots.pkl"
            with open(snapshots_path, 'wb') as f:
                pickle.dump(self.snapshots, f)
            
            logger.info(f"æœ€ç»ˆæ‘˜è¦å·²ä¿å­˜åˆ°: {summary_path}")
            
        except Exception as e:
            logger.error(f"ä¿å­˜æœ€ç»ˆæ‘˜è¦å¤±è´¥: {e}")
    
    def plot_loss_curves(self, save_path: Optional[str] = None, show: bool = False):
        """ç»˜åˆ¶æŸå¤±æ›²çº¿"""
        try:
            plt.figure(figsize=(12, 8))
            
            # ç»˜åˆ¶ä¸»è¦æŸå¤±
            for i, (loss_name, values) in enumerate(self.loss_history.items()):
                if 'total' in loss_name.lower() or len(self.loss_history) <= 5:
                    plt.subplot(2, 2, min(i + 1, 4))
                    plt.plot(values, label=loss_name)
                    plt.title(f"{loss_name} å˜åŒ–æ›²çº¿")
                    plt.xlabel("è¿­ä»£æ¬¡æ•°")
                    plt.ylabel("æŸå¤±å€¼")
                    plt.legend()
                    plt.grid(True, alpha=0.3)
            
            plt.tight_layout()
            
            if save_path:
                plt.savefig(save_path, dpi=300, bbox_inches='tight')
                logger.info(f"æŸå¤±æ›²çº¿å·²ä¿å­˜åˆ°: {save_path}")
            
            if show:
                plt.show()
            
            if not show:
                plt.close()
                
        except Exception as e:
            logger.error(f"ç»˜åˆ¶æŸå¤±æ›²çº¿å¤±è´¥: {e}")
    
    def get_best_iteration(self, metric: str = "total_loss", minimize: bool = True) -> Optional[int]:
        """
        è·å–æœ€ä½³è¿­ä»£æ¬¡æ•°
        
        Args:
            metric: è¯„ä¼°æŒ‡æ ‡
            minimize: æ˜¯å¦æœ€å°åŒ–æŒ‡æ ‡
            
        Returns:
            æœ€ä½³è¿­ä»£æ¬¡æ•°
        """
        if metric not in self.loss_history or len(self.loss_history[metric]) == 0:
            return None
        
        values = self.loss_history[metric]
        if minimize:
            best_idx = np.argmin(values)
        else:
            best_idx = np.argmax(values)
        
        return best_idx
    
    def load_checkpoint(self, checkpoint_path: str) -> Optional[ProgressSnapshot]:
        """
        ä»æ£€æŸ¥ç‚¹åŠ è½½è¿›åº¦çŠ¶æ€
        
        Args:
            checkpoint_path: æ£€æŸ¥ç‚¹æ–‡ä»¶è·¯å¾„
            
        Returns:
            è¿›åº¦å¿«ç…§
        """
        try:
            with open(checkpoint_path, 'rb') as f:
                checkpoint = pickle.load(f)
            
            if isinstance(checkpoint, ProgressSnapshot):
                return checkpoint
            else:
                logger.warning("æ£€æŸ¥ç‚¹æ–‡ä»¶æ ¼å¼ä¸æ­£ç¡®")
                return None
                
        except Exception as e:
            logger.error(f"åŠ è½½æ£€æŸ¥ç‚¹å¤±è´¥: {e}")
            return None

class OptimizationLogger:
    """ä¼˜åŒ–è¿‡ç¨‹æ—¥å¿—è®°å½•å™¨"""
    
    def __init__(self, log_dir: str, experiment_name: str = "optimization"):
        """
        åˆå§‹åŒ–æ—¥å¿—è®°å½•å™¨
        
        Args:
            log_dir: æ—¥å¿—ç›®å½•
            experiment_name: å®éªŒåç§°
        """
        self.log_dir = Path(log_dir)
        self.experiment_name = experiment_name
        self.log_dir.mkdir(parents=True, exist_ok=True)
        
        # åˆ›å»ºå”¯ä¸€çš„å®éªŒID
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        self.experiment_id = f"{experiment_name}_{timestamp}"
        self.experiment_dir = self.log_dir / self.experiment_id
        self.experiment_dir.mkdir(exist_ok=True)
        
        # è®¾ç½®æ—¥å¿—æ–‡ä»¶
        log_file = self.experiment_dir / "optimization.log"
        self.logger = logging.getLogger(f"optimization_{self.experiment_id}")
        self.logger.setLevel(logging.INFO)
        
        # æ–‡ä»¶å¤„ç†å™¨
        file_handler = logging.FileHandler(log_file, encoding='utf-8')
        file_formatter = logging.Formatter(
            '%(asctime)s - %(name)s - %(levelname)s - %(message)s'
        )
        file_handler.setFormatter(file_formatter)
        self.logger.addHandler(file_handler)
        
        self.logger.info(f"ä¼˜åŒ–æ—¥å¿—è®°å½•å™¨åˆå§‹åŒ–: {self.experiment_id}")
    
    def log_config(self, config: Dict[str, Any]):
        """è®°å½•é…ç½®ä¿¡æ¯"""
        config_file = self.experiment_dir / "config.json"
        with open(config_file, 'w', encoding='utf-8') as f:
            json.dump(config, f, indent=2, ensure_ascii=False)
        
        self.logger.info(f"é…ç½®å·²è®°å½•: {config}")
    
    def log_iteration(self, iteration: int, losses: Dict[str, float], **kwargs):
        """è®°å½•è¿­ä»£ä¿¡æ¯"""
        loss_str = ", ".join([f"{k}={v:.6f}" for k, v in losses.items()])
        extra_str = ", ".join([f"{k}={v}" for k, v in kwargs.items()])
        
        message = f"Iter {iteration}: {loss_str}"
        if extra_str:
            message += f", {extra_str}"
        
        self.logger.info(message)
    
    def log_milestone(self, message: str):
        """è®°å½•é‡Œç¨‹ç¢‘äº‹ä»¶"""
        self.logger.info(f"MILESTONE: {message}")
    
    def get_experiment_dir(self) -> Path:
        """è·å–å®éªŒç›®å½•"""
        return self.experiment_dir

def create_progress_manager(
    total_iterations: int,
    save_dir: Optional[str] = None,
    experiment_name: str = "optimization",
    save_frequency: int = 10,
    **kwargs
) -> AdvancedProgressBar:
    """
    åˆ›å»ºè¿›åº¦ç®¡ç†å™¨çš„ä¾¿æ·å‡½æ•°
    
    Args:
        total_iterations: æ€»è¿­ä»£æ¬¡æ•°
        save_dir: ä¿å­˜ç›®å½•
        experiment_name: å®éªŒåç§°
        save_frequency: ä¿å­˜é¢‘ç‡
        **kwargs: å…¶ä»–å‚æ•°
        
    Returns:
        è¿›åº¦ç®¡ç†å™¨å®ä¾‹
    """
    if save_dir is None:
        save_dir = f"experiments/{experiment_name}_{datetime.now().strftime('%Y%m%d_%H%M%S')}"
    
    return AdvancedProgressBar(
        total_iterations=total_iterations,
        save_dir=save_dir,
        save_frequency=save_frequency,
        **kwargs
    )

def test_progress_utils():
    """æµ‹è¯•è¿›åº¦å·¥å…·"""
    print("ğŸ§ª æµ‹è¯•è¿›åº¦å·¥å…·...")
    
    try:
        # åˆ›å»ºæµ‹è¯•ç›®å½•
        test_dir = "test_progress"
        
        # æµ‹è¯•è¿›åº¦æ¡ç®¡ç†å™¨
        progress_manager = create_progress_manager(
            total_iterations=20,
            save_dir=test_dir,
            experiment_name="test_exp",
            save_frequency=5
        )
        
        print("âœ… è¿›åº¦ç®¡ç†å™¨åˆ›å»ºæˆåŠŸ")
        
        # æ¨¡æ‹Ÿä¼˜åŒ–è¿‡ç¨‹
        progress_manager.start()
        
        for i in range(20):
            # æ¨¡æ‹ŸæŸå¤±
            losses = {
                'total_loss': 1.0 - i * 0.04 + np.random.normal(0, 0.01),
                'id_loss': 0.5 - i * 0.02 + np.random.normal(0, 0.005),
                'lpips_loss': 0.3 - i * 0.01 + np.random.normal(0, 0.003)
            }
            
            learning_rate = 0.01 * (0.95 ** (i // 5))
            
            # æ¨¡æ‹Ÿå›¾åƒæ•°æ®
            if i % 5 == 0:
                fake_image = torch.rand(1, 3, 64, 64)
                fake_latents = torch.rand(1, 4, 8, 8)
            else:
                fake_image = None
                fake_latents = None
            
            progress_manager.update(
                iteration=i,
                losses=losses,
                learning_rate=learning_rate,
                protected_image=fake_image,
                protected_latents=fake_latents,
                additional_data={'step_type': 'normal'}
            )
            
            time.sleep(0.05)  # æ¨¡æ‹Ÿå¤„ç†æ—¶é—´
        
        progress_manager.finish({'final_total_loss': 0.1})
        
        print("âœ… ä¼˜åŒ–è¿‡ç¨‹æ¨¡æ‹Ÿå®Œæˆ")
        
        # æµ‹è¯•æœ€ä½³è¿­ä»£æŸ¥æ‰¾
        best_iter = progress_manager.get_best_iteration('total_loss')
        print(f"âœ… æœ€ä½³è¿­ä»£: {best_iter}")
        
        # æµ‹è¯•æŸå¤±æ›²çº¿ç»˜åˆ¶
        plot_path = f"{test_dir}/loss_curves.png"
        progress_manager.plot_loss_curves(save_path=plot_path)
        print(f"âœ… æŸå¤±æ›²çº¿å·²ä¿å­˜: {plot_path}")
        
        # æµ‹è¯•æ—¥å¿—è®°å½•å™¨
        logger = OptimizationLogger(test_dir, "test_logging")
        logger.log_config({"learning_rate": 0.01, "batch_size": 1})
        logger.log_iteration(0, {'loss': 1.0}, lr=0.01)
        logger.log_milestone("æµ‹è¯•å®Œæˆ")
        
        print("âœ… æ—¥å¿—è®°å½•å™¨æµ‹è¯•å®Œæˆ")
        
        # æ¸…ç†æµ‹è¯•æ–‡ä»¶
        import shutil
        if os.path.exists(test_dir):
            shutil.rmtree(test_dir)
            print("âœ… æµ‹è¯•æ–‡ä»¶å·²æ¸…ç†")
        
        print("ğŸ‰ è¿›åº¦å·¥å…·æµ‹è¯•å…¨éƒ¨é€šè¿‡ï¼")
        return True
        
    except Exception as e:
        print(f"âŒ æµ‹è¯•å¤±è´¥: {e}")
        import traceback
        traceback.print_exc()
        return False

if __name__ == "__main__":
    test_progress_utils() 