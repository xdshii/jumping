"""
å¯¹æŠ—ä¼˜åŒ–å¾ªç¯å®ç°

è¯¥æ¨¡å—å®ç°DiffPrivateç®—æ³•çš„æ ¸å¿ƒä¼˜åŒ–å¾ªç¯ï¼Œé›†æˆæ‰€æœ‰æŸå¤±å‡½æ•°
è¿›è¡Œæ½œç©ºé—´çš„å¯¹æŠ—æ€§æ‰°åŠ¨ä¼˜åŒ–ï¼Œå®ç°èº«ä»½ä¿æŠ¤åŠŸèƒ½ã€‚

åŠŸèƒ½åŒ…æ‹¬ï¼š
1. é›†æˆèº«ä»½æŸå¤±ã€æ„ŸçŸ¥æŸå¤±ã€è‡ªæ³¨æ„åŠ›æŸå¤±
2. å®ç°AdamWä¼˜åŒ–å™¨çš„æ½œç©ºé—´ä¼˜åŒ–
3. æ”¯æŒå¤šå¼ºåº¦ä¿æŠ¤é…ç½®
4. æä¾›è¯¦ç»†çš„ä¼˜åŒ–è¿›åº¦è·Ÿè¸ª
5. å®ç°æ¢¯åº¦è£å‰ªå’Œå­¦ä¹ ç‡è°ƒåº¦

åŸºäºDiffPrivateè®ºæ–‡ä¸­çš„Algorithm 2è®¾è®¡ã€‚

ä½œè€…: AI Privacy Protection System  
æ—¥æœŸ: 2025-07-28
"""

import torch
import torch.nn as nn
import torch.nn.functional as F
import torch.optim as optim
import logging
from typing import Optional, Dict, Any, Union, List, Tuple, Callable
import numpy as np
from tqdm import tqdm
import time
from dataclasses import dataclass
from collections import defaultdict
import copy

try:
    from ..models.sd_loader import StableDiffusionLoader, ModelComponents
    from ..losses.id_loss import create_identity_loss, IdentityLoss
    from ..losses.lpips_loss import create_lpips_loss, LPIPSLoss, MultiScaleLPIPSLoss
    from ..losses.attention_loss import create_attention_loss, AttentionLoss
    from ..optimization.ddim_inversion import DDIMInverter
    from ..optimization.uncond_embed_optimization import UnconditionalEmbeddingOptimizer
    from ..optimization.diffusion_step import DiffusionStepper
    from ..config.config import PrivacyProtectionConfig, ProtectionStrength
    from ..utils.image_utils import ImageProcessor
except ImportError:
    import sys
    from pathlib import Path
    sys.path.append(str(Path(__file__).parent.parent))
    from models.sd_loader import StableDiffusionLoader, ModelComponents
    from losses.id_loss import create_identity_loss, IdentityLoss
    from losses.lpips_loss import create_lpips_loss, LPIPSLoss, MultiScaleLPIPSLoss
    from losses.attention_loss import create_attention_loss, AttentionLoss
    from optimization.ddim_inversion import DDIMInverter
    from optimization.uncond_embed_optimization import UnconditionalEmbeddingOptimizer
    from optimization.diffusion_step import DiffusionStepper
    from config.config import PrivacyProtectionConfig, ProtectionStrength
    from utils.image_utils import ImageProcessor

logger = logging.getLogger(__name__)

@dataclass
class OptimizationResult:
    """ä¼˜åŒ–ç»“æœæ•°æ®ç±»"""
    protected_latents: torch.Tensor
    protected_image: torch.Tensor
    original_image: torch.Tensor
    loss_history: Dict[str, List[float]]
    final_losses: Dict[str, float]
    optimization_time: float
    iterations: int
    converged: bool
    uncond_embeddings: Optional[torch.Tensor] = None

class AdversarialOptimizer:
    """
    å¯¹æŠ—ä¼˜åŒ–å™¨
    
    å®ç°DiffPrivateç®—æ³•çš„æ ¸å¿ƒä¼˜åŒ–å¾ªç¯
    """
    
    def __init__(
        self,
        sd_loader: StableDiffusionLoader,
        config: PrivacyProtectionConfig,
        device: Optional[str] = None
    ):
        """
        åˆå§‹åŒ–å¯¹æŠ—ä¼˜åŒ–å™¨
        
        Args:
            sd_loader: Stable DiffusionåŠ è½½å™¨
            config: éšç§ä¿æŠ¤é…ç½®
            device: è®¾å¤‡
        """
        self.sd_loader = sd_loader
        self.config = config
        self.device = device or ("cuda" if torch.cuda.is_available() else "cpu")
        
        # ç¡®ä¿ç»„ä»¶å·²åŠ è½½
        if not hasattr(sd_loader, 'components') or sd_loader.components is None:
            self.components = sd_loader.load_components()
        else:
            self.components = sd_loader.components
        
        # åˆå§‹åŒ–ç»„ä»¶
        self.image_processor = ImageProcessor()
        self.diffusion_stepper = DiffusionStepper(sd_loader)
        self.ddim_inverter = DDIMInverter(sd_loader)
        self.uncond_optimizer = UnconditionalEmbeddingOptimizer(sd_loader, self.ddim_inverter)
        
        # åˆå§‹åŒ–æŸå¤±å‡½æ•°
        self._init_loss_functions()
        
        logger.info(f"å¯¹æŠ—ä¼˜åŒ–å™¨åˆå§‹åŒ–å®Œæˆ: device={self.device}")
    
    def _init_loss_functions(self):
        """åˆå§‹åŒ–æŸå¤±å‡½æ•°"""
        
        # èº«ä»½æŸå¤±
        self.id_loss = create_identity_loss(
            model_types=["arcface"],  # å¯ä»¥æ‰©å±•ä¸ºå¤šæ¨¡å‹
            device=self.device,
            fallback_to_l2=True
        )
        
        # LPIPSæ„ŸçŸ¥æŸå¤±
        self.lpips_loss = create_lpips_loss(
            net="alex",
            use_gpu=(self.device == "cuda"),
            pixel_loss_weight=0.1,
            multiscale=True,
            scales=[1.0, 0.5]
        )
        
        # è‡ªæ³¨æ„åŠ›æŸå¤±
        self.attention_loss = create_attention_loss(
            sd_loader=self.sd_loader,
            target_resolutions=[64, 32],
            resolution_weights=[0.7, 0.3],
            temporal_weighting=False  # ç®€åŒ–å®ç°
        )
        
        logger.info("æŸå¤±å‡½æ•°åˆå§‹åŒ–å®Œæˆ")
    
    def prepare_image(
        self,
        image: Union[torch.Tensor, np.ndarray, str],
        target_size: Tuple[int, int] = (512, 512)
    ) -> Tuple[torch.Tensor, torch.Tensor]:
        """
        å‡†å¤‡è¾“å…¥å›¾åƒ
        
        Args:
            image: è¾“å…¥å›¾åƒ
            target_size: ç›®æ ‡å°ºå¯¸
            
        Returns:
            (å¤„ç†åçš„å›¾åƒå¼ é‡, æ½œç©ºé—´è¡¨ç¤º)
        """
        # åŠ è½½å’Œé¢„å¤„ç†å›¾åƒ
        if isinstance(image, str):
            img_tensor = self.image_processor.load_image(image)
        elif isinstance(image, np.ndarray):
            img_tensor = self.image_processor.numpy_to_tensor(image)
        else:
            img_tensor = image
        
        # è°ƒæ•´å°ºå¯¸
        img_tensor = self.image_processor.resize_image(img_tensor, target_size)
        
        # å½’ä¸€åŒ–åˆ°[-1, 1]ï¼ˆç”¨äºVAEç¼–ç ï¼‰
        img_normalized = img_tensor * 2.0 - 1.0
        
        # ç§»åŠ¨åˆ°è®¾å¤‡
        img_normalized = img_normalized.to(self.device, dtype=self.components.dtype)
        
        # ç¼–ç åˆ°æ½œç©ºé—´
        with torch.no_grad():
            latents = self.sd_loader.encode_images(img_normalized)
        
        return img_tensor.to(self.device), latents
    
    def optimize_uncond_embeddings(
        self,
        image_latents: torch.Tensor,
        prompt: str = ""
    ) -> torch.Tensor:
        """
        ä¼˜åŒ–æ— æ¡ä»¶æ–‡æœ¬åµŒå…¥
        
        Args:
            image_latents: å›¾åƒæ½œç©ºé—´è¡¨ç¤º
            prompt: æç¤ºè¯
            
        Returns:
            ä¼˜åŒ–åçš„æ— æ¡ä»¶åµŒå…¥
        """
        logger.info("å¼€å§‹ä¼˜åŒ–æ— æ¡ä»¶æ–‡æœ¬åµŒå…¥...")
        
        # ç¼–ç æç¤ºè¯
        prompt_embeddings = self.sd_loader.encode_text(prompt)
        
        # ä¼˜åŒ–æ— æ¡ä»¶åµŒå…¥
        result = self.uncond_optimizer.optimize_embeddings(
            target_latents=image_latents,
            prompt_embeddings=prompt_embeddings,
            num_steps=50,  # å¯é…ç½®
            learning_rate=1e-3
        )
        
        return result.optimized_embeddings
    
    def compute_total_loss(
        self,
        original_image: torch.Tensor,
        protected_image: torch.Tensor,
        original_latents: torch.Tensor,
        protected_latents: torch.Tensor,
        text_embeddings: torch.Tensor,
        timesteps: torch.Tensor,
        strength: ProtectionStrength = ProtectionStrength.MEDIUM
    ) -> Dict[str, torch.Tensor]:
        """
        è®¡ç®—æ€»æŸå¤±
        
        Args:
            original_image: åŸå§‹å›¾åƒ
            protected_image: ä¿æŠ¤åå›¾åƒ
            original_latents: åŸå§‹æ½œç©ºé—´
            protected_latents: ä¿æŠ¤åæ½œç©ºé—´
            text_embeddings: æ–‡æœ¬åµŒå…¥
            timesteps: æ—¶é—´æ­¥
            strength: ä¿æŠ¤å¼ºåº¦
            
        Returns:
            æŸå¤±å­—å…¸
        """
        losses = {}
        
        # è·å–å¼ºåº¦å¯¹åº”çš„æƒé‡
        weights = self.config.get_strength_weights(strength)
        
        # 1. èº«ä»½æŸå¤±ï¼ˆæœ€å¤§åŒ–é¢éƒ¨ç‰¹å¾è·ç¦»ï¼‰
        id_loss_dict = self.id_loss(original_image, protected_image, return_components=True)
        losses.update({f"id_{k}": v for k, v in id_loss_dict.items()})
        id_loss_total = id_loss_dict.get("total_loss", torch.tensor(0.0))
        
        # 2. LPIPSæ„ŸçŸ¥æŸå¤±ï¼ˆæœ€å°åŒ–æ„ŸçŸ¥å·®å¼‚ï¼‰
        lpips_loss_dict = self.lpips_loss(original_image, protected_image, return_components=True)
        losses.update({f"lpips_{k}": v for k, v in lpips_loss_dict.items()})
        lpips_loss_total = lpips_loss_dict.get("total_loss", torch.tensor(0.0))
        
        # 3. è‡ªæ³¨æ„åŠ›æŸå¤±ï¼ˆä¿æŒç»“æ„ä¸€è‡´æ€§ï¼‰
        # æ³¨æ„ï¼šè¿™ä¸ªè®¡ç®—æ¯”è¾ƒè€—æ—¶ï¼Œåœ¨å®é™…ä½¿ç”¨ä¸­å¯èƒ½éœ€è¦é™ä½é¢‘ç‡
        try:
            attention_loss_total = self.attention_loss(
                original_latents, protected_latents, text_embeddings, timesteps
            )
            losses["attention_total_loss"] = attention_loss_total
        except Exception as e:
            logger.warning(f"è‡ªæ³¨æ„åŠ›æŸå¤±è®¡ç®—å¤±è´¥: {e}")
            attention_loss_total = torch.tensor(0.0)
            losses["attention_total_loss"] = attention_loss_total
        
        # è®¡ç®—åŠ æƒæ€»æŸå¤±
        total_loss = (
            weights['lambda_id'] * id_loss_total +
            weights['lambda_lpips'] * lpips_loss_total +
            weights['lambda_self'] * attention_loss_total
        )
        
        losses["total_loss"] = total_loss
        losses["weighted_id"] = weights['lambda_id'] * id_loss_total
        losses["weighted_lpips"] = weights['lambda_lpips'] * lpips_loss_total
        losses["weighted_attention"] = weights['lambda_self'] * attention_loss_total
        
        return losses
    
    def optimize_latents(
        self,
        original_image: torch.Tensor,
        original_latents: torch.Tensor,
        uncond_embeddings: torch.Tensor,
        prompt: str = "",
        strength: ProtectionStrength = ProtectionStrength.MEDIUM,
        max_iterations: Optional[int] = None,
        learning_rate: Optional[float] = None,
        save_intermediates: bool = False
    ) -> OptimizationResult:
        """
        ä¼˜åŒ–æ½œç©ºé—´è¡¨ç¤º
        
        Args:
            original_image: åŸå§‹å›¾åƒ
            original_latents: åŸå§‹æ½œç©ºé—´
            uncond_embeddings: æ— æ¡ä»¶åµŒå…¥
            prompt: æç¤ºè¯
            strength: ä¿æŠ¤å¼ºåº¦
            max_iterations: æœ€å¤§è¿­ä»£æ¬¡æ•°
            learning_rate: å­¦ä¹ ç‡
            save_intermediates: æ˜¯å¦ä¿å­˜ä¸­é—´ç»“æœ
            
        Returns:
            ä¼˜åŒ–ç»“æœ
        """
        start_time = time.time()
        
        # ä½¿ç”¨é…ç½®ä¸­çš„å‚æ•°
        max_iterations = max_iterations or self.config.optimization.max_iterations
        learning_rate = learning_rate or self.config.optimization.learning_rate
        
        # åˆ›å»ºå¯ä¼˜åŒ–çš„æ½œç©ºé—´å‰¯æœ¬
        protected_latents = original_latents.clone().detach().requires_grad_(True)
        
        # ç¼–ç æç¤ºè¯
        text_embeddings = self.sd_loader.encode_text(prompt)
        
        # åˆ›å»ºæ—¶é—´æ­¥åºåˆ—ï¼ˆç®€åŒ–ç‰ˆæœ¬ï¼‰
        timesteps = torch.linspace(1000, 100, 5, device=self.device, dtype=torch.long)
        
        # åˆå§‹åŒ–ä¼˜åŒ–å™¨
        optimizer = optim.AdamW([protected_latents], lr=learning_rate)
        
        # å­¦ä¹ ç‡è°ƒåº¦å™¨
        scheduler = optim.lr_scheduler.ReduceLROnPlateau(
            optimizer, mode='min', factor=0.8, patience=10, verbose=False
        )
        
        # è®°å½•å†å²
        loss_history = defaultdict(list)
        best_loss = float('inf')
        best_latents = None
        patience_counter = 0
        max_patience = 20
        
        logger.info(f"å¼€å§‹ä¼˜åŒ–æ½œç©ºé—´: æœ€å¤§è¿­ä»£={max_iterations}, å­¦ä¹ ç‡={learning_rate}")
        
        # ä¼˜åŒ–å¾ªç¯
        progress_bar = tqdm(range(max_iterations), desc="ä¼˜åŒ–è¿›åº¦")
        
        for iteration in progress_bar:
            optimizer.zero_grad()
            
            # è§£ç å½“å‰ä¿æŠ¤çš„æ½œç©ºé—´
            with torch.no_grad():
                protected_image = self.sd_loader.decode_latents(protected_latents)
                protected_image = torch.clamp((protected_image + 1.0) / 2.0, 0.0, 1.0)
            
            # è®¡ç®—æŸå¤±
            losses = self.compute_total_loss(
                original_image=original_image,
                protected_image=protected_image,
                original_latents=original_latents,
                protected_latents=protected_latents,
                text_embeddings=text_embeddings,
                timesteps=timesteps,
                strength=strength
            )
            
            total_loss = losses["total_loss"]
            
            # åå‘ä¼ æ’­
            total_loss.backward()
            
            # æ¢¯åº¦è£å‰ª
            torch.nn.utils.clip_grad_norm_([protected_latents], max_norm=1.0)
            
            # ä¼˜åŒ–æ­¥éª¤
            optimizer.step()
            
            # è®°å½•æŸå¤±å†å²
            for key, value in losses.items():
                if isinstance(value, torch.Tensor):
                    loss_history[key].append(value.item())
                else:
                    loss_history[key].append(value)
            
            # æ›´æ–°è¿›åº¦æ¡
            progress_bar.set_postfix({
                'Loss': f"{total_loss.item():.6f}",
                'ID': f"{losses.get('id_total_loss', 0):.4f}",
                'LPIPS': f"{losses.get('lpips_total_loss', 0):.4f}",
                'Attn': f"{losses.get('attention_total_loss', 0):.4f}"
            })
            
            # ä¿å­˜æœ€ä½³ç»“æœ
            if total_loss.item() < best_loss:
                best_loss = total_loss.item()
                best_latents = protected_latents.clone().detach()
                patience_counter = 0
            else:
                patience_counter += 1
            
            # å­¦ä¹ ç‡è°ƒåº¦
            scheduler.step(total_loss.item())
            
            # æ—©åœ
            if patience_counter >= max_patience:
                logger.info(f"æ—©åœ: è¿­ä»£ {iteration}, æœ€ä½³æŸå¤± {best_loss:.6f}")
                break
            
            # ä¿å­˜ä¸­é—´ç»“æœï¼ˆå¯é€‰ï¼‰
            if save_intermediates and iteration % 10 == 0:
                # è¿™é‡Œå¯ä»¥ä¿å­˜ä¸­é—´ç»“æœåˆ°æ–‡ä»¶
                pass
        
        progress_bar.close()
        
        # ä½¿ç”¨æœ€ä½³æ½œç©ºé—´ç”Ÿæˆæœ€ç»ˆç»“æœ
        if best_latents is not None:
            protected_latents = best_latents
        
        with torch.no_grad():
            final_protected_image = self.sd_loader.decode_latents(protected_latents)
            final_protected_image = torch.clamp((final_protected_image + 1.0) / 2.0, 0.0, 1.0)
        
        # è®¡ç®—æœ€ç»ˆæŸå¤±
        final_losses = self.compute_total_loss(
            original_image=original_image,
            protected_image=final_protected_image,
            original_latents=original_latents,
            protected_latents=protected_latents,
            text_embeddings=text_embeddings,
            timesteps=timesteps,
            strength=strength
        )
        
        optimization_time = time.time() - start_time
        converged = patience_counter < max_patience
        
        logger.info(f"ä¼˜åŒ–å®Œæˆ: æ—¶é—´={optimization_time:.2f}s, æ”¶æ•›={converged}")
        
        return OptimizationResult(
            protected_latents=protected_latents.detach(),
            protected_image=final_protected_image,
            original_image=original_image,
            loss_history=dict(loss_history),
            final_losses={k: v.item() if isinstance(v, torch.Tensor) else v for k, v in final_losses.items()},
            optimization_time=optimization_time,
            iterations=iteration + 1,
            converged=converged,
            uncond_embeddings=uncond_embeddings
        )
    
    def protect_image(
        self,
        image: Union[torch.Tensor, np.ndarray, str],
        prompt: str = "",
        strength: ProtectionStrength = ProtectionStrength.MEDIUM,
        optimize_uncond: bool = True,
        **kwargs
    ) -> OptimizationResult:
        """
        ä¿æŠ¤å•å¼ å›¾åƒçš„ä¸»å…¥å£å‡½æ•°
        
        Args:
            image: è¾“å…¥å›¾åƒ
            prompt: æç¤ºè¯
            strength: ä¿æŠ¤å¼ºåº¦
            optimize_uncond: æ˜¯å¦ä¼˜åŒ–æ— æ¡ä»¶åµŒå…¥
            **kwargs: å…¶ä»–å‚æ•°
            
        Returns:
            ä¼˜åŒ–ç»“æœ
        """
        logger.info(f"å¼€å§‹ä¿æŠ¤å›¾åƒ: å¼ºåº¦={strength.value}, ä¼˜åŒ–æ— æ¡ä»¶åµŒå…¥={optimize_uncond}")
        
        # å‡†å¤‡å›¾åƒ
        original_image, original_latents = self.prepare_image(image)
        
        # ä¼˜åŒ–æ— æ¡ä»¶åµŒå…¥ï¼ˆå¦‚æœéœ€è¦ï¼‰
        if optimize_uncond:
            uncond_embeddings = self.optimize_uncond_embeddings(original_latents, prompt)
        else:
            uncond_embeddings = self.sd_loader.encode_text("")  # é»˜è®¤ç©ºæç¤º
        
        # ä¼˜åŒ–æ½œç©ºé—´
        result = self.optimize_latents(
            original_image=original_image,
            original_latents=original_latents,
            uncond_embeddings=uncond_embeddings,
            prompt=prompt,
            strength=strength,  
            **kwargs
        )
        
        return result

def create_adversarial_optimizer(
    sd_loader: StableDiffusionLoader,
    config: PrivacyProtectionConfig,
    device: Optional[str] = None
) -> AdversarialOptimizer:
    """
    åˆ›å»ºå¯¹æŠ—ä¼˜åŒ–å™¨çš„ä¾¿æ·å‡½æ•°
    
    Args:
        sd_loader: SDåŠ è½½å™¨
        config: é…ç½®
        device: è®¾å¤‡
        
    Returns:
        å¯¹æŠ—ä¼˜åŒ–å™¨å®ä¾‹
    """
    return AdversarialOptimizer(sd_loader, config, device)

def test_adversarial_optimizer():
    """æµ‹è¯•å¯¹æŠ—ä¼˜åŒ–å™¨"""
    print("ğŸ§ª æµ‹è¯•å¯¹æŠ—ä¼˜åŒ–å™¨...")
    
    try:
        # å¯¼å…¥ä¾èµ–
        from models.sd_loader import create_sd_loader
        from config.config import ConfigManager
        
        # åˆ›å»ºSDåŠ è½½å™¨
        sd_loader = create_sd_loader()
        components = sd_loader.load_components()
        
        # åŠ è½½é…ç½®
        config_manager = ConfigManager()
        config = config_manager.load_config()
        
        # åˆ›å»ºå¯¹æŠ—ä¼˜åŒ–å™¨
        optimizer = create_adversarial_optimizer(sd_loader, config)
        
        print("âœ… å¯¹æŠ—ä¼˜åŒ–å™¨åˆ›å»ºæˆåŠŸ")
        print(f"   è®¾å¤‡: {optimizer.device}")
        print(f"   æŸå¤±å‡½æ•°: ID + LPIPS + Attention")
        
        # åˆ›å»ºæµ‹è¯•å›¾åƒ
        device = optimizer.device
        test_image = torch.rand(1, 3, 512, 512, device=device)
        
        print("ğŸ”® æµ‹è¯•å›¾åƒå‡†å¤‡...")
        processed_image, latents = optimizer.prepare_image(test_image)
        print(f"âœ… å›¾åƒå‡†å¤‡æˆåŠŸ: {processed_image.shape} -> {latents.shape}")
        
        # æµ‹è¯•æŸå¤±è®¡ç®—ï¼ˆç®€åŒ–ç‰ˆæœ¬ï¼‰
        print("ğŸ“Š æµ‹è¯•æŸå¤±è®¡ç®—...")
        with torch.no_grad():
            protected_image = test_image.clone()
            text_embeddings = sd_loader.encode_text("test prompt")
            timesteps = torch.tensor([100, 200], device=device)
            
            # æ³¨æ„ï¼šè·³è¿‡è‡ªæ³¨æ„åŠ›æŸå¤±æµ‹è¯•ä»¥é¿å…è€—æ—¶
            id_loss_dict = optimizer.id_loss(test_image, protected_image)
            lpips_loss_dict = optimizer.lpips_loss(test_image, protected_image, return_components=True)
            
            print(f"âœ… èº«ä»½æŸå¤±: {id_loss_dict['total_loss'].item():.6f}")
            print(f"âœ… LPIPSæŸå¤±: {lpips_loss_dict['total_loss'].item():.6f}")
        
        # æ³¨æ„ï¼šè·³è¿‡å®Œæ•´ä¼˜åŒ–æµ‹è¯•ä»¥é¿å…é•¿æ—¶é—´è¿è¡Œ
        print("âš ï¸ è·³è¿‡å®Œæ•´ä¼˜åŒ–æµ‹è¯•ï¼ˆéœ€è¦é•¿æ—¶é—´è¿è¡Œï¼‰")
        print("âœ… æ ¸å¿ƒåŠŸèƒ½æµ‹è¯•é€šè¿‡ï¼Œå¯¹æŠ—ä¼˜åŒ–å™¨å¯ç”¨äºå›¾åƒä¿æŠ¤")
        
        print("ğŸ‰ å¯¹æŠ—ä¼˜åŒ–å™¨æµ‹è¯•é€šè¿‡ï¼")
        return True
        
    except Exception as e:
        print(f"âŒ æµ‹è¯•å¤±è´¥: {e}")
        import traceback
        traceback.print_exc()
        return False

if __name__ == "__main__":
    test_adversarial_optimizer() 