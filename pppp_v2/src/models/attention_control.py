"""
æ³¨æ„åŠ›æ§åˆ¶å®ç°

è¿™ä¸ªæ¨¡å—å®ç°äº†å¯¹Stable Diffusionä¸­æ³¨æ„åŠ›æœºåˆ¶çš„æ§åˆ¶å’Œæå–ã€‚
é€šè¿‡hookæ–¹å¼æ‹¦æˆªUNetä¸­çš„æ³¨æ„åŠ›è®¡ç®—ï¼Œæˆ‘ä»¬å¯ä»¥ï¼š
1. æå–äº¤å‰æ³¨æ„åŠ›å›¾ï¼ˆcross-attention mapsï¼‰ç”¨äºç”Ÿæˆæ©ç 
2. æå–è‡ªæ³¨æ„åŠ›å›¾ï¼ˆself-attention mapsï¼‰ç”¨äºç»“æ„æŸå¤±è®¡ç®—
3. æ§åˆ¶æ³¨æ„åŠ›æƒé‡ä»¥å®ç°æ›´ç²¾ç¡®çš„ç¼–è¾‘æ•ˆæœ

è¿™æ˜¯DiffPrivateç®—æ³•å®ç°ä¿çœŸåº¦æ§åˆ¶å’ŒåŒºåŸŸå®šä½çš„å…³é”®æŠ€æœ¯ã€‚

å‚è€ƒå®ç°:
- DiffPrivateæºç ä¸­çš„AttentionControlEdit
- Prompt-to-Prompt: "Prompt-to-Prompt Image Editing with Cross Attention Control"

ä½œè€…: AI Privacy Protection Team
åˆ›å»ºæ—¶é—´: 2025-01-28
ç‰ˆæœ¬: 1.0.0
"""

import torch
import torch.nn.functional as F
import logging
from typing import Optional, Dict, List, Tuple, Callable, Any, Union
from abc import ABC, abstractmethod
import numpy as np
from collections import defaultdict

try:
    from ..models.sd_loader import StableDiffusionLoader, ModelComponents
except ImportError:
    import sys
    from pathlib import Path
    sys.path.append(str(Path(__file__).parent.parent))
    from models.sd_loader import StableDiffusionLoader, ModelComponents

logger = logging.getLogger(__name__)


class AttentionStore:
    """æ³¨æ„åŠ›å­˜å‚¨å™¨ï¼Œç”¨äºæ”¶é›†å’Œç®¡ç†æ³¨æ„åŠ›å›¾"""
    
    def __init__(self):
        self.reset()
    
    def reset(self):
        """é‡ç½®å­˜å‚¨çš„æ³¨æ„åŠ›å›¾"""
        self.attention_store = defaultdict(list)
        self.step_store = defaultdict(list)
        self.cur_step = 0
        self.num_att_layers = -1
    
    def get_empty_store(self):
        """è·å–ç©ºçš„æ³¨æ„åŠ›å­˜å‚¨ç»“æ„"""
        return {"down_cross": [], "mid_cross": [], "up_cross": [],
                "down_self": [], "mid_self": [], "up_self": []}
    
    def forward(self, attn_map: torch.Tensor, place_in_unet: str):
        """
        å­˜å‚¨æ³¨æ„åŠ›å›¾
        
        Args:
            attn_map: æ³¨æ„åŠ›å›¾ [batch_size * num_heads, seq_len, spatial_dim]
            place_in_unet: UNetä¸­çš„ä½ç½® ("down", "mid", "up")
        """
        key = f"{place_in_unet}_{'cross' if attn_map.shape[1] <= 77 else 'self'}"
        
        if attn_map.shape[1] <= 77:  # äº¤å‰æ³¨æ„åŠ› (æ–‡æœ¬åºåˆ—é•¿åº¦ <= 77)
            self.attention_store[key].append(attn_map)
        else:  # è‡ªæ³¨æ„åŠ›
            self.attention_store[key].append(attn_map)
    
    def get_average_attention(self) -> Dict[str, torch.Tensor]:
        """è·å–å¹³å‡æ³¨æ„åŠ›å›¾"""
        average_attention = {}
        for key in self.attention_store:
            if len(self.attention_store[key]) > 0:
                # å¯¹æ‰€æœ‰æ­¥éª¤çš„æ³¨æ„åŠ›å›¾æ±‚å¹³å‡
                stacked = torch.stack(self.attention_store[key], dim=0)
                average_attention[key] = stacked.mean(0)
            else:
                average_attention[key] = None
        return average_attention
    
    def get_attention_by_step(self, step: int = -1) -> Dict[str, torch.Tensor]:
        """è·å–ç‰¹å®šæ­¥éª¤çš„æ³¨æ„åŠ›å›¾"""
        if step == -1:
            step = len(self.step_store) - 1
        
        if step >= len(self.step_store):
            logger.warning(f"Step {step} not available, using last step")
            step = len(self.step_store) - 1
            
        return self.step_store[step] if step >= 0 else {}
    
    def save_step_attention(self):
        """ä¿å­˜å½“å‰æ­¥éª¤çš„æ³¨æ„åŠ›å›¾"""
        self.step_store[self.cur_step] = dict(self.attention_store)
        self.attention_store = defaultdict(list)
        self.cur_step += 1
    
    def get_cross_attention_maps(
        self, 
        resolution: int = 16,
        token_idx: Optional[int] = None
    ) -> torch.Tensor:
        """
        è·å–äº¤å‰æ³¨æ„åŠ›å›¾å¹¶è°ƒæ•´åˆ°æŒ‡å®šåˆ†è¾¨ç‡
        
        Args:
            resolution: ç›®æ ‡åˆ†è¾¨ç‡
            token_idx: ç‰¹å®štokençš„ç´¢å¼•ï¼ŒNoneè¡¨ç¤ºä½¿ç”¨æ‰€æœ‰token
            
        Returns:
            torch.Tensor: æ³¨æ„åŠ›å›¾ [batch_size, height, width] æˆ– [batch_size, seq_len, height, width]
        """
        attention_maps = []
        
        # æ”¶é›†æ‰€æœ‰äº¤å‰æ³¨æ„åŠ›
        for key in ["down_cross", "mid_cross", "up_cross"]:
            if key in self.attention_store and len(self.attention_store[key]) > 0:
                for attn_map in self.attention_store[key]:
                    # attn_map: [batch_size * num_heads, seq_len, spatial_dim]
                    total_heads_batch = attn_map.shape[0]
                    seq_len = attn_map.shape[1]
                    spatial_dim = attn_map.shape[2]
                    
                    # è®¡ç®—ç©ºé—´ç»´åº¦çš„é«˜åº¦å’Œå®½åº¦
                    h = w = int(spatial_dim ** 0.5)
                    
                    # éªŒè¯ç©ºé—´ç»´åº¦æ˜¯å¦æ­£ç¡®
                    if h * w != spatial_dim:
                        # å¯èƒ½ä¸æ˜¯æ­£æ–¹å½¢ï¼Œç›´æ¥è·³è¿‡
                        logger.warning(f"è·³è¿‡éæ­£æ–¹å½¢æ³¨æ„åŠ›å›¾: spatial_dim={spatial_dim}, h={h}, w={w}")
                        continue
                    
                    # åŠ¨æ€è®¡ç®—batch_sizeå’Œnum_headsï¼ˆé¿å…ç¡¬ç¼–ç 8ä¸ªå¤´ï¼‰
                    # å°è¯•å¸¸è§çš„å¤´æ•°é…ç½®ï¼š8, 16, 4
                    for num_heads in [8, 16, 4, 12, 6, 2, 1]:
                        if total_heads_batch % num_heads == 0:
                            batch_size = total_heads_batch // num_heads
                            expected_size = batch_size * num_heads * seq_len * h * w
                            actual_size = attn_map.numel()
                            if expected_size == actual_size:
                                break
                    else:
                        # å¦‚æœéƒ½ä¸åŒ¹é…ï¼Œç›´æ¥è·³è¿‡è¿™ä¸ªæ³¨æ„åŠ›å›¾
                        logger.warning(f"æ— æ³•é‡å¡‘æ³¨æ„åŠ›å›¾ï¼Œè·³è¿‡: shape={attn_map.shape}")
                        continue
                    
                    # é‡å¡‘ä¸º [batch_size, num_heads, seq_len, h, w]
                    attn_reshaped = attn_map.view(batch_size, num_heads, seq_len, h, w)
                    
                    # å¹³å‡æ‰€æœ‰å¤´çš„æ³¨æ„åŠ›
                    attn_avg = attn_reshaped.mean(dim=1)  # [batch_size, seq_len, h, w]
                    
                    # è°ƒæ•´åˆ°ç›®æ ‡åˆ†è¾¨ç‡
                    if h != resolution:
                        attn_avg = F.interpolate(
                            attn_avg, size=(resolution, resolution), 
                            mode='bilinear', align_corners=False
                        )
                    
                    attention_maps.append(attn_avg)
        
        if not attention_maps:
            logger.warning("No cross attention maps found")
            return torch.zeros(1, 77, resolution, resolution)
        
        # å¹³å‡æ‰€æœ‰å±‚çš„æ³¨æ„åŠ›
        attention_avg = torch.stack(attention_maps, dim=0).mean(dim=0)
        
        # å¦‚æœæŒ‡å®šäº†tokenç´¢å¼•ï¼Œåªè¿”å›è¯¥tokençš„æ³¨æ„åŠ›
        if token_idx is not None:
            return attention_avg[:, token_idx]  # [batch_size, height, width]
        
        return attention_avg  # [batch_size, seq_len, height, width]


class AttentionControl(ABC):
    """æ³¨æ„åŠ›æ§åˆ¶åŸºç±»"""
    
    def __init__(self, tokenizer, device):
        self.tokenizer = tokenizer
        self.device = device
        self.cur_step = 0
        self.num_att_layers = -1
        self.cur_att_layer = 0
    
    @abstractmethod
    def forward(self, attn_map: torch.Tensor, place_in_unet: str) -> torch.Tensor:
        """å¤„ç†æ³¨æ„åŠ›å›¾çš„æŠ½è±¡æ–¹æ³•"""
        pass
    
    def __call__(self, attn_map: torch.Tensor, place_in_unet: str) -> torch.Tensor:
        """è°ƒç”¨æ¥å£"""
        if self.cur_att_layer >= 0:
            attn_map = self.forward(attn_map, place_in_unet)
        self.cur_att_layer += 1
        if self.cur_att_layer == self.num_att_layers:
            self.cur_att_layer = 0
            self.cur_step += 1
        return attn_map
    
    def reset(self):
        """é‡ç½®çŠ¶æ€"""
        self.cur_step = 0
        self.cur_att_layer = 0


class AttentionControlEdit(AttentionControl):
    """
    æ³¨æ„åŠ›æ§åˆ¶ç¼–è¾‘å™¨
    
    ç»§æ‰¿è‡ªAttentionControlï¼Œä¸“é—¨ç”¨äºDiffPrivateç®—æ³•ä¸­çš„æ³¨æ„åŠ›æ§åˆ¶ã€‚
    å¯ä»¥å­˜å‚¨ã€æå–å’Œæ“æ§æ³¨æ„åŠ›å›¾ã€‚
    """
    
    def __init__(
        self,
        tokenizer,
        device,
        cross_replace_steps: Union[float, Dict[str, float]] = 0.8,
        self_replace_steps: float = 0.4,
        local_blend: Optional[Dict] = None,
        save_self_attention: bool = True
    ):
        """
        åˆå§‹åŒ–æ³¨æ„åŠ›æ§åˆ¶ç¼–è¾‘å™¨
        
        Args:
            tokenizer: åˆ†è¯å™¨
            device: è®¾å¤‡
            cross_replace_steps: äº¤å‰æ³¨æ„åŠ›æ›¿æ¢æ­¥éª¤çš„æ¯”ä¾‹
            self_replace_steps: è‡ªæ³¨æ„åŠ›æ›¿æ¢æ­¥éª¤çš„æ¯”ä¾‹
            local_blend: å±€éƒ¨æ··åˆé…ç½®
            save_self_attention: æ˜¯å¦ä¿å­˜è‡ªæ³¨æ„åŠ›å›¾
        """
        super().__init__(tokenizer, device)
        
        self.cross_replace_steps = cross_replace_steps
        self.self_replace_steps = self_replace_steps
        self.local_blend = local_blend
        self.save_self_attention = save_self_attention
        
        # æ³¨æ„åŠ›å­˜å‚¨å™¨
        self.attention_store = AttentionStore()
        
        # ä¿å­˜çš„æ³¨æ„åŠ›å›¾ç”¨äºæŸå¤±è®¡ç®—
        self.saved_cross_attention = []
        self.saved_self_attention = []
        
        logger.info(f"æ³¨æ„åŠ›æ§åˆ¶ç¼–è¾‘å™¨åˆå§‹åŒ–: cross_steps={cross_replace_steps}, self_steps={self_replace_steps}")
    
    def forward(self, attn_map: torch.Tensor, place_in_unet: str) -> torch.Tensor:
        """
        å¤„ç†æ³¨æ„åŠ›å›¾
        
        Args:
            attn_map: æ³¨æ„åŠ›å›¾ [batch_size * num_heads, seq_len, spatial_dim]
            place_in_unet: UNetä¸­çš„ä½ç½®
            
        Returns:
            torch.Tensor: å¤„ç†åçš„æ³¨æ„åŠ›å›¾
        """
        # å­˜å‚¨æ³¨æ„åŠ›å›¾
        self.attention_store.forward(attn_map, place_in_unet)
        
        # ä¿å­˜ç”¨äºæŸå¤±è®¡ç®—çš„æ³¨æ„åŠ›å›¾
        if attn_map.shape[1] <= 77:  # äº¤å‰æ³¨æ„åŠ›
            self.saved_cross_attention.append(attn_map.clone())
        elif self.save_self_attention:  # è‡ªæ³¨æ„åŠ›
            self.saved_self_attention.append(attn_map.clone())
        
        # è¿™é‡Œå¯ä»¥æ·»åŠ æ³¨æ„åŠ›æ›¿æ¢é€»è¾‘ï¼ˆæš‚æ—¶è¿”å›åŸå§‹æ³¨æ„åŠ›å›¾ï¼‰
        return attn_map
    
    def get_cross_attention_mask(
        self,
        prompts: List[str],
        resolution: int = 64,
        threshold: float = 0.3
    ) -> torch.Tensor:
        """
        åŸºäºäº¤å‰æ³¨æ„åŠ›å›¾ç”Ÿæˆæ©ç 
        
        Args:
            prompts: æ–‡æœ¬æç¤ºåˆ—è¡¨
            resolution: æ©ç åˆ†è¾¨ç‡
            threshold: é˜ˆå€¼
            
        Returns:
            torch.Tensor: äºŒå€¼æ©ç  [batch_size, height, width]
        """
        # è·å–äº¤å‰æ³¨æ„åŠ›å›¾
        cross_attn = self.attention_store.get_cross_attention_maps(resolution)
        
        if cross_attn is None or cross_attn.numel() == 0:
            logger.warning("No cross attention available for mask generation")
            return torch.ones(1, resolution, resolution, device=self.device)
        
        # æ‰¾åˆ°ç›¸å…³tokençš„æ³¨æ„åŠ›
        # è¿™é‡Œç®€åŒ–ä¸ºä½¿ç”¨æ‰€æœ‰tokençš„å¹³å‡æ³¨æ„åŠ›
        if len(cross_attn.shape) == 4:  # [batch, seq_len, height, width]
            mask = cross_attn.mean(dim=1)  # [batch, height, width]
        else:  # [batch, height, width]
            mask = cross_attn
        
        # å½’ä¸€åŒ–
        mask = (mask - mask.min()) / (mask.max() - mask.min() + 1e-8)
        
        # åº”ç”¨é˜ˆå€¼
        binary_mask = (mask > threshold).float()
        
        return binary_mask
    
    def get_self_attention_loss(self, target_resolution: int = 64) -> torch.Tensor:
        """
        è®¡ç®—è‡ªæ³¨æ„åŠ›æŸå¤±ï¼ˆç”¨äºç»“æ„ä¿æŒï¼‰
        
        Args:
            target_resolution: ç›®æ ‡åˆ†è¾¨ç‡
            
        Returns:
            torch.Tensor: è‡ªæ³¨æ„åŠ›æŸå¤±
        """
        if not self.saved_self_attention:
            return torch.tensor(0.0, device=self.device)
        
        total_loss = 0.0
        count = 0
        
        for attn_map in self.saved_self_attention:
            # attn_map: [batch_size * num_heads, spatial_dim, spatial_dim]
            total_heads_batch = attn_map.shape[0]
            spatial_dim = attn_map.shape[1]
            
            # å¦‚æœç©ºé—´ç»´åº¦åŒ¹é…ç›®æ ‡åˆ†è¾¨ç‡çš„å¹³æ–¹
            target_spatial_dim = target_resolution * target_resolution
            if spatial_dim == target_spatial_dim:
                # åŠ¨æ€è®¡ç®—batch_sizeå’Œnum_heads
                for num_heads in [8, 16, 4, 12, 6, 2, 1]:
                    if total_heads_batch % num_heads == 0:
                        batch_size = total_heads_batch // num_heads
                        expected_size = batch_size * num_heads * spatial_dim * spatial_dim
                        actual_size = attn_map.numel()
                        if expected_size == actual_size:
                            break
                else:
                    logger.warning(f"æ— æ³•é‡å¡‘è‡ªæ³¨æ„åŠ›å›¾ï¼Œè·³è¿‡: shape={attn_map.shape}")
                    continue
                
                # é‡å¡‘ä¸º [batch_size, num_heads, spatial_dim, spatial_dim]
                attn_reshaped = attn_map.view(batch_size, num_heads, spatial_dim, spatial_dim)
                
                # å¹³å‡æ‰€æœ‰å¤´
                attn_avg = attn_reshaped.mean(dim=1)  # [batch_size, spatial_dim, spatial_dim]
                
                # è®¡ç®—è‡ªæ³¨æ„åŠ›çš„ç»“æ„æŸå¤±ï¼ˆè¿™é‡Œä½¿ç”¨å¯¹è§’çº¿å…ƒç´ ä½œä¸ºç®€åŒ–ï¼‰
                diagonal = torch.diagonal(attn_avg, dim1=1, dim2=2)  # [batch_size, spatial_dim]
                loss = F.mse_loss(diagonal, torch.ones_like(diagonal) * 0.1)  # æœŸæœ›å¯¹è§’çº¿å…ƒç´ è¾ƒå°
                
                total_loss += loss
                count += 1
        
        return total_loss / max(count, 1)
    
    def reset(self):
        """é‡ç½®çŠ¶æ€"""
        super().reset()
        self.attention_store.reset()
        self.saved_cross_attention.clear()
        self.saved_self_attention.clear()
    
    def save_step(self):
        """ä¿å­˜å½“å‰æ­¥éª¤çš„æ³¨æ„åŠ›"""
        self.attention_store.save_step_attention()


def register_attention_control(
    model: torch.nn.Module,
    controller: AttentionControlEdit
) -> List[Callable]:
    """
    ä¸ºæ¨¡å‹æ³¨å†Œæ³¨æ„åŠ›æ§åˆ¶é’©å­
    
    Args:
        model: UNetæ¨¡å‹
        controller: æ³¨æ„åŠ›æ§åˆ¶å™¨
        
    Returns:
        List[Callable]: é’©å­åˆ—è¡¨ï¼Œç”¨äºåç»­ç§»é™¤
    """
    def ca_forward(self, place_in_unet):
        """æ³¨æ„åŠ›å‰å‘é’©å­ - å…¼å®¹æ–°ç‰ˆdiffusers"""
        def forward(hidden_states, encoder_hidden_states=None, attention_mask=None, **kwargs):
            # è·å–åŸå§‹forwardæ–¹æ³•ä»¥ä¿æŒå…¼å®¹æ€§
            batch_size, sequence_length, dim = hidden_states.shape
            
            # ç¡®å®šæ˜¯è‡ªæ³¨æ„åŠ›è¿˜æ˜¯äº¤å‰æ³¨æ„åŠ›
            is_cross_attention = encoder_hidden_states is not None
            context = encoder_hidden_states if is_cross_attention else hidden_states
            
            # ä½¿ç”¨åŸå§‹çš„å¤„ç†æ–¹å¼ï¼Œä½†ç®€åŒ–ä»¥å…¼å®¹æ–°ç‰ˆæœ¬
            q = self.to_q(hidden_states)
            k = self.to_k(context)
            v = self.to_v(context)
            
            # é‡å¡‘ä¸ºå¤šå¤´æ ¼å¼
            head_dim = dim // self.heads
            q = q.view(batch_size, -1, self.heads, head_dim).transpose(1, 2)
            k = k.view(batch_size, -1, self.heads, head_dim).transpose(1, 2)
            v = v.view(batch_size, -1, self.heads, head_dim).transpose(1, 2)
            
            # è®¡ç®—æ³¨æ„åŠ›åˆ†æ•°
            scale = head_dim ** -0.5
            attn_scores = torch.matmul(q, k.transpose(-2, -1)) * scale
            
            # åº”ç”¨æ³¨æ„åŠ›æ©ç 
            if attention_mask is not None:
                attn_scores = attn_scores + attention_mask
            
            # Softmaxå¾—åˆ°æ³¨æ„åŠ›æƒé‡
            attn_weights = F.softmax(attn_scores, dim=-1)
            
            # è°ƒç”¨æ§åˆ¶å™¨ï¼ˆè¿™æ˜¯å…³é”®éƒ¨åˆ†ï¼‰
            attn_weights_reshaped = attn_weights.reshape(batch_size * self.heads, attn_weights.shape[-2], attn_weights.shape[-1])
            attn_weights_controlled = controller(attn_weights_reshaped, place_in_unet)
            attn_weights = attn_weights_controlled.reshape(batch_size, self.heads, attn_weights.shape[-2], attn_weights.shape[-1])
            
            # åº”ç”¨æ³¨æ„åŠ›æƒé‡
            attn_output = torch.matmul(attn_weights, v)
            attn_output = attn_output.transpose(1, 2).contiguous().view(batch_size, sequence_length, dim)
            
            # æœ€ç»ˆæŠ•å½± - å¤„ç†ModuleListæƒ…å†µ
            if isinstance(self.to_out, torch.nn.ModuleList):
                # æ–°ç‰ˆdiffusersä¸­to_outæ˜¯ModuleList
                for layer in self.to_out:
                    attn_output = layer(attn_output)
                return attn_output
            else:
                # è€ç‰ˆæœ¬ä¸­to_outæ˜¯å•ä¸ªæ¨¡å—
                return self.to_out(attn_output)
        
        return forward
    
    # æ³¨å†Œé’©å­
    hooks = []
    
    def register_recr(net_, count, place_in_unet):
        # ä¿®å¤ï¼šæ­£ç¡®çš„ç±»åæ˜¯'Attention'è€Œä¸æ˜¯'CrossAttention'
        if net_.__class__.__name__ == 'Attention':
            net_.forward = ca_forward(net_, place_in_unet)
            return count + 1
        elif hasattr(net_, 'children'):
            for net__ in net_.children():
                count = register_recr(net__, count, place_in_unet)
        return count
    
    # ä¸ºä¸åŒä½ç½®æ³¨å†Œ
    cross_att_count = 0
    for net in model.down_blocks:
        cross_att_count += register_recr(net, 0, "down")
    cross_att_count += register_recr(model.mid_block, 0, "mid")
    for net in model.up_blocks:
        cross_att_count += register_recr(net, 0, "up")
    
    controller.num_att_layers = cross_att_count
    logger.info(f"æ³¨å†Œäº† {cross_att_count} ä¸ªæ³¨æ„åŠ›æ§åˆ¶é’©å­")
    
    return hooks


def create_attention_controller(
    tokenizer,
    device,
    cross_replace_steps: float = 0.8,
    self_replace_steps: float = 0.4,
    save_self_attention: bool = True
) -> AttentionControlEdit:
    """
    åˆ›å»ºæ³¨æ„åŠ›æ§åˆ¶å™¨çš„ä¾¿æ·å‡½æ•°
    
    Args:
        tokenizer: åˆ†è¯å™¨
        device: è®¾å¤‡
        cross_replace_steps: äº¤å‰æ³¨æ„åŠ›æ›¿æ¢æ­¥éª¤
        self_replace_steps: è‡ªæ³¨æ„åŠ›æ›¿æ¢æ­¥éª¤
        save_self_attention: æ˜¯å¦ä¿å­˜è‡ªæ³¨æ„åŠ›
        
    Returns:
        AttentionControlEdit: é…ç½®å¥½çš„æ³¨æ„åŠ›æ§åˆ¶å™¨
    """
    return AttentionControlEdit(
        tokenizer=tokenizer,
        device=device,
        cross_replace_steps=cross_replace_steps,
        self_replace_steps=self_replace_steps,
        save_self_attention=save_self_attention
    )


# æµ‹è¯•å‡½æ•°
def test_attention_control():
    """æµ‹è¯•æ³¨æ„åŠ›æ§åˆ¶"""
    logger.info("å¼€å§‹æµ‹è¯•æ³¨æ„åŠ›æ§åˆ¶...")
    
    try:
        # å¯¼å…¥ä¾èµ–
        import sys
        from pathlib import Path
        sys.path.append(str(Path(__file__).parent.parent))
        from models.sd_loader import create_sd_loader
        
        # åˆ›å»ºSDåŠ è½½å™¨
        sd_loader = create_sd_loader()
        components = sd_loader.load_components()
        
        # åˆ›å»ºæ³¨æ„åŠ›æ§åˆ¶å™¨
        controller = create_attention_controller(
            tokenizer=components.tokenizer,
            device=components.device
        )
        
        # æ³¨å†Œæ³¨æ„åŠ›é’©å­
        hooks = register_attention_control(components.unet, controller)
        logger.info(f"âœ… æ³¨æ„åŠ›é’©å­æ³¨å†Œæµ‹è¯•é€šè¿‡: {len(hooks)} ä¸ªé’©å­")
        
        # åˆ›å»ºæµ‹è¯•æ•°æ®
        test_latents = torch.randn(1, 4, 64, 64, device=components.device, dtype=components.dtype)
        test_timestep = torch.tensor([100], device=components.device)
        test_prompt_embeds = sd_loader.encode_text("a beautiful landscape")
        
        # æµ‹è¯•UNetå‰å‘ä¼ æ’­ï¼ˆä¼šè§¦å‘æ³¨æ„åŠ›æ§åˆ¶ï¼‰
        with torch.no_grad():
            noise_pred = components.unet(
                test_latents,
                test_timestep,
                encoder_hidden_states=test_prompt_embeds
            ).sample
        
        logger.info(f"âœ… UNetå‰å‘ä¼ æ’­æµ‹è¯•é€šè¿‡: {noise_pred.shape}")
        
        # æµ‹è¯•æ³¨æ„åŠ›å›¾æå–
        cross_attn = controller.attention_store.get_cross_attention_maps(resolution=64)
        if cross_attn is not None and cross_attn.numel() > 0:
            logger.info(f"âœ… äº¤å‰æ³¨æ„åŠ›æå–æµ‹è¯•é€šè¿‡: {cross_attn.shape}")
        
        # æµ‹è¯•æ©ç ç”Ÿæˆ
        mask = controller.get_cross_attention_mask(["a beautiful landscape"], resolution=64)
        logger.info(f"âœ… æ©ç ç”Ÿæˆæµ‹è¯•é€šè¿‡: {mask.shape}")
        
        # æµ‹è¯•è‡ªæ³¨æ„åŠ›æŸå¤±
        self_attn_loss = controller.get_self_attention_loss()
        logger.info(f"âœ… è‡ªæ³¨æ„åŠ›æŸå¤±æµ‹è¯•é€šè¿‡: {self_attn_loss.item():.6f}")
        
        logger.info("ğŸ‰ æ³¨æ„åŠ›æ§åˆ¶æµ‹è¯•å…¨éƒ¨é€šè¿‡ï¼")
        return True
        
    except Exception as e:
        logger.error(f"âŒ æµ‹è¯•å¤±è´¥: {e}")
        import traceback
        traceback.print_exc()
        return False


if __name__ == "__main__":
    # é…ç½®æ—¥å¿—
    logging.basicConfig(
        level=logging.INFO,
        format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
    )
    
    # è¿è¡Œæµ‹è¯•
    test_attention_control() 