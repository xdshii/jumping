"""
DDIMåå‘é‡‡æ ·å®ç°

DDIMï¼ˆDenoising Diffusion Implicit Modelsï¼‰åå‘é‡‡æ ·æ˜¯DiffPrivateç®—æ³•çš„æ ¸å¿ƒç»„ä»¶ã€‚
é€šè¿‡åå‘DDIMè¿‡ç¨‹ï¼Œæˆ‘ä»¬å¯ä»¥å°†çœŸå®å›¾åƒç²¾ç¡®åœ°åè½¬åˆ°å™ªå£°ç©ºé—´ï¼Œè¿™ä¸ºåç»­çš„
å¯¹æŠ—æ€§ä¼˜åŒ–æä¾›äº†èµ·ç‚¹ã€‚

å‚è€ƒè®ºæ–‡:
- DDIM: Song et al. "Denoising Diffusion Implicit Models" (ICLR 2021)
- DiffPrivate: Wu et al. "DiffPrivate: Invisible Adversarial Samples for Privacy Protection"

ä½œè€…: AI Privacy Protection Team
åˆ›å»ºæ—¶é—´: 2025-01-28
ç‰ˆæœ¬: 1.0.0
"""

import torch
import torch.nn.functional as F
import logging
from typing import Optional, List, Tuple, Union, Callable
from tqdm import tqdm
import numpy as np

try:
    from ..models.sd_loader import StableDiffusionLoader, ModelComponents
except ImportError:
    # å¦‚æœç›¸å¯¹å¯¼å…¥å¤±è´¥ï¼Œå°è¯•ç»å¯¹å¯¼å…¥
    import sys
    from pathlib import Path
    sys.path.append(str(Path(__file__).parent.parent))
    from models.sd_loader import StableDiffusionLoader, ModelComponents

logger = logging.getLogger(__name__)


class DDIMInverter:
    """DDIMåå‘é‡‡æ ·å™¨"""
    
    def __init__(
        self,
        sd_loader: StableDiffusionLoader,
        num_inference_steps: int = 25,  # å¹³è¡¡è´¨é‡å’Œå†…å­˜ä½¿ç”¨
        guidance_scale: float = 7.5,
        eta: float = 0.0
    ):
        """
        åˆå§‹åŒ–DDIMåå‘é‡‡æ ·å™¨
        
        Args:
            sd_loader: Stable DiffusionåŠ è½½å™¨
            num_inference_steps: æ¨ç†æ­¥æ•°
            guidance_scale: å¼•å¯¼å°ºåº¦
            eta: DDIMå‚æ•°ï¼Œ0ä¸ºå®Œå…¨ç¡®å®šæ€§ï¼Œ1ä¸ºæ ‡å‡†DDPM
        """
        self.sd_loader = sd_loader
        self.num_inference_steps = num_inference_steps
        self.guidance_scale = guidance_scale
        self.eta = eta
        
        # åŠ è½½æ¨¡å‹ç»„ä»¶
        self.components = sd_loader.load_components()
        
        # è®¾ç½®è°ƒåº¦å™¨
        self.scheduler = self.components.scheduler
        self.scheduler.set_timesteps(num_inference_steps)
        self.timesteps = self.scheduler.timesteps
        
        logger.info(f"DDIMåå‘é‡‡æ ·å™¨åˆå§‹åŒ–: steps={num_inference_steps}, guidance={guidance_scale}, eta={eta}")
    
    def get_noise_pred(
        self,
        latents: torch.Tensor,
        timestep: torch.Tensor,
        text_embeddings: torch.Tensor,
        guidance_scale: Optional[float] = None
    ) -> torch.Tensor:
        """
        è·å–å™ªå£°é¢„æµ‹
        
        Args:
            latents: æ½œåœ¨è¡¨ç¤º [B, C, H, W]
            timestep: æ—¶é—´æ­¥ [B] æˆ– scalar
            text_embeddings: æ–‡æœ¬åµŒå…¥ [B, seq_len, dim]
            guidance_scale: å¼•å¯¼å°ºåº¦ï¼ŒNoneä½¿ç”¨é»˜è®¤å€¼
            
        Returns:
            torch.Tensor: é¢„æµ‹çš„å™ªå£° [B, C, H, W]
        """
        if guidance_scale is None:
            guidance_scale = self.guidance_scale
        
        # ç¡®ä¿timestepæ˜¯æ­£ç¡®çš„å½¢çŠ¶å’Œè®¾å¤‡
        if timestep.dim() == 0:
            timestep = timestep.unsqueeze(0).repeat(latents.shape[0]).to(latents.device)
        elif timestep.shape[0] != latents.shape[0]:
            timestep = timestep.repeat(latents.shape[0]).to(latents.device)
        else:
            timestep = timestep.to(latents.device)
        
        if guidance_scale <= 1.0:
            # æ— å¼•å¯¼æƒ…å†µ
            noise_pred = self.components.unet(
                latents,
                timestep,
                encoder_hidden_states=text_embeddings
            ).sample
        else:
            # æœ‰å¼•å¯¼æƒ…å†µï¼šéœ€è¦æ¡ä»¶å’Œæ— æ¡ä»¶é¢„æµ‹
            # åˆ›å»ºæ— æ¡ä»¶åµŒå…¥ï¼ˆç©ºæ–‡æœ¬ï¼‰
            batch_size = latents.shape[0]
            uncond_embeddings = self.sd_loader.encode_text("")
            uncond_embeddings = uncond_embeddings.repeat(batch_size, 1, 1)
            
            # æ‹¼æ¥æ¡ä»¶å’Œæ— æ¡ä»¶è¾“å…¥
            latents_input = torch.cat([latents, latents], dim=0)
            text_embeddings_input = torch.cat([uncond_embeddings, text_embeddings], dim=0)
            timestep_input = torch.cat([timestep, timestep], dim=0)
            
            # é¢„æµ‹å™ªå£°
            noise_pred = self.components.unet(
                latents_input,
                timestep_input,
                encoder_hidden_states=text_embeddings_input
            ).sample
            
            # åˆ†ç¦»æ¡ä»¶å’Œæ— æ¡ä»¶é¢„æµ‹
            noise_pred_uncond, noise_pred_cond = noise_pred.chunk(2, dim=0)
            
            # åº”ç”¨å¼•å¯¼
            noise_pred = noise_pred_uncond + guidance_scale * (noise_pred_cond - noise_pred_uncond)
        
        return noise_pred
    
    def ddim_step_reverse(
        self,
        latents: torch.Tensor,
        noise_pred: torch.Tensor,
        timestep: int,
        prev_timestep: int
    ) -> torch.Tensor:
        """
        DDIMåå‘æ­¥éª¤ï¼šä»x_tåˆ°x_{t+1}
        
        Args:
            latents: å½“å‰æ½œåœ¨è¡¨ç¤º x_t
            noise_pred: é¢„æµ‹çš„å™ªå£°
            timestep: å½“å‰æ—¶é—´æ­¥ t
            prev_timestep: ä¸‹ä¸€ä¸ªæ—¶é—´æ­¥ t+1
            
        Returns:
            torch.Tensor: ä¸‹ä¸€æ­¥çš„æ½œåœ¨è¡¨ç¤º x_{t+1}
        """
        # è·å–alphaå€¼
        alpha_prod_t = self.scheduler.alphas_cumprod[timestep]
        alpha_prod_t_prev = self.scheduler.alphas_cumprod[prev_timestep] if prev_timestep >= 0 else self.scheduler.final_alpha_cumprod
        
        beta_prod_t = 1 - alpha_prod_t
        beta_prod_t_prev = 1 - alpha_prod_t_prev
        
        if self.scheduler.config.prediction_type == "epsilon":
            # æ ‡å‡†æƒ…å†µï¼šé¢„æµ‹å™ªå£°
            pred_original_sample = (latents - beta_prod_t ** 0.5 * noise_pred) / alpha_prod_t ** 0.5
        elif self.scheduler.config.prediction_type == "sample":
            # ç›´æ¥é¢„æµ‹åŸå§‹æ ·æœ¬
            pred_original_sample = noise_pred
        elif self.scheduler.config.prediction_type == "v_prediction":
            # v-å‚æ•°åŒ–
            pred_original_sample = (alpha_prod_t ** 0.5) * latents - (beta_prod_t ** 0.5) * noise_pred
        else:
            raise ValueError(f"Unknown prediction type: {self.scheduler.config.prediction_type}")
        
        # è®¡ç®—æ–¹å‘å‘é‡
        pred_sample_direction = (beta_prod_t_prev ** 0.5) * noise_pred
        
        # åå‘DDIMæ­¥éª¤
        prev_sample = (alpha_prod_t_prev ** 0.5) * pred_original_sample + pred_sample_direction
        
        return prev_sample
    
    def invert_image(
        self,
        image: torch.Tensor,
        prompt: str = "",
        return_intermediates: bool = False,
        callback: Optional[Callable[[int, torch.Tensor], None]] = None
    ) -> Union[torch.Tensor, Tuple[torch.Tensor, List[torch.Tensor]]]:
        """
        åè½¬å›¾åƒåˆ°å™ªå£°ç©ºé—´
        
        Args:
            image: è¾“å…¥å›¾åƒ [B, C, H, W], å€¼åŸŸ[0, 1]
            prompt: æ–‡æœ¬æç¤º
            return_intermediates: æ˜¯å¦è¿”å›ä¸­é—´ç»“æœ
            callback: å›è°ƒå‡½æ•°ï¼Œç”¨äºç›‘æ§è¿›åº¦
            
        Returns:
            torch.Tensor: åè½¬åçš„å™ªå£° [B, C_latent, H//8, W//8]
            å¦‚æœreturn_intermediates=Trueï¼Œè¿˜è¿”å›ä¸­é—´æ½œåœ¨è¡¨ç¤ºåˆ—è¡¨
        """
        logger.info(f"å¼€å§‹DDIMåå‘é‡‡æ ·: image.shape={image.shape}, prompt='{prompt}'")
        
        # ç¼–ç å›¾åƒåˆ°æ½œç©ºé—´
        latents = self.sd_loader.encode_images(image)
        
        # ç¼–ç æ–‡æœ¬
        text_embeddings = self.sd_loader.encode_text(prompt)
        text_embeddings = text_embeddings.repeat(latents.shape[0], 1, 1)
        
        # å­˜å‚¨ä¸­é—´ç»“æœ
        intermediates = [latents.clone()] if return_intermediates else []
        
        # åå‘DDIMå¾ªç¯
        with torch.no_grad():
            for i, timestep in enumerate(tqdm(self.timesteps, desc="DDIM Inversion")):
                # é¢„æµ‹å™ªå£°
                noise_pred = self.get_noise_pred(latents, timestep, text_embeddings)
                
                # è®¡ç®—ä¸‹ä¸€ä¸ªæ—¶é—´æ­¥
                if i < len(self.timesteps) - 1:
                    next_timestep = self.timesteps[i + 1]
                else:
                    # æœ€åä¸€æ­¥ï¼Œåˆ°çº¯å™ªå£°
                    next_timestep = torch.tensor(self.scheduler.config.num_train_timesteps - 1)
                
                # æ‰§è¡Œåå‘æ­¥éª¤
                latents = self.ddim_step_reverse(
                    latents, noise_pred, timestep.item(), next_timestep.item()
                )
                
                # å­˜å‚¨ä¸­é—´ç»“æœ
                if return_intermediates:
                    intermediates.append(latents.clone())
                
                # è°ƒç”¨å›è°ƒå‡½æ•°
                if callback is not None:
                    callback(i + 1, latents.clone())
        
        logger.info(f"DDIMåå‘é‡‡æ ·å®Œæˆ: noise.shape={latents.shape}")
        
        if return_intermediates:
            return latents, intermediates
        else:
            return latents
    
    def forward_sample(
        self,
        noise: torch.Tensor,
        prompt: str = "",
        num_inference_steps: Optional[int] = None,
        guidance_scale: Optional[float] = None,
        callback: Optional[Callable[[int, torch.Tensor], None]] = None
    ) -> torch.Tensor:
        """
        ä»å™ªå£°å‰å‘é‡‡æ ·ç”Ÿæˆå›¾åƒï¼ˆç”¨äºéªŒè¯åè½¬è´¨é‡ï¼‰
        
        Args:
            noise: è¾“å…¥å™ªå£° [B, C_latent, H//8, W//8]
            prompt: æ–‡æœ¬æç¤º
            num_inference_steps: æ¨ç†æ­¥æ•°
            guidance_scale: å¼•å¯¼å°ºåº¦
            callback: å›è°ƒå‡½æ•°
            
        Returns:
            torch.Tensor: ç”Ÿæˆçš„å›¾åƒ [B, C, H, W]
        """
        if num_inference_steps is None:
            num_inference_steps = self.num_inference_steps
        if guidance_scale is None:
            guidance_scale = self.guidance_scale
        
        logger.info(f"å¼€å§‹DDIMå‰å‘é‡‡æ ·: noise.shape={noise.shape}")
        
        # è®¾ç½®è°ƒåº¦å™¨
        self.scheduler.set_timesteps(num_inference_steps)
        timesteps = self.scheduler.timesteps
        
        # ç¼–ç æ–‡æœ¬
        text_embeddings = self.sd_loader.encode_text(prompt)
        text_embeddings = text_embeddings.repeat(noise.shape[0], 1, 1)
        
        # åˆå§‹åŒ–æ½œåœ¨è¡¨ç¤º
        latents = noise.clone()
        
        # å‰å‘DDIMå¾ªç¯
        with torch.no_grad():
            for i, timestep in enumerate(tqdm(timesteps, desc="DDIM Sampling")):
                # é¢„æµ‹å™ªå£°
                noise_pred = self.get_noise_pred(latents, timestep, text_embeddings, guidance_scale)
                
                # DDIMæ­¥éª¤
                latents = self.scheduler.step(noise_pred, timestep, latents, eta=self.eta).prev_sample
                
                # è°ƒç”¨å›è°ƒå‡½æ•°
                if callback is not None:
                    callback(i + 1, latents.clone())
        
        # è§£ç åˆ°å›¾åƒ
        images = self.sd_loader.decode_latents(latents)
        
        logger.info(f"DDIMå‰å‘é‡‡æ ·å®Œæˆ: images.shape={images.shape}")
        return images
    
    def test_inversion_quality(
        self,
        image: torch.Tensor,
        prompt: str = "",
        num_test_steps: Optional[int] = None
    ) -> Tuple[torch.Tensor, torch.Tensor, float]:
        """
        æµ‹è¯•åè½¬è´¨é‡
        
        Args:
            image: æµ‹è¯•å›¾åƒ
            prompt: æ–‡æœ¬æç¤º
            num_test_steps: æµ‹è¯•æ­¥æ•°
            
        Returns:
            Tuple[torch.Tensor, torch.Tensor, float]: (åŸå›¾, é‡å»ºå›¾, MSEè¯¯å·®)
        """
        if num_test_steps is None:
            num_test_steps = self.num_inference_steps
        
        logger.info("æµ‹è¯•DDIMåè½¬è´¨é‡...")
        
        # åè½¬å›¾åƒ
        noise = self.invert_image(image, prompt)
        
        # é‡å»ºå›¾åƒ
        reconstructed = self.forward_sample(noise, prompt, num_test_steps)
        
        # è®¡ç®—è¯¯å·®
        mse_error = F.mse_loss(image, reconstructed).item()
        
        logger.info(f"åè½¬è´¨é‡æµ‹è¯•å®Œæˆ: MSE={mse_error:.6f}")
        
        return image, reconstructed, mse_error


def create_ddim_inverter(
    sd_loader: StableDiffusionLoader,
    num_inference_steps: int = 50,
    guidance_scale: float = 7.5,
    eta: float = 0.0
) -> DDIMInverter:
    """
    åˆ›å»ºDDIMåå‘é‡‡æ ·å™¨çš„ä¾¿æ·å‡½æ•°
    
    Args:
        sd_loader: SDåŠ è½½å™¨
        num_inference_steps: æ¨ç†æ­¥æ•°
        guidance_scale: å¼•å¯¼å°ºåº¦
        eta: DDIMå‚æ•°
        
    Returns:
        DDIMInverter: é…ç½®å¥½çš„DDIMåå‘é‡‡æ ·å™¨
    """
    return DDIMInverter(
        sd_loader=sd_loader,
        num_inference_steps=num_inference_steps,
        guidance_scale=guidance_scale,
        eta=eta
    )


# æµ‹è¯•å‡½æ•°
def test_ddim_inversion():
    """æµ‹è¯•DDIMåå‘é‡‡æ ·"""
    logger.info("å¼€å§‹æµ‹è¯•DDIMåå‘é‡‡æ ·...")
    
    try:
        # åˆ›å»ºSDåŠ è½½å™¨
        import sys
        from pathlib import Path
        sys.path.append(str(Path(__file__).parent.parent))
        from models.sd_loader import create_sd_loader
        sd_loader = create_sd_loader()
        
        # åˆ›å»ºDDIMåå‘é‡‡æ ·å™¨
        inverter = create_ddim_inverter(sd_loader, num_inference_steps=20)  # ä½¿ç”¨è¾ƒå°‘æ­¥æ•°åŠ å¿«æµ‹è¯•
        
        # åˆ›å»ºæµ‹è¯•å›¾åƒ
        test_image = torch.randn(1, 3, 512, 512, dtype=sd_loader.dtype, device=sd_loader.device)
        test_image = torch.clamp(test_image * 0.5 + 0.5, 0.0, 1.0)
        
        # æµ‹è¯•åè½¬
        noise = inverter.invert_image(test_image, "a beautiful landscape")
        logger.info(f"âœ… åè½¬æµ‹è¯•é€šè¿‡: {test_image.shape} -> {noise.shape}")
        
        # æµ‹è¯•é‡å»º
        reconstructed = inverter.forward_sample(noise, "a beautiful landscape", num_inference_steps=20)
        logger.info(f"âœ… é‡å»ºæµ‹è¯•é€šè¿‡: {noise.shape} -> {reconstructed.shape}")
        
        # æµ‹è¯•è´¨é‡
        original, recon, mse = inverter.test_inversion_quality(test_image, "a beautiful landscape", 20)
        logger.info(f"âœ… è´¨é‡æµ‹è¯•é€šè¿‡: MSE={mse:.6f}")
        
        logger.info("ğŸ‰ DDIMåå‘é‡‡æ ·æµ‹è¯•å…¨éƒ¨é€šè¿‡ï¼")
        return True
        
    except Exception as e:
        logger.error(f"âŒ æµ‹è¯•å¤±è´¥: {e}")
        return False


if __name__ == "__main__":
    # é…ç½®æ—¥å¿—
    logging.basicConfig(
        level=logging.INFO,
        format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
    )
    
    # è¿è¡Œæµ‹è¯•
    test_ddim_inversion()