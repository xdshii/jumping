"""
èº«ä»½æŸå¤±å‡½æ•°å®ç°

è¯¥æ¨¡å—æä¾›èº«ä»½ä¿æŠ¤çš„æ ¸å¿ƒæŸå¤±å‡½æ•°ï¼ŒåŒ…æ‹¬ï¼š
1. ArcFaceé¢éƒ¨ç‰¹å¾æå–
2. é¢éƒ¨ç‰¹å¾ç›¸ä¼¼åº¦è®¡ç®—  
3. èº«ä»½æŸå¤±è®¡ç®—ï¼ˆæœ€å¤§åŒ–ç‰¹å¾è·ç¦»ï¼‰
4. å¤šæ¨¡å‹é›†æˆæ”¯æŒ

åŸºäºDiffPrivateè®ºæ–‡ä¸­çš„èº«ä»½æŸå¤±L_IDè®¾è®¡ã€‚

ä½œè€…: AI Privacy Protection System
æ—¥æœŸ: 2025-07-28
"""

import torch
import torch.nn as nn
import torch.nn.functional as F
import logging
from typing import Optional, List, Dict, Any, Union, Tuple
import numpy as np
from pathlib import Path
import cv2
from PIL import Image

try:
    import insightface
    INSIGHTFACE_AVAILABLE = True
except ImportError:
    INSIGHTFACE_AVAILABLE = False
    logging.warning("InsightFace not available. ArcFaceåŠŸèƒ½å°†è¢«ç¦ç”¨ã€‚")

try:
    from facenet_pytorch import MTCNN, InceptionResnetV1, fixed_image_standardization
    FACENET_AVAILABLE = True
except ImportError:
    FACENET_AVAILABLE = False
    logging.warning("FaceNetä¸å¯ç”¨ã€‚FaceNetåŠŸèƒ½å°†è¢«ç¦ç”¨ã€‚")

logger = logging.getLogger(__name__)

class ArcFaceExtractor:
    """
    ArcFaceç‰¹å¾æå–å™¨
    
    ä½¿ç”¨InsightFaceåº“çš„ArcFaceæ¨¡å‹æå–äººè„¸ç‰¹å¾
    """
    
    def __init__(
        self,
        model_path: str = "checkpoints/face_models/arcface/models/buffalo_l",
        device: str = "cuda"
    ):
        """
        åˆå§‹åŒ–ArcFaceç‰¹å¾æå–å™¨
        
        Args:
            model_path: æ¨¡å‹è·¯å¾„
            device: è®¾å¤‡
        """
        if not INSIGHTFACE_AVAILABLE:
            raise ImportError("InsightFaceæœªå®‰è£…ï¼Œæ— æ³•ä½¿ç”¨ArcFaceåŠŸèƒ½")
            
        self.device = device
        self.model_path = model_path
        
        # åˆå§‹åŒ–InsightFaceåº”ç”¨
        self.app = insightface.app.FaceAnalysis(
            root=str(Path(model_path).parent),
            providers=['CUDAExecutionProvider', 'CPUExecutionProvider'] if device == 'cuda' else ['CPUExecutionProvider']
        )
        self.app.prepare(ctx_id=0 if device == 'cuda' else -1, det_size=(640, 640))
        
        logger.info(f"ArcFaceç‰¹å¾æå–å™¨åˆå§‹åŒ–å®Œæˆ: {model_path}")
    
    def extract_features(
        self,
        images: Union[torch.Tensor, np.ndarray, List[np.ndarray]],
        return_tensor: bool = True
    ) -> Union[torch.Tensor, np.ndarray]:
        """
        æå–é¢éƒ¨ç‰¹å¾
        
        Args:
            images: å›¾åƒæ•°æ®ï¼Œæ”¯æŒå¤šç§æ ¼å¼
            return_tensor: æ˜¯å¦è¿”å›torch.Tensor
            
        Returns:
            é¢éƒ¨ç‰¹å¾å‘é‡ [batch_size, feature_dim] æˆ– Noneï¼ˆå¦‚æœæœªæ£€æµ‹åˆ°äººè„¸ï¼‰
        """
        # è½¬æ¢è¾“å…¥æ ¼å¼
        if isinstance(images, torch.Tensor):
            # ä»tensorè½¬æ¢ä¸ºnumpy array (RGB, 0-255)
            if images.dim() == 4:  # batch
                images_np = []
                for i in range(images.shape[0]):
                    img = images[i].cpu().detach()
                    if img.max() <= 1.0:  # å½’ä¸€åŒ–çš„å›¾åƒ
                        img = (img * 255).clamp(0, 255)
                    img = img.permute(1, 2, 0).numpy().astype(np.uint8)
                    # è½¬æ¢ä¸ºBGRï¼ˆInsightFaceæœŸæœ›BGRæ ¼å¼ï¼‰
                    img = cv2.cvtColor(img, cv2.COLOR_RGB2BGR)
                    images_np.append(img)
            else:  # single image
                img = images.cpu().detach()
                if img.max() <= 1.0:
                    img = (img * 255).clamp(0, 255)
                img = img.permute(1, 2, 0).numpy().astype(np.uint8)
                img = cv2.cvtColor(img, cv2.COLOR_RGB2BGR)
                images_np = [img]
        elif isinstance(images, np.ndarray):
            if images.ndim == 4:  # batch
                images_np = [images[i] for i in range(images.shape[0])]
            else:
                images_np = [images]
        else:
            images_np = images if isinstance(images, list) else [images]
        
        # æå–ç‰¹å¾
        features = []
        for img in images_np:
            try:
                faces = self.app.get(img)
                if len(faces) > 0:
                    # ä½¿ç”¨æœ€å¤§çš„äººè„¸
                    face = max(faces, key=lambda x: (x.bbox[2] - x.bbox[0]) * (x.bbox[3] - x.bbox[1]))
                    feature = face.normed_embedding
                    features.append(feature)
                else:
                    # å¦‚æœæ²¡æœ‰æ£€æµ‹åˆ°äººè„¸ï¼Œè¿”å›é›¶å‘é‡
                    logger.warning("æœªæ£€æµ‹åˆ°äººè„¸ï¼Œè¿”å›é›¶ç‰¹å¾å‘é‡")
                    features.append(np.zeros(512, dtype=np.float32))
            except Exception as e:
                logger.error(f"ç‰¹å¾æå–å¤±è´¥: {e}")
                features.append(np.zeros(512, dtype=np.float32))
        
        if not features:
            return None
            
        features = np.stack(features, axis=0)
        
        if return_tensor:
            return torch.from_numpy(features).to(self.device)
        else:
            return features

class FaceNetExtractor:
    """
    FaceNetç‰¹å¾æå–å™¨
    
    ä½¿ç”¨FaceNetæ¨¡å‹æå–äººè„¸ç‰¹å¾
    """
    
    def __init__(
        self,
        device: str = "cuda"
    ):
        """
        åˆå§‹åŒ–FaceNetç‰¹å¾æå–å™¨
        
        Args:
            device: è®¾å¤‡
        """
        if not FACENET_AVAILABLE:
            raise ImportError("FaceNetåº“æœªå®‰è£…ï¼Œæ— æ³•ä½¿ç”¨FaceNetåŠŸèƒ½")
            
        self.device = device
        
        # åˆå§‹åŒ–MTCNNç”¨äºäººè„¸æ£€æµ‹
        self.mtcnn = MTCNN(
            image_size=160,
            margin=0,
            device=device,
            keep_all=False,
            post_process=False
        )
        
        # åˆå§‹åŒ–FaceNetæ¨¡å‹
        self.resnet = InceptionResnetV1(pretrained='vggface2').eval().to(device)
        
        logger.info(f"FaceNetç‰¹å¾æå–å™¨åˆå§‹åŒ–å®Œæˆ")
    
    def extract_features(
        self,
        images: Union[torch.Tensor, List[Image.Image]],
        return_tensor: bool = True
    ) -> Union[torch.Tensor, np.ndarray]:
        """
        æå–é¢éƒ¨ç‰¹å¾
        
        Args:
            images: å›¾åƒæ•°æ®
            return_tensor: æ˜¯å¦è¿”å›torch.Tensor
            
        Returns:
            é¢éƒ¨ç‰¹å¾å‘é‡ [batch_size, feature_dim]
        """
        if isinstance(images, torch.Tensor):
            # è½¬æ¢tensorä¸ºPILå›¾åƒåˆ—è¡¨
            pil_images = []
            for i in range(images.shape[0]):
                img = images[i].cpu().detach()
                if img.max() <= 1.0:
                    img = (img * 255).clamp(0, 255)
                img = img.permute(1, 2, 0).numpy().astype(np.uint8)
                pil_images.append(Image.fromarray(img))
        else:
            pil_images = images
        
        # æ£€æµ‹å’Œå¯¹é½äººè„¸
        aligned_faces = []
        for img in pil_images:
            try:
                face = self.mtcnn(img)
                if face is not None:
                    aligned_faces.append(face)
                else:
                    # å¦‚æœæ£€æµ‹å¤±è´¥ï¼Œä½¿ç”¨é›¶å¼ é‡
                    aligned_faces.append(torch.zeros(3, 160, 160, device=self.device))
            except Exception as e:
                logger.error(f"äººè„¸æ£€æµ‹å¤±è´¥: {e}")
                aligned_faces.append(torch.zeros(3, 160, 160, device=self.device))
        
        if not aligned_faces:
            return None
            
        # æ‰¹å¤„ç†æå–ç‰¹å¾
        aligned_batch = torch.stack(aligned_faces).to(self.device)
        
        with torch.no_grad():
            features = self.resnet(aligned_batch)
            features = F.normalize(features, p=2, dim=1)  # L2å½’ä¸€åŒ–
        
        if return_tensor:
            return features
        else:
            return features.cpu().numpy()

class IdentityLoss(nn.Module):
    """
    èº«ä»½æŸå¤±å‡½æ•°
    
    å®ç°DiffPrivateè®ºæ–‡ä¸­çš„èº«ä»½æŸå¤±L_IDï¼Œé€šè¿‡æœ€å¤§åŒ–é¢éƒ¨ç‰¹å¾è·ç¦»æ¥å®ç°èº«ä»½ä¿æŠ¤
    """
    
    def __init__(
        self,
        model_types: List[str] = ["arcface"],
        model_weights: Optional[List[float]] = None,
        distance_metric: str = "cosine",
        device: str = "cuda",
        fallback_to_l2: bool = True
    ):
        """
        åˆå§‹åŒ–èº«ä»½æŸå¤±å‡½æ•°
        
        Args:
            model_types: ä½¿ç”¨çš„æ¨¡å‹ç±»å‹ ["arcface", "facenet"]
            model_weights: å„æ¨¡å‹æƒé‡ï¼ˆå¦‚æœä¸ºNoneåˆ™å‡ç­‰æƒé‡ï¼‰
            distance_metric: è·ç¦»åº¦é‡ ("cosine", "l2", "l1")
            device: è®¾å¤‡
            fallback_to_l2: å½“é¢éƒ¨æ£€æµ‹å¤±è´¥æ—¶æ˜¯å¦å›é€€åˆ°åƒç´ L2æŸå¤±
        """
        super().__init__()
        
        self.model_types = model_types
        self.distance_metric = distance_metric
        self.device = device
        self.fallback_to_l2 = fallback_to_l2
        
        # è®¾ç½®æ¨¡å‹æƒé‡
        if model_weights is None:
            self.model_weights = [1.0 / len(model_types)] * len(model_types)
        else:
            assert len(model_weights) == len(model_types), "æƒé‡æ•°é‡å¿…é¡»ä¸æ¨¡å‹æ•°é‡åŒ¹é…"
            self.model_weights = model_weights
        
        # åˆå§‹åŒ–ç‰¹å¾æå–å™¨
        self.extractors = {}
        for model_type in model_types:
            if model_type == "arcface" and INSIGHTFACE_AVAILABLE:
                self.extractors["arcface"] = ArcFaceExtractor(device=device)
            elif model_type == "facenet" and FACENET_AVAILABLE:
                self.extractors["facenet"] = FaceNetExtractor(device=device)
            else:
                logger.error(f"ä¸æ”¯æŒçš„æ¨¡å‹ç±»å‹æˆ–ä¾èµ–ç¼ºå¤±: {model_type}")
        
        logger.info(f"èº«ä»½æŸå¤±å‡½æ•°åˆå§‹åŒ–: models={model_types}, metric={distance_metric}")
    
    def compute_distance(
        self,
        features1: torch.Tensor,
        features2: torch.Tensor,
        metric: str = None
    ) -> torch.Tensor:
        """
        è®¡ç®—ç‰¹å¾è·ç¦»
        
        Args:
            features1: ç¬¬ä¸€ç»„ç‰¹å¾ [batch_size, feature_dim]
            features2: ç¬¬äºŒç»„ç‰¹å¾ [batch_size, feature_dim]  
            metric: è·ç¦»åº¦é‡
            
        Returns:
            è·ç¦»å€¼ [batch_size]
        """
        if metric is None:
            metric = self.distance_metric
            
        if metric == "cosine":
            # ä½™å¼¦è·ç¦» (1 - ä½™å¼¦ç›¸ä¼¼åº¦)
            features1_norm = F.normalize(features1, p=2, dim=1)
            features2_norm = F.normalize(features2, p=2, dim=1)
            cosine_sim = torch.sum(features1_norm * features2_norm, dim=1)
            return 1.0 - cosine_sim
        elif metric == "l2":
            # æ¬§æ°è·ç¦»
            return torch.norm(features1 - features2, p=2, dim=1)
        elif metric == "l1":
            # æ›¼å“ˆé¡¿è·ç¦»
            return torch.norm(features1 - features2, p=1, dim=1)
        else:
            raise ValueError(f"ä¸æ”¯æŒçš„è·ç¦»åº¦é‡: {metric}")
    
    def compute_pixel_l2_loss(
        self,
        original_images: torch.Tensor,
        protected_images: torch.Tensor,
        reduction: str = "mean"
    ) -> torch.Tensor:
        """
        è®¡ç®—åƒç´ çº§L2æŸå¤±ï¼ˆä½œä¸ºå¤‡ç”¨æ–¹æ¡ˆï¼‰
        
        Args:
            original_images: åŸå§‹å›¾åƒ
            protected_images: ä¿æŠ¤å›¾åƒ
            reduction: é™ç»´æ–¹å¼
            
        Returns:
            L2æŸå¤±
        """
        l2_loss = F.mse_loss(original_images, protected_images, reduction='none')
        l2_loss = l2_loss.view(l2_loss.shape[0], -1).mean(dim=1)  # [batch_size]
        
        if reduction == "mean":
            return l2_loss.mean()
        elif reduction == "sum":
            return l2_loss.sum()
        else:
            return l2_loss
    
    def forward(
        self,
        original_images: torch.Tensor,
        protected_images: torch.Tensor,
        target_distance: float = 1.0,
        reduction: str = "mean"
    ) -> Dict[str, torch.Tensor]:
        """
        è®¡ç®—èº«ä»½æŸå¤±
        
        Args:
            original_images: åŸå§‹å›¾åƒ [batch_size, 3, H, W]
            protected_images: ä¿æŠ¤åå›¾åƒ [batch_size, 3, H, W]
            target_distance: ç›®æ ‡è·ç¦»ï¼ˆç”¨äºå½’ä¸€åŒ–ï¼‰
            reduction: é™ç»´æ–¹å¼ ("mean", "sum", "none")
            
        Returns:
            æŸå¤±å­—å…¸ï¼ŒåŒ…å«å„ä¸ªæ¨¡å‹çš„æŸå¤±å’Œæ€»æŸå¤±
        """
        losses = {}
        total_loss = torch.tensor(0.0, device=original_images.device, requires_grad=True)
        valid_models = 0
        
        for i, model_type in enumerate(self.model_types):
            if model_type not in self.extractors:
                continue
                
            extractor = self.extractors[model_type]
            
            try:
                # æå–åŸå§‹å›¾åƒç‰¹å¾
                orig_features = extractor.extract_features(original_images)
                if orig_features is None:
                    logger.warning(f"{model_type}: æ— æ³•æå–åŸå§‹å›¾åƒç‰¹å¾")
                    continue
                
                # æå–ä¿æŠ¤å›¾åƒç‰¹å¾
                prot_features = extractor.extract_features(protected_images)
                if prot_features is None:
                    logger.warning(f"{model_type}: æ— æ³•æå–ä¿æŠ¤å›¾åƒç‰¹å¾")
                    continue
                
                # æ£€æŸ¥ç‰¹å¾æ˜¯å¦éƒ½æ˜¯é›¶ï¼ˆè¡¨ç¤ºæ²¡æœ‰æ£€æµ‹åˆ°äººè„¸ï¼‰
                orig_zero_mask = (orig_features.sum(dim=1) == 0)
                prot_zero_mask = (prot_features.sum(dim=1) == 0)
                all_zero_mask = orig_zero_mask & prot_zero_mask
                
                if all_zero_mask.all() and self.fallback_to_l2:
                    # å¦‚æœæ‰€æœ‰æ ·æœ¬éƒ½æ²¡æœ‰æ£€æµ‹åˆ°äººè„¸ï¼Œå›é€€åˆ°åƒç´ L2æŸå¤±
                    logger.info(f"{model_type}: å›é€€åˆ°åƒç´ L2æŸå¤±")
                    pixel_loss = self.compute_pixel_l2_loss(original_images, protected_images, reduction)
                    identity_loss = -pixel_loss  # è´Ÿå€¼è¡¨ç¤ºæœ€å¤§åŒ–è·ç¦»
                    distances = pixel_loss
                else:
                    # è®¡ç®—ç‰¹å¾è·ç¦»
                    distances = self.compute_distance(orig_features, prot_features)
                    
                    # èº«ä»½æŸå¤±ï¼šè´Ÿè·ç¦»ï¼ˆæœ€å¤§åŒ–è·ç¦»ï¼‰
                    identity_loss = -distances / target_distance
                    
                    if reduction == "mean":
                        identity_loss = identity_loss.mean()
                        distances = distances.mean()
                    elif reduction == "sum":
                        identity_loss = identity_loss.sum()
                        distances = distances.sum()
                
                losses[f"{model_type}_loss"] = identity_loss
                losses[f"{model_type}_distance"] = distances
                
                # åŠ æƒç´¯åŠ åˆ°æ€»æŸå¤±ï¼ˆé¿å…in-placeæ“ä½œï¼‰
                total_loss = total_loss + self.model_weights[i] * identity_loss
                valid_models += 1
                
            except Exception as e:
                logger.error(f"{model_type}ç‰¹å¾æå–å¤±è´¥: {e}")
                continue
        
        if valid_models == 0 and self.fallback_to_l2:
            # å¦‚æœæ‰€æœ‰æ¨¡å‹éƒ½å¤±è´¥ï¼Œä½¿ç”¨åƒç´ L2æŸå¤±ä½œä¸ºæœ€åæ‰‹æ®µ
            logger.info("æ‰€æœ‰é¢éƒ¨æ¨¡å‹å¤±è´¥ï¼Œä½¿ç”¨åƒç´ L2æŸå¤±")
            pixel_loss = self.compute_pixel_l2_loss(original_images, protected_images, reduction)
            total_loss = -pixel_loss
            losses["pixel_l2_loss"] = -pixel_loss
            losses["pixel_l2_distance"] = pixel_loss
        
        losses["total_loss"] = total_loss
        # è®¡ç®—å¹³å‡è·ç¦»ï¼Œç¡®ä¿ç±»å‹ä¸€è‡´æ€§
        distance_values = [losses[f"{m}_distance"] for m in self.model_types if f"{m}_distance" in losses]
        if distance_values:
            if isinstance(distance_values[0], torch.Tensor):
                avg_distance = torch.stack(distance_values).mean()
            else:
                avg_distance = sum(distance_values) / len(distance_values)
        else:
            avg_distance = torch.tensor(0.0, device=original_images.device)
        losses["avg_distance"] = avg_distance
        
        return losses

def create_identity_loss(
    model_types: List[str] = ["arcface"],
    model_weights: Optional[List[float]] = None,
    distance_metric: str = "cosine",
    device: str = "cuda",
    fallback_to_l2: bool = True
) -> IdentityLoss:
    """
    åˆ›å»ºèº«ä»½æŸå¤±å‡½æ•°çš„ä¾¿æ·å‡½æ•°
    
    Args:
        model_types: ä½¿ç”¨çš„æ¨¡å‹ç±»å‹
        model_weights: å„æ¨¡å‹æƒé‡
        distance_metric: è·ç¦»åº¦é‡
        device: è®¾å¤‡
        fallback_to_l2: æ˜¯å¦å¯ç”¨L2å›é€€
        
    Returns:
        èº«ä»½æŸå¤±å‡½æ•°å®ä¾‹
    """
    return IdentityLoss(model_types, model_weights, distance_metric, device, fallback_to_l2)

def test_identity_loss():
    """æµ‹è¯•èº«ä»½æŸå¤±å‡½æ•°"""
    print("ğŸ§ª æµ‹è¯•èº«ä»½æŸå¤±å‡½æ•°...")
    
    try:
        # æ£€æŸ¥ä¾èµ–
        available_models = []
        if INSIGHTFACE_AVAILABLE:
            available_models.append("arcface")
        if FACENET_AVAILABLE:
            available_models.append("facenet")
        
        if not available_models:
            print("âŒ æ²¡æœ‰å¯ç”¨çš„é¢éƒ¨è¯†åˆ«æ¨¡å‹ï¼Œè·³è¿‡æµ‹è¯•")
            return False
        
        print(f"âœ… å¯ç”¨æ¨¡å‹: {available_models}")
        
        # åˆ›å»ºèº«ä»½æŸå¤±å‡½æ•°ï¼ˆå¯ç”¨L2å›é€€ï¼‰
        id_loss = create_identity_loss(
            model_types=available_models[:1],  # åªä½¿ç”¨ç¬¬ä¸€ä¸ªå¯ç”¨æ¨¡å‹è¿›è¡Œæµ‹è¯•
            device="cuda" if torch.cuda.is_available() else "cpu",
            fallback_to_l2=True
        )
        
        # åˆ›å»ºæµ‹è¯•å›¾åƒ
        device = "cuda" if torch.cuda.is_available() else "cpu"
        batch_size = 2
        
        # åˆ›å»ºéšæœºå›¾åƒï¼ˆæ¨¡æ‹Ÿäººè„¸ï¼‰- ä½¿ç”¨requires_gradç¡®ä¿æ¢¯åº¦è¿æ¥
        original_images = torch.rand(batch_size, 3, 224, 224, device=device, requires_grad=True)
        protected_images = torch.rand(batch_size, 3, 224, 224, device=device, requires_grad=True)
        
        print(f"âœ… èº«ä»½æŸå¤±å‡½æ•°åˆ›å»ºæˆåŠŸ")
        print(f"   è®¾å¤‡: {device}")
        print(f"   æ¨¡å‹: {available_models[:1]}")
        print(f"   è·ç¦»åº¦é‡: {id_loss.distance_metric}")
        print(f"   L2å›é€€: {id_loss.fallback_to_l2}")
        
        # æµ‹è¯•å‰å‘ä¼ æ’­
        print("ğŸ”® æµ‹è¯•å‰å‘ä¼ æ’­...")
        loss_dict = id_loss(original_images, protected_images)
        
        print("âœ… å‰å‘ä¼ æ’­æˆåŠŸ:")
        for key, value in loss_dict.items():
            if isinstance(value, torch.Tensor):
                print(f"   {key}: {value.item():.6f}")
            else:
                print(f"   {key}: {value:.6f}")
        
        # æµ‹è¯•æ¢¯åº¦è®¡ç®—
        print("ğŸ“ˆ æµ‹è¯•æ¢¯åº¦è®¡ç®—...")
        total_loss = loss_dict["total_loss"]
        
        # æ¸…é™¤ä¹‹å‰çš„æ¢¯åº¦
        if original_images.grad is not None:
            original_images.grad.zero_()
        if protected_images.grad is not None:
            protected_images.grad.zero_()
        
        total_loss.backward()
        
        # æ£€æŸ¥æ¢¯åº¦
        orig_grad_norm = original_images.grad.norm().item() if original_images.grad is not None else 0
        prot_grad_norm = protected_images.grad.norm().item() if protected_images.grad is not None else 0
        
        print(f"âœ… æ¢¯åº¦è®¡ç®—æˆåŠŸ:")
        print(f"   åŸå§‹å›¾åƒæ¢¯åº¦èŒƒæ•°: {orig_grad_norm:.6f}")
        print(f"   ä¿æŠ¤å›¾åƒæ¢¯åº¦èŒƒæ•°: {prot_grad_norm:.6f}")
        
        if orig_grad_norm > 0 or prot_grad_norm > 0:
            print("ğŸ‰ èº«ä»½æŸå¤±å‡½æ•°æµ‹è¯•å…¨éƒ¨é€šè¿‡ï¼")
            return True
        else:
            print("âš ï¸ æ¢¯åº¦ä¸ºé›¶ï¼Œå¯èƒ½å­˜åœ¨é—®é¢˜")
            return False
        
    except Exception as e:
        print(f"âŒ æµ‹è¯•å¤±è´¥: {e}")
        import traceback
        traceback.print_exc()
        return False

if __name__ == "__main__":
    test_identity_loss() 