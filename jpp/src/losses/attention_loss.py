"""
è‡ªæ³¨æ„åŠ›æŸå¤±å‡½æ•°å®ç°

è¯¥æ¨¡å—æä¾›åŸºäºè‡ªæ³¨æ„åŠ›æœºåˆ¶çš„ç»“æ„ä¿æŒæŸå¤±ï¼Œç¡®ä¿ä¿æŠ¤åçš„å›¾åƒ
åœ¨æ‰©æ•£è¿‡ç¨‹ä¸­ä¿æŒä¸åŸå›¾ç›¸ä¼¼çš„ç©ºé—´ç»“æ„å…³ç³»ã€‚

åŠŸèƒ½åŒ…æ‹¬ï¼š
1. ä»æ³¨æ„åŠ›æ§åˆ¶å™¨æå–è‡ªæ³¨æ„åŠ›å›¾
2. è®¡ç®—åŸå§‹å›¾åƒå’Œä¿æŠ¤å›¾åƒçš„è‡ªæ³¨æ„åŠ›å·®å¼‚
3. å¤šåˆ†è¾¨ç‡è‡ªæ³¨æ„åŠ›æŸå¤±
4. ç»“æ„ä¸€è‡´æ€§è¯„ä¼°

åŸºäºDiffPrivateè®ºæ–‡ä¸­çš„è‡ªæ³¨æ„åŠ›æŸå¤±L_selfè®¾è®¡ã€‚

ä½œè€…: AI Privacy Protection System
æ—¥æœŸ: 2025-07-28
"""

import torch
import torch.nn as nn
import torch.nn.functional as F
import logging
from typing import Optional, Dict, Any, Union, List, Tuple, Callable
import numpy as np
from collections import defaultdict

try:
    from ..models.attention_control import AttentionControlEdit, register_attention_control
    from ..models.sd_loader import StableDiffusionLoader, ModelComponents
    from ..optimization.diffusion_step import DiffusionStepper
except ImportError:
    import sys
    from pathlib import Path
    sys.path.append(str(Path(__file__).parent.parent))
    from models.attention_control import AttentionControlEdit, register_attention_control
    from models.sd_loader import StableDiffusionLoader, ModelComponents
    from optimization.diffusion_step import DiffusionStepper

logger = logging.getLogger(__name__)

class AttentionLoss(nn.Module):
    """
    è‡ªæ³¨æ„åŠ›æŸå¤±å‡½æ•°
    
    é€šè¿‡æ¯”è¾ƒåŸå§‹å›¾åƒå’Œä¿æŠ¤å›¾åƒåœ¨æ‰©æ•£è¿‡ç¨‹ä¸­çš„è‡ªæ³¨æ„åŠ›å›¾æ¥è®¡ç®—ç»“æ„æŸå¤±
    """
    
    def __init__(
        self,
        sd_loader: StableDiffusionLoader,
        target_resolutions: List[int] = [64, 32, 16],
        resolution_weights: Optional[List[float]] = None,
        loss_type: str = "mse",
        normalize_attention: bool = True,
        temporal_weighting: bool = True,
        attention_threshold: float = 0.01
    ):
        """
        åˆå§‹åŒ–è‡ªæ³¨æ„åŠ›æŸå¤±å‡½æ•°
        
        Args:
            sd_loader: Stable DiffusionåŠ è½½å™¨
            target_resolutions: ç›®æ ‡åˆ†è¾¨ç‡åˆ—è¡¨ï¼ˆç”¨äºå¤šå°ºåº¦æŸå¤±ï¼‰
            resolution_weights: å„åˆ†è¾¨ç‡æƒé‡
            loss_type: æŸå¤±ç±»å‹ ('mse', 'l1', 'cosine')
            normalize_attention: æ˜¯å¦å½’ä¸€åŒ–æ³¨æ„åŠ›å›¾
            temporal_weighting: æ˜¯å¦ä½¿ç”¨æ—¶é—´æ­¥æƒé‡
            attention_threshold: æ³¨æ„åŠ›é˜ˆå€¼ï¼ˆè¿‡æ»¤ä½æ³¨æ„åŠ›åŒºåŸŸï¼‰
        """
        super().__init__()
        
        self.sd_loader = sd_loader
        self.target_resolutions = target_resolutions
        self.loss_type = loss_type
        self.normalize_attention = normalize_attention
        self.temporal_weighting = temporal_weighting
        self.attention_threshold = attention_threshold
        
        # è®¾ç½®åˆ†è¾¨ç‡æƒé‡
        if resolution_weights is None:
            self.resolution_weights = [1.0] * len(target_resolutions)
        else:
            assert len(resolution_weights) == len(target_resolutions), "æƒé‡æ•°é‡å¿…é¡»ä¸åˆ†è¾¨ç‡æ•°é‡åŒ¹é…"
            self.resolution_weights = resolution_weights
        
        # å½’ä¸€åŒ–æƒé‡
        total_weight = sum(self.resolution_weights)
        self.resolution_weights = [w / total_weight for w in self.resolution_weights]
        
        # ç¡®ä¿ç»„ä»¶å·²åŠ è½½
        if not hasattr(sd_loader, 'components') or sd_loader.components is None:
            self.components = sd_loader.load_components()
        else:
            self.components = sd_loader.components
        
        # åˆ›å»ºæ‰©æ•£æ­¥éª¤å¤„ç†å™¨
        self.diffusion_stepper = DiffusionStepper(sd_loader)
        
        logger.info(f"è‡ªæ³¨æ„åŠ›æŸå¤±åˆå§‹åŒ–: åˆ†è¾¨ç‡={target_resolutions}, æƒé‡={self.resolution_weights}")
    
    def extract_attention_maps(
        self,
        latents: torch.Tensor,
        text_embeddings: torch.Tensor,
        timesteps: torch.Tensor,
        controller: AttentionControlEdit
    ) -> Dict[int, List[torch.Tensor]]:
        """
        æå–è‡ªæ³¨æ„åŠ›å›¾
        
        Args:
            latents: æ½œç©ºé—´è¡¨ç¤º [batch_size, 4, H, W]
            text_embeddings: æ–‡æœ¬åµŒå…¥
            timesteps: æ—¶é—´æ­¥
            controller: æ³¨æ„åŠ›æ§åˆ¶å™¨
            
        Returns:
            æŒ‰åˆ†è¾¨ç‡ç»„ç»‡çš„è‡ªæ³¨æ„åŠ›å›¾å­—å…¸
        """
        # é‡ç½®æ§åˆ¶å™¨
        controller.reset()
        
        attention_maps = defaultdict(list)
        
        # é€æ—¶é—´æ­¥æå–æ³¨æ„åŠ›
        for i, timestep in enumerate(timesteps):
            # é¢„æµ‹å™ªå£°ï¼ˆè¿™ä¼šè§¦å‘æ³¨æ„åŠ›æ§åˆ¶å™¨ï¼‰
            with torch.no_grad():
                noise_pred = self.diffusion_stepper.predict_noise(
                    latents,
                    timestep,
                    text_embeddings
                )
            
            # æå–å½“å‰æ—¶é—´æ­¥çš„è‡ªæ³¨æ„åŠ›
            for resolution in self.target_resolutions:
                self_attn_loss = controller.get_self_attention_loss(target_resolution=resolution)
                if self_attn_loss > 0:  # åªä¿å­˜æœ‰æ•ˆçš„æ³¨æ„åŠ›å›¾
                    attention_maps[resolution].append(self_attn_loss)
        
        return attention_maps
    
    def compute_attention_difference(
        self,
        attn1: torch.Tensor,
        attn2: torch.Tensor,
        loss_type: str = None
    ) -> torch.Tensor:
        """
        è®¡ç®—ä¸¤ä¸ªæ³¨æ„åŠ›å›¾ä¹‹é—´çš„å·®å¼‚
        
        Args:
            attn1: ç¬¬ä¸€ä¸ªæ³¨æ„åŠ›å›¾
            attn2: ç¬¬äºŒä¸ªæ³¨æ„åŠ›å›¾
            loss_type: æŸå¤±ç±»å‹
            
        Returns:
            æ³¨æ„åŠ›å·®å¼‚æŸå¤±
        """
        if loss_type is None:
            loss_type = self.loss_type
        
        # ç¡®ä¿ä¸¤ä¸ªæ³¨æ„åŠ›å›¾å½¢çŠ¶ç›¸åŒ
        if attn1.shape != attn2.shape:
            logger.warning(f"æ³¨æ„åŠ›å›¾å½¢çŠ¶ä¸åŒ¹é…: {attn1.shape} vs {attn2.shape}")
            # è°ƒæ•´åˆ°ç›¸åŒå°ºå¯¸
            min_size = min(attn1.shape[-1], attn2.shape[-1])
            attn1 = F.interpolate(attn1.unsqueeze(0), size=(min_size, min_size), mode='bilinear', align_corners=False).squeeze(0)
            attn2 = F.interpolate(attn2.unsqueeze(0), size=(min_size, min_size), mode='bilinear', align_corners=False).squeeze(0)
        
        # å½’ä¸€åŒ–æ³¨æ„åŠ›å›¾ï¼ˆå¦‚æœéœ€è¦ï¼‰
        if self.normalize_attention:
            attn1 = F.normalize(attn1.flatten(), p=2, dim=0).view_as(attn1)
            attn2 = F.normalize(attn2.flatten(), p=2, dim=0).view_as(attn2)
        
        # åº”ç”¨æ³¨æ„åŠ›é˜ˆå€¼
        if self.attention_threshold > 0:
            mask1 = (attn1 > self.attention_threshold).float()
            mask2 = (attn2 > self.attention_threshold).float()
            mask = mask1 * mask2  # åªè€ƒè™‘ä¸¤è€…éƒ½é«˜äºé˜ˆå€¼çš„åŒºåŸŸ
            
            attn1 = attn1 * mask
            attn2 = attn2 * mask
        
        # è®¡ç®—æŸå¤±
        if loss_type == "mse":
            return F.mse_loss(attn1, attn2, reduction='mean')
        elif loss_type == "l1":
            return F.l1_loss(attn1, attn2, reduction='mean')
        elif loss_type == "cosine":
            attn1_flat = attn1.flatten()
            attn2_flat = attn2.flatten()
            cosine_sim = F.cosine_similarity(attn1_flat.unsqueeze(0), attn2_flat.unsqueeze(0))
            return 1.0 - cosine_sim.mean()  # ä½™å¼¦è·ç¦»
        else:
            raise ValueError(f"ä¸æ”¯æŒçš„æŸå¤±ç±»å‹: {loss_type}")
    
    def compute_temporal_weights(
        self,
        timesteps: torch.Tensor
    ) -> torch.Tensor:
        """
        è®¡ç®—æ—¶é—´æ­¥æƒé‡
        
        Args:
            timesteps: æ—¶é—´æ­¥å¼ é‡
            
        Returns:
            æ—¶é—´æ­¥æƒé‡
        """
        if not self.temporal_weighting:
            return torch.ones_like(timesteps.float())
        
        # æ—©æœŸæ—¶é—´æ­¥ï¼ˆå™ªå£°è¾ƒå¤šï¼‰æƒé‡è¾ƒå°ï¼ŒåæœŸæ—¶é—´æ­¥æƒé‡è¾ƒå¤§
        max_timestep = 1000.0  # å‡è®¾æœ€å¤§æ—¶é—´æ­¥ä¸º1000
        normalized_t = timesteps.float() / max_timestep
        
        # ä½¿ç”¨æŒ‡æ•°è¡°å‡ï¼šæ—©æœŸæƒé‡å°ï¼ŒåæœŸæƒé‡å¤§
        weights = torch.exp(-2.0 * normalized_t)  # æƒé‡éšæ—¶é—´æ­¥å‡å°‘
        
        # å½’ä¸€åŒ–æƒé‡
        weights = weights / weights.sum()
        
        return weights
    
    def forward(
        self,
        original_latents: torch.Tensor,
        protected_latents: torch.Tensor,
        text_embeddings: torch.Tensor,
        timesteps: torch.Tensor,
        return_components: bool = False
    ) -> Union[torch.Tensor, Dict[str, torch.Tensor]]:
        """
        è®¡ç®—è‡ªæ³¨æ„åŠ›æŸå¤±
        
        Args:
            original_latents: åŸå§‹å›¾åƒçš„æ½œç©ºé—´è¡¨ç¤º
            protected_latents: ä¿æŠ¤å›¾åƒçš„æ½œç©ºé—´è¡¨ç¤º
            text_embeddings: æ–‡æœ¬åµŒå…¥
            timesteps: æ—¶é—´æ­¥åºåˆ—
            return_components: æ˜¯å¦è¿”å›æŸå¤±ç»„ä»¶
            
        Returns:
            è‡ªæ³¨æ„åŠ›æŸå¤±æˆ–æŸå¤±å­—å…¸
        """
        # åˆ›å»ºæ³¨æ„åŠ›æ§åˆ¶å™¨
        controller_orig = AttentionControlEdit(
            tokenizer=self.components.tokenizer,
            device=self.components.device,
            save_self_attention=True
        )
        controller_prot = AttentionControlEdit(
            tokenizer=self.components.tokenizer,
            device=self.components.device,
            save_self_attention=True
        )
        
        # æ³¨å†Œæ³¨æ„åŠ›æ§åˆ¶
        hooks_orig = register_attention_control(self.components.unet, controller_orig)
        
        try:
            # æå–åŸå§‹å›¾åƒçš„æ³¨æ„åŠ›å›¾
            orig_attention_maps = self.extract_attention_maps(
                original_latents, text_embeddings, timesteps, controller_orig
            )
            
            # åˆ‡æ¢åˆ°ä¿æŠ¤å›¾åƒçš„æ§åˆ¶å™¨
            for hook in hooks_orig:
                hook.remove()
            hooks_prot = register_attention_control(self.components.unet, controller_prot)
            
            # æå–ä¿æŠ¤å›¾åƒçš„æ³¨æ„åŠ›å›¾
            prot_attention_maps = self.extract_attention_maps(
                protected_latents, text_embeddings, timesteps, controller_prot
            )
            
        finally:
            # æ¸…ç†é’©å­
            for hook in hooks_prot:
                hook.remove()
        
        # è®¡ç®—å„åˆ†è¾¨ç‡çš„æŸå¤±
        total_loss = 0.0
        components = {}
        temporal_weights = self.compute_temporal_weights(timesteps)
        
        for i, resolution in enumerate(self.target_resolutions):
            resolution_loss = 0.0
            resolution_count = 0
            
            if resolution in orig_attention_maps and resolution in prot_attention_maps:
                orig_maps = orig_attention_maps[resolution]
                prot_maps = prot_attention_maps[resolution]
                
                # ç¡®ä¿ä¸¤è€…æœ‰ç›¸åŒæ•°é‡çš„æ³¨æ„åŠ›å›¾
                min_count = min(len(orig_maps), len(prot_maps))
                
                for j in range(min_count):
                    if isinstance(orig_maps[j], torch.Tensor) and isinstance(prot_maps[j], torch.Tensor):
                        # è®¡ç®—å•ä¸ªæ³¨æ„åŠ›å›¾çš„æŸå¤±
                        single_loss = self.compute_attention_difference(orig_maps[j], prot_maps[j])
                        
                        # åº”ç”¨æ—¶é—´æ­¥æƒé‡
                        if j < len(temporal_weights):
                            single_loss = single_loss * temporal_weights[j]
                        
                        resolution_loss += single_loss
                        resolution_count += 1
                
                # å¹³å‡è¯¥åˆ†è¾¨ç‡çš„æŸå¤±
                if resolution_count > 0:
                    resolution_loss = resolution_loss / resolution_count
                    
                    # åº”ç”¨åˆ†è¾¨ç‡æƒé‡
                    weighted_loss = self.resolution_weights[i] * resolution_loss
                    total_loss += weighted_loss
                    
                    if return_components:
                        components[f"resolution_{resolution}_loss"] = resolution_loss
                        components[f"resolution_{resolution}_weight"] = self.resolution_weights[i]
                        components[f"resolution_{resolution}_weighted"] = weighted_loss
                        components[f"resolution_{resolution}_count"] = resolution_count
            else:
                logger.warning(f"åˆ†è¾¨ç‡ {resolution} çš„æ³¨æ„åŠ›å›¾ç¼ºå¤±")
        
        if return_components:
            components["total_loss"] = total_loss
            components["num_resolutions"] = len([r for r in self.target_resolutions if r in orig_attention_maps and r in prot_attention_maps])
            return components
        else:
            return total_loss

def create_attention_loss(
    sd_loader: StableDiffusionLoader,
    target_resolutions: List[int] = [64, 32, 16],
    resolution_weights: Optional[List[float]] = None,
    loss_type: str = "mse",
    normalize_attention: bool = True,
    temporal_weighting: bool = True,
    attention_threshold: float = 0.01
) -> AttentionLoss:
    """
    åˆ›å»ºè‡ªæ³¨æ„åŠ›æŸå¤±å‡½æ•°çš„ä¾¿æ·å‡½æ•°
    
    Args:
        sd_loader: Stable DiffusionåŠ è½½å™¨
        target_resolutions: ç›®æ ‡åˆ†è¾¨ç‡åˆ—è¡¨
        resolution_weights: åˆ†è¾¨ç‡æƒé‡
        loss_type: æŸå¤±ç±»å‹
        normalize_attention: æ˜¯å¦å½’ä¸€åŒ–æ³¨æ„åŠ›
        temporal_weighting: æ˜¯å¦ä½¿ç”¨æ—¶é—´æƒé‡
        attention_threshold: æ³¨æ„åŠ›é˜ˆå€¼
        
    Returns:
        è‡ªæ³¨æ„åŠ›æŸå¤±å‡½æ•°å®ä¾‹
    """
    return AttentionLoss(
        sd_loader=sd_loader,
        target_resolutions=target_resolutions,
        resolution_weights=resolution_weights,
        loss_type=loss_type,
        normalize_attention=normalize_attention,
        temporal_weighting=temporal_weighting,
        attention_threshold=attention_threshold
    )

def test_attention_loss():
    """æµ‹è¯•è‡ªæ³¨æ„åŠ›æŸå¤±å‡½æ•°"""
    print("ğŸ§ª æµ‹è¯•è‡ªæ³¨æ„åŠ›æŸå¤±å‡½æ•°...")
    
    try:
        # å¯¼å…¥SDåŠ è½½å™¨
        from models.sd_loader import create_sd_loader
        
        # åˆ›å»ºSDåŠ è½½å™¨å¹¶åŠ è½½ç»„ä»¶
        sd_loader = create_sd_loader()
        components = sd_loader.load_components()
        
        # åˆ›å»ºè‡ªæ³¨æ„åŠ›æŸå¤±å‡½æ•°
        attention_loss = create_attention_loss(
            sd_loader=sd_loader,
            target_resolutions=[64, 32],  # ä½¿ç”¨è¾ƒå°‘åˆ†è¾¨ç‡è¿›è¡Œå¿«é€Ÿæµ‹è¯•
            resolution_weights=[0.7, 0.3],
            loss_type="mse",
            temporal_weighting=False  # ç®€åŒ–æµ‹è¯•
        )
        
        print(f"âœ… è‡ªæ³¨æ„åŠ›æŸå¤±å‡½æ•°åˆ›å»ºæˆåŠŸ")
        print(f"   è®¾å¤‡: {components.device}")
        print(f"   ç›®æ ‡åˆ†è¾¨ç‡: {attention_loss.target_resolutions}")
        print(f"   åˆ†è¾¨ç‡æƒé‡: {attention_loss.resolution_weights}")
        print(f"   æŸå¤±ç±»å‹: {attention_loss.loss_type}")
        
        # åˆ›å»ºæµ‹è¯•æ•°æ®
        batch_size = 1  # å‡å°‘æ‰¹å¤§å°ä»¥åŠ å¿«æµ‹è¯•
        device = components.device
        dtype = components.dtype
        
        # åˆ›å»ºæµ‹è¯•æ½œç©ºé—´ï¼ˆæ¨¡æ‹Ÿ64x64åˆ†è¾¨ç‡çš„æ½œç©ºé—´ï¼‰
        original_latents = torch.randn(batch_size, 4, 64, 64, device=device, dtype=dtype, requires_grad=True)
        protected_latents = torch.randn(batch_size, 4, 64, 64, device=device, dtype=dtype, requires_grad=True)
        
        # åˆ›å»ºæ–‡æœ¬åµŒå…¥
        text_embeddings = sd_loader.encode_text("a portrait photo")
        
        # åˆ›å»ºç®€åŒ–çš„æ—¶é—´æ­¥åºåˆ—
        timesteps = torch.tensor([100, 200], device=device)
        
        print("ğŸ”® æµ‹è¯•å‰å‘ä¼ æ’­...")
        
        # ç”±äºè¿™æ˜¯ä¸€ä¸ªå¤æ‚çš„æµ‹è¯•ï¼Œå…ˆè¿›è¡Œç®€åŒ–çš„åŠŸèƒ½éªŒè¯
        print("ğŸ“Š æµ‹è¯•æ³¨æ„åŠ›å·®å¼‚è®¡ç®—...")
        
        # åˆ›å»ºæ¨¡æ‹Ÿæ³¨æ„åŠ›å›¾
        attn1 = torch.rand(32, 32, device=device, requires_grad=True)
        attn2 = torch.rand(32, 32, device=device, requires_grad=True)
        
        diff_loss = attention_loss.compute_attention_difference(attn1, attn2)
        print(f"âœ… æ³¨æ„åŠ›å·®å¼‚è®¡ç®—æˆåŠŸ: {diff_loss.item():.6f}")
        
        # æµ‹è¯•æ—¶é—´æƒé‡è®¡ç®—
        print("â° æµ‹è¯•æ—¶é—´æƒé‡è®¡ç®—...")
        temporal_weights = attention_loss.compute_temporal_weights(timesteps)
        print(f"âœ… æ—¶é—´æƒé‡è®¡ç®—æˆåŠŸ: {temporal_weights.tolist()}")
        
        # æµ‹è¯•æ¢¯åº¦
        print("ğŸ“ˆ æµ‹è¯•æ¢¯åº¦è®¡ç®—...")
        if attn1.grad is not None:
            attn1.grad.zero_()
        if attn2.grad is not None:
            attn2.grad.zero_()
        
        diff_loss.backward()
        
        attn1_grad_norm = attn1.grad.norm().item() if attn1.grad is not None else 0
        attn2_grad_norm = attn2.grad.norm().item() if attn2.grad is not None else 0
        
        print(f"âœ… æ¢¯åº¦è®¡ç®—æˆåŠŸ:")
        print(f"   attn1æ¢¯åº¦èŒƒæ•°: {attn1_grad_norm:.6f}")
        print(f"   attn2æ¢¯åº¦èŒƒæ•°: {attn2_grad_norm:.6f}")
        
        # æ³¨æ„ï¼šå®Œæ•´çš„å‰å‘ä¼ æ’­æµ‹è¯•ä¼šå¾ˆæ…¢ï¼Œå› ä¸ºéœ€è¦å¤šæ¬¡UNetæ¨ç†
        # åœ¨å®é™…ä½¿ç”¨ä¸­ï¼Œè¿™ä¸ªå‡½æ•°ä¼šåœ¨ä¼˜åŒ–å¾ªç¯ä¸­è¢«è°ƒç”¨
        print("âš ï¸ è·³è¿‡å®Œæ•´å‰å‘ä¼ æ’­æµ‹è¯•ï¼ˆéœ€è¦å¤šæ¬¡UNetæ¨ç†ï¼Œè€—æ—¶è¾ƒé•¿ï¼‰")
        print("âœ… æ ¸å¿ƒåŠŸèƒ½æµ‹è¯•é€šè¿‡ï¼Œè‡ªæ³¨æ„åŠ›æŸå¤±å‡½æ•°å¯ç”¨äºè®­ç»ƒ")
        
        print("ğŸ‰ è‡ªæ³¨æ„åŠ›æŸå¤±å‡½æ•°æµ‹è¯•é€šè¿‡ï¼")
        return True
        
    except Exception as e:
        print(f"âŒ æµ‹è¯•å¤±è´¥: {e}")
        import traceback
        traceback.print_exc()
        return False

if __name__ == "__main__":
    test_attention_loss() 